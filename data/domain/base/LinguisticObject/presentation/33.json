{
  "@context": "https://linked.art/ns/v1/linked-art.json",
  "id": "https://example.org/cdkg/LinguisticObject/presentation/33",
  "type": "LinguisticObject",
  "_label": "Textual content of \"Knowledge Graphs as Hub for Data, Metadata and Content\"",
  "classified_as": [
    {
      "id": "http://vocab.getty.edu/aat/300027388",
      "type": "Type",
      "_label": "Transcript"
    }
  ],
  "content": "It's my great pleasure to introduce, Naso Kiriakov and Vasil Momchev from, our good friends, OntoText. OntoText have been in this space a long time, and really looking forward to finding out the latest kind of direction of the, the the product that, they've they've developed. And I'll be moderating this session. Please, say hello, introduce yourselves, tell us where you're from, and also don't hesitate to, answer any questions, and we'll, we'll cover those at the end. Great. Welcome, Naso and, Vasil, and and over to you. Thank you. Thank you very much, James. It's indeed a great pleasure to, yeah, to be to be again part of this conference. We have fresh memories from one year ago in London. So, it will be myself presenting and and OntoTech's, CTO, Vasil Mamcheff. So I I I will I will entertain you with a bit of introduction of, who we are, and then I'll share our vision on on the market for knowledge gap technology and different applications and how, semantic metadata are the key to put, to for quite a number of different applications and how how they, can improve both data management and content management, and and actually put data and content together. Then, I'll hand over to Vasil to share with you, yeah, few updates about, Onstext platform and and and our database engine, Graph DB, and to to to basically walk you through, the different aspects of what it takes to to get data in a graph database, to to transform them, to make a proper graph out of it, and then to consume them either from whatever tool all the way from BI tool, that access data to a JDBC driver to another application that use use it through, say, GraphQL or other APIs. And then we'll finish off with an overview of our partner ecosystem because, with the those wide range of applications of these technologies, there is no single vendor, at least, no no no none of these, specialized technology providers, who who can deliver all the applications to you. So we built a partner ecosystem, to to to to be able to, put the right technology stack in place and to be able to to to also deliver it at at scale. So let's get started with the introduction . So on to take this, yeah, we started with, what was back then semantic weapon, semantic technology in year two thousand. And we had the chance to to to be to be part of the avant garde in this technology and went to the all the evolution to link to open data, nowadays, knowledge gaps. And, well, nowadays, we are, like, most of our clients, are are the total leaders in different industries and markets, like multinational companies. Still, if we have to to to come to where where their headquarters are, it's quite an even split between UK where where we got our first clients, big clients in stories ten years ago, and, then US has a growing area, Asia also, and main, mainland Europe. Up until a year ago, it was easy to say Europe. Now you gotta say Europe and UK. But, anyway so, you see some selection of our clients. I'll talk a bit more about the verticals , But, yeah, you see that most of them are precisely this type of really big enterprises. We we are not allowed to mention some of them, but that's that's the way it is. We being being part of this industry and part of the this community that is, developing the trends and setting the standards, we we do this through through a number of, yeah, organizations, worldwide web that we would see, being the main one, which not only standardizes World Wide Web, HTML, HTP, and all these standards on which the World Wide Web is built, but they are also the ones that standardized RDS, Fargo, OWL, and the stack of, the stack of standards that we we build our technology on. And we have we had the chance, and we we are still very happy to, to keep pushing our research to a collaborative project. So we are we are very, very well interconnected with those sorts of academic groups in this field in Europe. Our best, our most popular product is, Onstead's Graph DB. We're getting, I don't know, probably, thousand or a bit more downloads, of, Graph DB per month. And, yeah, we have an active user community of more than thirty thousand deployed instances. So there are plenty of statistics like that, and and we're happy that we also see recognition among, yeah, different, market analysts. So, OnSpace GovDB is seen as as one one of the one of the leading engines in this field. On top of this engine, we actually have a bigger set of, a bigger platform, knowledge graph platform, called you wouldn't guess it on the text platform, which extends what we get out of Graph DB with, analytics services and infrastructure for text analysis, for data quality management, and quite a few other things. So, this platform, yeah, helps you yeah. It makes it much easier to to to deploy, to to make knowledge center solutions. But we'll talk about the platform, later on in this presentation. So stop here. On the right hand side, you see, yeah, a business case or or or the typical solutions, that we offer and that we can customize across across different industries. Like, historically, we started with media and publishing, then we went to financial services. Life science and health care is a is a strong one too. So to finish off with the intro, what makes us unique is this combination of the graph graph database engine, which is suited for all all kinds of use cases for for knowledge graphs. And the combination of this database with text analysis, services and infrastructure tools, like curation tools, and manual annotation tools and so on and so forth that can use and are tuned to use big knowledge graphs for, text analysis. As I said, the entire portfolio is based on, the double two c stack of, standards. And there is also what if open source tooling as part of this this environment. So the entire front end of our database engine is open source. You can get it for kit, use it for everything you want. Then the the plug in API is open source. We are supporter of one of the leading open source projects in this space. The probably the biggest supporter historically, are the four j, also for text analysis platforms, and so on and so forth. So we had the chance to to to and I had the chance to gather a team with a very, very diverse set of, skills and and expertise across different types of technology and and science and and verticals. I'm not going to list them because that's not the purpose of this this presentation. Well, over time, we realized that, actually, the biggest the biggest asset and the most important thing that we have is this ecosystem of partners, which, two years came to trust on to text technology and work with us in in in different ways. So we have all all sorts of consultants and OEM partners, and we partner with quite a few, tool providers. And and many of these people are actually present on this conference. So Symantecweb company, MetaFacts, and and and few others. So, this was all in terms of introduction. So now let's talk about the market. If I gotta talk to to to the investors, the positive message is, well, this market is doing very well. If Gartner is saying that it will be doubling for another two years two years, That's that's just great. And if you look at these, hype cycles and curves, you see knowledge graphs are still still still going up, so on the this peak of, inflated expectations. So, luckily, we're still getting people to who who are calling us just because knowledge graphs are hot. But we also see that, and and and in this case, I can agree with with Gartner. We see that, this trend, is just about to to, yeah, go go down to the yeah. What what they call disillusionment because, yeah, what is important now is to see, really a mature ways, mature patterns, best practices, and and and tools which can be used to deliver to deliver value. So on the bigger picture, knowledge graphs are the adoption of this technology is slower than what my investors would want to see, and probably yours and and and, everybody. The fundamental reason is that, well, that this technology is quite complex. If you make it small, if you just convert a piece of data into into RDF, or you convert the taxonomy in and represent its cost, very likely, you'll you'll not you'll not be able to deliver on all these promises of all these knowledge graph magic does. So you you you have to use, and make big knowledge graphs that can really provide provide some context and allow better better, interpretation of the data and bet better, like, cap better capabilities for linking data. And once we get there, well, it's not as as easy. It takes a while, a while to do it to get the right tooling and the infrastructure to do it. That's why it's it's, our interpretation, why it's slower than expected. We we others this by, yeah, having prepackaged solutions and and partnering with, other vendors who who have solutions also. So the short story here is that, well, there is no single techno. It's not a single, single market. Knowledge graph technology is in the middle of other number of different applications. And, actually, there is no mature market for it now because there is no critical mass of such applications. We we gotta make the applications. We gotta find our way to to to to the end customers, to to really demonstrate the value behind the hype. And then five years from now, we we we we we can expect to see, a a critical mass of applications that generates a critical level of demand for the technology. So, it's clear that with all the diversity of what's possible to do with this technology, no no no single company of, our size can can deal with it. Even much bigger companies cannot deal with this this scale. So, again, our approach to this is to, have a have a ecosystem of partners, that can, that have the right vision for the specific type of applications, and and and have the engineering talent to implement them, taking into account all the specifics and the capacity to deliver. That's quite often a combination of different players. So, that's, yeah, our mind map of knowledge graph applications. So, to to us, knowledge graph is knowledge graphs are on the bigger picture part of the bigger metadata management story, and the applications go into, yeah, equally between data management and content management. There are plenty of knowledge management tools which also use this technology, and that's a big part of our best partners, like, Symantec Web Company and Synoptica. They are in in part of their offerings are in the space of enterprise taxonomies, and and and, vocabulary and this kind of technology, this time based on on on on on, yeah, knowledge gaps. Quite often, these taxonomies and vocabularies, they are actually used for better content management. This is the the the the the applications that you see on the right hand side. That's what Google calls inside engines. So so it's it's all about being able to do cool things with content, to to search, to recommend, to to to re if you're a publisher, for instance, to to repackage it and to to to better deliver it, to extract information, to extract data from it, and so on and so forth. Nowadays, probably the most, yeah, the the the most popular application, although probably not the most often delivered one, is in data management. So using using knowledge graphs for, data integration analytics. And that's what, Gartner called data fabrics. That's that's the whole story of, what was the data warehouse in the past and how you can use them to, yeah, do better better business intelligence and better decision making. Data management is not just that. It's actually much more than this. You have, a range of data governance and cataloging applications, And quite a a big number of these projects are are about, dealing with big, databases, like master and reference data management tasks, so that, yeah, you can aid improve your transactional systems either, like in our case, we we have many projects where we use big volumes of reference data, master data for better text analysis and content content analytics. So you see that, yeah, the the range of applications is quite different. Essentially, each of these, are already some mature markets. So, we we we have to find a way in which this, technology in the middle can contribute to to to, to to making more efficient specific applications in this market, and find a way to to, yeah, gently disrupt them and and and and, offer you you you value there. So that's enough about it. So to summarize the role of, how we see the role of knowledge graphs, the main thing is that they they they essentially put it in context, and and and they do it in two ways. One thing is, linking. So by interlinking descriptions of different things, you you basically help help help different pieces contribute to each other's description. And then the other the other thing is semantic metadata. So semantic metadata is is king. So you you have better formal descriptions of everything that's that's in there. What is that value? Well, someone gets better better insights, better results, better, discoveries, through, through the capability, by by by being able to have unified view, on diverse data. Diverse databases be being able to combine, structured data from different sources, being able to also put in the mix text documents and other content. And, well, quite often, what we see is that to to get the right level of analytics, enterprises should combine proprietary data with publicly available data, with with global data, to combine their wisdom and understanding of the market with background knowledge, which can serve for better interpretation. That's that's what we, deliver most of the time. So, we combine, master data, reference data from different with the different sources, what you see on the right hand side in a knowledge graph, and then we use this knowledge graph to analyze documents. Analyzing documents, we produce metadata, which becomes part of the knowledge graph. And this way, we we put, different kinds of databases and content together. Obviously, there are plenty of applications, and and benefits of doing this both on the content management side and and data management. So metadata is king. Metadata is everything. If you listen to to Gartner, metadata is, at the middle of the, yeah, next generation data integration applications, what they call data fabric. So it's about gathering different types of, metadata, putting them together, structuring them, interlinking them, activating them, and so on. The important thing to realize here is the the diversity that, you everything could be could be metadata. It could be, everything from from the schema of your your database to all sorts of catalogs or taxonomies or reference data, tagging, annotations of content, operational logs, and so on and so forth. So it's really diverse. You quite often need several, many types of metadata combined from from different systems to be able to to to make a flexible data integration platform or content management. I'm not going to read and, through all these, citations of Gartner. The baseline is that the only way to manage this metadata and to put it together is a knowledge graph. And, it is it is a semantic knowledge graph. The semantic approach having formal descriptions of, this metadata that helps you put data and content together. So the RDS tech web technologies allows you to do all this, in well. And there there are several features of the standards in this tech which contribute to this all the way from having global identifiers, which is key for interoperability. Formal semantics is really important because, it it allows sharing meaning and and, interpretation without ambiguities and and and, basically enables enables, much much better analytics and combining different analytic techniques. Standardization is, yeah, someone that everyone in in enterprise data management, enterprise content management, needs and wants unless you want to stick with a single vendor. And, yeah, at the end of the day, it's always about quality and being able to maintain quality. So, also so all means for, like, RDF shapes and other techniques for validation are very important. All these things are missing with the property graph standard. So, they they do great great job for for graph analytics, but there is essentially no standardization, and there is no notion of semantics. There is even no notion of schema. There is no data schema language. So it's obvious that RDF is much much better suited for knowledge gaps, and that's what we hear from, our clients being predominantly enterprise date data architects. We have built the methodology of everything that that it takes to, build in a and and if of knowledge graph all the way from requirement analysis, data gathering, and so on and so forth to, putting together semantic data model, integrating the data, reconciling the data, cleaning the data, and then being able to store, index them, enrich them, make them accessible, and, unable, updates and be able to maintain this knowledge graph. Once once you come with such a complex phenomenon, it's, not a piece of cake to keep it in good shape and good quality and be able to make all the updates that are necessary. So we introduced the On-tech platform, which supports quite quite a big fraction of this tech. Everything that is about, allowing data architects and everyone who who data management people, to collaborate with content management people and deliver value for the business users and the data scientists, which are quite often the consumers of this technology, as part of downstream models or people who train specific machine machine learning, models on it. Quite important to all here is dedicated to to business analysts and subject matter experts. So, part part for offering, is a set of tools that that allows, defining APIs and interfaces and specifying, views on this on this, knowledge graph in the middle by experts, by business analysts. So so you don't don't need a a big team of, programmers to to code middleware. With this, I'll finish my part of the presentation. I want to hand over to Vasil to tell you some updates about the OnText platform and and Graph DB. I'll stop sharing so that Vasil can take over. Thanks for the excellent market overview and giving us all the complexity of the market of around the knowledge graphs and how we can deliver these type of solutions. As database provider, when we meet clients, often we are asked, okay. I understand the concept of agreeing cohort shared identifiers for classes, instances, and some predicates, which identify the meaning of our organizational data. But could you tell me how should I put the data in Gravity, in your database, and how I should really develop applications on top of it? And this is probably the most frequent question we get from our clients when they expect really some sort of a mature tooling around the technology in order to quickly develop solutions. In this my presentation, I'm going to share five key highlights which has happened this year of twenty twenty, And these are really the major developments and tools we which we enable our clients to develop and partners, of course, to develop faster and better knowledge graph solutions. So the first one is really the data virtualization, which gives you a virtual sparkle endpoint access to the different databases. Then we enable the mapping with the mapping new mapping UI subject matter export with no coding to generate RDF models around some skimmers. Then I will speak about the data reconciliation, how you really need to integrate multiple datasets, which speaks about the same type of entities. Then we we go to the consumption of the data. Once we have the knowledge graph, how you can let your data scientists and business analysts to consume the data. And last but not least, I'm going to introduce the GraphQL platform, which gives you a really fantastic way how you can design APIs also for your application solution developers, and there is an automatic way to interact with the knowledge graph with really minimal calling. Okay. First thing first, data virtualization. Nearly all enterprise data is still stored in relational databases, and there is a good reason there because these are transactional systems, master reference records, where the information is primarily stored. The most logical question is how I can move the data from relational databases into a knowledge graph, basically. And we give you two type of use cases. The first type of use case is I want to make a decorative transformation of the relational database into RDF, or I want to access the database on the file. In order to enable this use case, Graphvini nine dot five integrates with the on top framework, which supports decorative, descriptors like the and the OBDA, which are part of the On top project to give you all the fly access to a wide range of relational models like Postgres, DRAMU Oracle, MySQL, Microsoft SQL Server, really a big range of technologies. In order the design decision we took was to expose every virtual every relational database as its own virtual repository in order to make the data locality very transparent for the users. This made the users fully aware that they are accessing different type systems. However, in order to optimize the performance, we implement the highly efficient internal federation, where if you federate two endpoints which are located on the same CloudDB Server, you use a highly efficient internal protocol for data exchange. The benefit is that you get to this on top integration also integrated with all the security features and the the Graph DB security model, single sign on, and all these type of functionalities. So the second feature is really mapping UI, how you can let your subject matter experts and the people knowing the data without coding to populate the knowledge graph. How many of your organizations have most of the critical data in some sort of a spreadsheet, CSV file, or something which is used as a reference model? Using the open refine tool, which is nicely integrated with open with Graph DB and Graph DB Workbench, you can transform any type of structure that you are form of JSON, XML, CSVs, g files, whatever you have, and it gives you an efficient way to clean and cluster the data. Usually, once you start with any type of CSV or Excel files, there is a really sort some type of cleaning. This is nicely integrated with CloudDB so you can create a project with the table of data. On the right side, you can see the new mapping interface, which checks what are the word ontologies in the repository, and it can guide the user what are the product predicates and the properties sorry, the types to use in order to generate a mapping, which is then transformed into RDF file. So from this table or data, you can really record this as a structure. You you basically say how you transgenerate the RDA values and how these values are connected. As a next step, it supports two modes. One is really the interactive mode where the user can, preview the information, clean it, check what we would the output. But, also, it supports the tool supports to batch everything, to record the transformations, and really to make it sound and transparent using a batch script. Also, the API supports streaming data streaming where you can batch really big files and process them on demand. The third big feature is the data reconciliation, and this is the case when you have to integrate multiple datasets sharing the same identifiers. Once again, our focus is to subject matter experts and how without code to perform the entity resolution. The data resolution is the entity resolution is actually matching entities into different type of, tables. And we say this is a mapping of string two things. By using the graph connectivity and the schema, this gives you really a power of subject matter experts in a stepwise process to say how you can reconcile the data. On your screen, you see some American US banks, and the task is really to find the right ID in dataset, like QuickiData or whatever. So why this is an efficient work process and it requires no calling? Because the subject matter experts can start reconciling the state. Like you see on the diagram, the reconciling the US states is an easy task because there is no big community. Then you can go to the cities by using the state, which should use the, in the because the, cities are connected to the states. Later on, at the final step, you need to really reconcile the organization and pick the right organization. Next, let's imagine that you have the knowledge graph and you passed all the hurdles to put the data in a knowledge graph and the RDL databases. But the next question is, hey. I have a big data scientist and knowledge engineer team, and none of them knows Parco. They're really good with business BA type of tools like Tableau, Microsoft Power BI. Could you just export me a spreadsheet and a table in where I can analyze it? No. This is really a forward approach because if you you'll be replicating data, which is wrong. Now browDB comes with the JDB access, which enables all type of tools which supports JDBC like Tableau, Microsoft BI, to directly interact with the data and to connect, investigate what are the tables. And you can directly visualize and take the data out of the database. The way how we do this is we define SQL views, which are basically SPARQL results. In these SPARQL results, there is a nice user interface where you put your SPARQL query, you say what is the SQL table name, and the interface, depending on the data distribution, guides you how to use this right SQL types in order to get the results. At the end so below you see a one technicality is that this driver will also try to push as much as possible of the complexity down to the database. So if you have work logs in the SQL query, it will be pushed down to the sparkle in order to minimize the exchange between the SQL engine and the RTL database. Last but not least, this is really a very big topic. Autotech invested in a platform based on GraphQL, how you can really expose how you can expose knowledge graphs and quickly build and generate APIs. One of the main challenges is once you design any type of a system is that you need to plan for the APIs, and these APIs needs to be synchronized with the scheme of the model. Now the Graph DB and the now the auto text platform can read the data with using this auto text platform workbench, the schema of the of the graph database, and automatically generate the Graph QL API for you. Using this GraphQL API, you can select what are only the required permitted operations in order to control the access to the data. By for bridging the GraphQL to RDF, Autotext develops some really key features. One is really a highly efficient graph to sparkle translation. This seems as an easy task, but in order to get the full complexity, we have tweak the semantic the semantics of SPARQL and having a special execution mode, which makes this really efficient. Also, this is a bidirectional mapping where we're using mutations, which can be also automatically generated. You can interact with the data and modify it, directly from the GraphQL. This is really very good if you have UI applications who need some which needs some sort of data validation or access control, which is also supported over the schema. What's point here is, this I'm not sure how many of you are familiar with the GraphQL ecosystem, but there is another excellent feature, which is the GraphQL federation, which allows the integration of third party services. This is a service level integration of the acquired data visualization, but for other type of services. Since this is a very broad topic and I don't have time to go in details, I really invite you to join the later session, which will be led by Jim Rayfield, our chief solution, Arhite, who can give you in the deep technicalities of GraphQL federation and knowledge graphs and how you can control your knowledge graph via the GraphQL APIs and how you open the spectral chaos. Okay. Thank you. I give it now back to Naso to continue with the partner ecosystem and how all these tooling and key features we develop as a successful business model using our partners. Thank you, Vasil. Although, yeah, we we we have a very capable database engine, and we have, a platform on top of it that that provides a lot of infrastructure and technology, we still cannot cover everything that you may want to do with the knowledge graph, and, before start speaking of applications. So, to address this, we have, a range of portfolio partners, which are companies like Symantec Web Company, Synaptica, MetaFacts, the sensor, Omni, Memotix, Workbench, quite a number of partners who provide tools and platforms which integrate Graph DB. And and and, this way, we complement each other's portfolio, and we can much better add the the full stack of needs for this technology. Beyond that, we we we work quite a lot with, different kind of consulting partners. So we work with, some of the biggest, IT service, providers and system integrators on Earth. On the list there, you have, like, Infosys, WIPO, Auto SentiData, Fujitsu. These are five of the top ten biggest IT services companies, and, they they are familiar with the technology and and and delivery to to to big clients all the way from government to to pharma and back. We also have, much, yes, more specialized, let's call them boutique consulting and delivery partners, like, data language in in UK and enterprise knowledge in in US, Southworks in Korea, and and and many others. So, this is an ecosystem that currently consists of more than fifty partners. So, we will not try to list all of them, and we took care to to to put it right on for all kinds of, agreements to to make to make this ecosystem operate, properly. So, yeah, we're looking forward to join other partners to complement the even further, the ecosystem with either consulting delivery skills, either a a a, yeah, tools. And this this good point, to to invite you to the webinar. Vasu, if you can go to the next slide. The the the the webinar that, will take place in twenty minutes. That's part of the promotion to party eight of our partner, Semantic Web Company. You see that Vasil is having a very intensive day today. So he'll finish here, and he'll be part of, the webinar that starts in twenty minute with minutes with Pu party eight. So, it will be it will essentially show how, yeah, yeah, the integration between pool party eight and and and and and Gravity makes pool party much better performant and, improves on on the scalability side. And we also demonstrate, Vasil, I think you gotta press next because there is some animation. Okay. Part of this demo will be, well, we have a we we we have a, new stacking service scope now, and it works with giant technology graph derived from Wikidata, geo names, Wikipedia, whatever. And we use it to analyze news articles. So part of this service will be a demonstration of how you can, tune the performance, the the the the behavior of this, very sophisticated natural language processing text analysis pipeline by, doing small corrections to this knowledge graph with pool party eight and immediately see improvement of results. In this way, we we show how voice management and tailor made vocabularies, and and, expert work can can can combine and can deliver value in combination with very big knowledge graphs. So it's it's a work with them. We've been and we're I've been working half half life to make this possible, so you you still have a chance to join in twenty minutes. And that's just one of our partners. We are very happy with our partnership with Symantec Web company. So join the webinar if you wanna pursue, and, the the people from Semantic Web Company tell you more about it. That's everything from us. Thank you very much for your time and attention. Massive, thanks to Naso and, Vasil. It's really interesting to see.",
  "language": [
    {
      "id": "http://vocab.getty.edu/aat/300388277",
      "type": "Language",
      "_label": "English"
    }
  ],
  "created_by": {
    "id": "https://example.org/cdkg/Creation/presentation/33",
    "type": "Creation",
    "caused_by": [
      {
        "id": "https://example.org/cdkg/Activity/presentation/33",
        "type": "Activity",
        "_label": "Presentation of \"Knowledge Graphs as Hub for Data, Metadata and Content\""
      }
    ]
  },
  "digitally_carried_by": [
    {
      "id": "https://example.org/cdkg/DigitalObject/recording/33",
      "type": "DigitalObject",
      "_label": "Recording of \"Knowledge Graphs as Hub for Data, Metadata and Content\""
    }
  ]
}