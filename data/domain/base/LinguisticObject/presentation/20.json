{
  "@context": "https://linked.art/ns/v1/linked-art.json",
  "id": "https://example.org/cdkg/LinguisticObject/presentation/20",
  "type": "LinguisticObject",
  "_label": "Textual content of \"Graph Abstractions Matter\"",
  "classified_as": [
    {
      "id": "http://vocab.getty.edu/aat/300027388",
      "type": "Type",
      "_label": "Transcript"
    }
  ],
  "content": "Hello. Good afternoon. I'm Ora Lassila, and I will be talking about graph abstractions. This is titled as a personal view, mostly because I'm kind of talking about things that I've been thinking about for several years now. Not only in the context of my current position as a principal technologist in the Amazon Neptune graph database team, but also throughout the work that I've done with with RDF and the Symantec Web over the past more than twenty years. So here is a rough game plan today. I'll talk a little bit about the history of graphs first, and then I'll get into graph abstractions and and really how to access graph data from the software standpoint. From that, I'll go to treating an entire graph as an abstraction and talk a little bit about what we can do to unify the currently somewhat fragmented world of graphs. And, I'll conclude with, some thoughts about how to how to move forward from the current state. So this is my take on, on on the history of graphs and ontologies. Oftentimes, these things are presented as something new and the reality is quite the opposite. So if we look at the graphs, graph theory as a branch of mathematics dates back to the early seventeen hundreds. And ever since the, the birth of computer science as a as a field of study, we've understood that graphs are really the essential underpinning for most of computer science. Social networks showed up sometime in the 1960s. Things like the small world experiment, six degrees of separation, and and, something we call the Erdos number. And then in throughout the sixties and seventies, there was work on on network databases or navigational databases and then semantic networks, which is really where we're starting to spill over to the ontology side here. If we look at ontologies, really, we can go back to the third century BC, and Aristotle's work on categories and logic. Of course, the whole idea of taxonomical classification of things, got more concrete in the early seventeen hundreds when Linnaeus classified plants and animals. In late eighteen hundreds, we saw work on library classification in the form of the Dewey classification system. And then around nineteen hundred, philosopher Edmund Husserl really worked on sort of understanding how ontology can be the foundation for representing information and the meaning of information. Now fast forward to the 1970s and onwards. Predicate logic became sort of the foundation of knowledge representation in the field of AI. And then in the late 90s is where these things come together in the inception of the semantic web and the related standards such as RDF and OWL and whatnot. And from that, we basically get the modern knowledge graphs and graph databases and all that. So long history. But despite this history, programming with graphs and ontologies is by and large still rather cumbersome. And this is something, I'd like to address. So, when we're talking about graphs today, we first of all have this a little bit of fragmentation of the space. And and and this may be all familiar to all of you, but let me just kind of summarize, anyway. So we basically have two kinds of graphs, RDF graphs and labeled property graphs. On the RDF side, graphs are such that vertices and edge types are really just identifiers. Graphs decompose into something we call triples. So basically, these are the edges and the two endpoints of an edge. Scalar data types in RDF are pulled from XSD, XML schema. And there are no composite data types per se, and composite objects are constructed using the graph structure itself. Now, on the property graph side, vertices and edges themselves are structured objects. There is no common identifier scheme like there is on the RDF side. And the scalar and composite data types, they are typically borrowed from the whatever is the implementation language underneath. We also have fragmentation when it comes to query languages. So on the RDF side, we have SPARQL. And in the context of this talk, on the property graph side, we have Gremlin and Cypher. There are others too, because the field is more fragmented. I'll be talking about RDF mostly. Towards the end, I'll come back to property graphs. If we look at RDF and the abstractions available for us in terms of programming with graphs, The triples, which RDF graphs decomposed to, are very low level abstractions. It's like programming with assembly language. So we need something different. And in the past, we've sort of tried to map graphs to trees, but we have a meta model mismatch there. Graphs and trees, there there are differences which can cause all kinds of confusion. And of course, examples of this are things like RDF XML, You're trying to use XML as the representation for RDF and JSON LD, which is basically does the same with JSON. I think of those as warning examples. Now, of course, it's also important to remember that really when it comes to graph, the syntax does not matter and it should not matter. You solve the syntax problem once, and then you forget about it. But what about the graph abstractions themselves? So could we do something that people have done on the relational side? So something that could be like an object graph mapping? Take data from a graph and map that to some kind of objects. There are it can be done, of course, but there are some interesting issues here. So first of all, what constitutes an object? So how much of the graph do you have to pull to build an object? And then, of course, there are these kind of shared references. And in RDF particularly, there's something called blank nodes. These are nodes that cannot be addressed from outside the graph and those can be difficult to handle if you do this mapping to objects. Graph queries, or as I'd like to sort of think about, this, how do I get there from here? So graph queries, mostly, they're about traversal in the sense that traversal is an excellent way to understand what you do with a graph and how you kind of find things in a graph. So questions like where can I go from here given this particular path pattern? Are these two nodes in the graph connected? And if they are, how are they connected? Using path patterns, for traversing a graph is a little bit like pattern matching the graph itself. And of course, SPARQL is a very powerful query language, but it's a very big hammer, very heavy tool for most graph access problems. We need something simpler. Something like a simple search, maybe faceted search or filtering of data from the graph. And once you've done the filtering, then of course the question is, give me everything you know. to the particular node in the graph. This is sort of related to the object graph mapping. And then, of course, queries that really are about traversal, specifically. So specifically. Now, graphs are the graph data, graphs they're modeled somehow. And here again, we have some problems. So first of all, the property graphs generally do not have schema languages that would let you do the modeling. So the model is implicit or it's buried in the code that you write. RDF, on the other hand, in some ways has too many schema languages. So, we have RDFS, we have OWL as a sort of more powerful schema language, and then we have SHACL and they are all languages that were created really for specific purposes. And I think there, what we would need is some kind of a unifying view of these because telling people that, hey, you can model your graph, but here are three schema languages that you have to use. It's just confusing. And then the third problem, and then maybe the most important problem is that how do you then take models of your graph data and map those to whatever it is that you have to do when you're writing code? There's also this question of shared ontologies or shared models. So let's say you have decided to use some shared ontology and you map your data to this particular ontology. Now what? What does that actually buy you? How do you take advantage of the fact that your data now conforms to a particular model? So here I'm sort of pondering, can we support ontologies with some kind of predefined software libraries that basically encapsulates some of the knowledge about this ontology. And I like to call these ontology engines. This is ongoing work on my part. And then finally, since we're talking about ontologies, there's the question of reasoning, which is really something we can do on the RDF side, particularly. First of all, we have symbolic reasoning. So this is really reasoning using some type of logical rules. Interesting things to to to note here is that generative reasoning or reasoning that kind of adds more edges to the graph, if you will, is something that can easily be hidden from the application itself. The application actually does not have to know that there's a reasoner running underneath. Queries and accesses to the graph are really not to the asserted graph itself, but they are to the graph plus all the things that we have managed to infer from the graph. Then, of course, we have non symbolic reasoning. So machine learning techniques can actually be used much like symbolic reasoning. You can do things like node classification or link prediction. And I think what we really need is sort of an expanded view of reasoning where we will kind of stop this sort of rift between symbolic reasoning and non symbolic reasoning. But I think more importantly, what we really need to do is we need to rethink what an application is. I think that logic should be associated with the data and the models and not with an application. Traditional view of an application is that it's sort of the gatekeeper to particular kind of data. And I think a better view of of an application is that it reflects the user's intent to, accomplish something. But in this view, you really have to kind of separate the application from the semantics of the data. I think graphs really could be sort of an overarching logical abstraction for all the data that we manipulate. Modern enterprise data practices, to be, to put this kindly, is messy and data silos, they're commonplace. And data integration often happens through ad hoc solutions or just a custom integration scripts or what have you. And this thing will never end unless we find some kind of a unifying logical not only manipulating the data from the standpoint of our software, not only manipulating the data from the standpoint of our of our software applications, but also from the standpoint of data integration. My view of this is that until something better shows up, RDF and OWL actually can provide this view. I chose this picture here specifically. This is the Tower of Babel. And I think that that's where we are when it comes to data practice today. Lots of languages, everybody's talking, nobody's understanding. And then, of course, we still have this question of RDF versus the labeled property graph, so let me talk a little bit about that. First of all, when users are given the choice between RDF and property graphs, it tends to confuse users. They don't know what to choose and once they make that choice, it tends to be hard to walk back that choice later on if you find out you made the wrong choice. This really should not matter. I think the real issue here deep down maybe that there are these two kind of ways to think about graphs. Graph as a logical representation of your data versus graph as a physical data structure. I think RDF tends to represent the former, property graphs often the latter. But regardless of this, the practical problem we face is that we still have two separate ecosystems for tooling for these two different kinds of graphs. So let's talk a little bit about these graphs, RDF and property graphs. What are the pros and what are the cons? So RDF is a W3C standard. It's well established, originally created over twenty years ago. RDF makes it easy to use external data sources. RDF comes with schema languages And, particularly when it comes to external data sources, RDF has formal semantics, and it particularly has formal semantics for graph merging. And then finally, RDF supports reasoning. So you can incorporate symbolic reasoning in your application. Property graphs on the other side are very intuitive for software more so than RDF. And they tend to integrate better with programming languages, particularly when it comes to the query language Grambling, which in a way sort of is a programming language in its own right. Now on the negative side, for RDF, it's often considered kind of too academic. I don't necessarily share this view myself, but this is what I keep hearing. And related to that is that there's no kind of easy on ramp to learn and adopt RDF. Now, on the property graphs, on the negative side is that there is no standard. There's many divergent implementations that are proprietary. There's no schema language and there is no formal semantics. So this makes it hard to do a lot of things and I'll come back to that. So this is the reality, but what we really want is we want both. We want the good things from RDF and we want the good things from property graphs. And we don't want to be confused with the choice between the two. So this sort of brings me back to the idea that the graph itself is sort of an abstraction. What we really want is a unifying model that covers both RDF and the property graphs. Why? Because this would help adoption of graph technology and I think this is important to the graph industry, if you will. It's still sort of a nascent industry and to really see mainstream adoption, we need to make things easier and less confusing. It also gives users more choices if we were to have this. So for example, I personally would very much like to run Gremlin over RDF, but today I can't do that. So in the Neptune team, at AWS, we're working on this, in a project code named OneGraph. And turns out that this is actually not so easy. There are a number of obstacles. We think we can solve them, but let me just kind of give you the highlights of what things are difficult. So first of all, there's the question of the formal semantics of RDF and also this emerging extension of RDF called RDF star that tries to bring RDF closer to how property graphs work. In RDF semantics, triples are unique, which means that that you can't have two triples that are the same but would be separate so that you could assign different properties to them. Whereas this is a very common pattern in property graphs. And of course, those of you who know RDF, there's the old style verification mechanism, for this, and it does not have this problem, but verification tends to be cumbersome and is often misunderstood. I think it's the most misunderstood part of the original RDF specification. There's, of course, this question of lack of formal semantics for property graphs. It's kind of difficult to determine what the correct semantics are. And so building this unifying and unifying model, it's now difficult because we don't actually know how to reconcile RDF semantics and property graph semantics. On the property graph side, there's also kind of a diversity of scalar, scalar data types, which would have to be reconciled against the set of scalar data types on the RDS. There's question of graph partitioning. So RDF supports a mechanism called named graphs that lets you partition your graph. This mechanism does not exist on the property graph side. Now, to make things even more complicated is that in RDF, vertices are just identifiers. There's no structure. So when you're partitioning your graph, you're really kind of what you're doing is that you're you're asking which particular partition does any particular edge belong to. But on the property graph side, both vertices and edges are structured objects. So what do you do about the partitioning when it comes to vertices? And then finally, there's this question of update semantics. So to come back to the first problem, so let's say we have these non unique triples. So kind of multiple instances of the same edge, if you will. When you delete one of them, what happens to the others? Are those deleted as well? We have to figure out what the right semantics for that is. And then, of course, if you have properties on edges, so you've associated some properties with the edges themselves, What happens when you delete such an edge? Are those properties deleted as well? So would we need some kind of a cascading delete for edge properties? All these are problems that we have to solve before we can say that we can have this unifying model. But anyway, we're working on this, we're getting there. So, the way forward, and this I really would like to present in terms of what are the real pain points when it comes to building knowledge graph software. So first of all is the basic question, how do I write software that leverages knowledge graph and knowledge graph data? We have to kind of bridge the gap between the graph and the code. Your graph has a model, your code needs to understand what that model is, how do you do that? And as I said earlier, we have these very low level interfaces to graphs, and that basically means that you end up writing a lot of boilerplate code. So I think the answer is that we really need to think carefully what the proper interfaces and abstractions are for graphs. So the second question is, okay, so I map my data to an ontology. Now what? So, what does it mean to have some kind of support for ontologies? What is the role of reasoning? As I said, hiding the reasoning works well and and and there are some things that you can pick from RDF and AL that are particularly useful. But how do you do this? At this point, I'm thinking libraries built to support a particular ontology might be the answer. Like I said earlier, I call these ontology engines so that you basically have some software support if you decide to adopt a particular ontology that you use for modeling your data. So then there's the question of property graphs versus RDF. What do we do about those? Choosing between the two is hard. It leads to confusion. We have this effort, the RDF star effort that is going to offer some alleviation to this, this problem, but may not get us quite quite there. For that, the OneGraph Unified Meta Model should bring a relief. And, we recently published a paper on this and, I'll have the link to that at the end of the presentation. And then finally, there's this question of, can we get out of this practice of never ending integrations between data that sits in silos. Modern tools we have do not actually offer a single unified view of data. And I do believe that graphs, particularly built on built on RDF and OWL modeling, can offer this unifying logical view. And that doesn't mean there has to be a physical view of the data. But we need some common language to talk about data so that we can talk about integrations and we can kind of separate the applications from the physical data. So those are my thoughts today. I'm happy to take questions and here are some links, some material about how to build knowledge graphs on using Neptune and then this paper of ours that talks about the unifying model, between RDF and Propertures. Thank you.",
  "language": [
    {
      "id": "http://vocab.getty.edu/aat/300388277",
      "type": "Language",
      "_label": "English"
    }
  ],
  "created_by": {
    "id": "https://example.org/cdkg/Creation/presentation/20",
    "type": "Creation",
    "caused_by": [
      {
        "id": "https://example.org/cdkg/Activity/presentation/20",
        "type": "Activity",
        "_label": "Presentation of \"Graph Abstractions Matter\""
      }
    ]
  },
  "digitally_carried_by": [
    {
      "id": "https://example.org/cdkg/DigitalObject/recording/20",
      "type": "DigitalObject",
      "_label": "Recording of \"Graph Abstractions Matter\""
    }
  ]
}