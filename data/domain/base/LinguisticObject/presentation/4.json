{
  "@context": "https://linked.art/ns/v1/linked-art.json",
  "id": "https://example.org/cdkg/LinguisticObject/presentation/4",
  "type": "LinguisticObject",
  "_label": "Textual content of \"Knowledge Mesh: From Data Silos to Data Fabric at Global 2000 Enterprises\"",
  "classified_as": [
    {
      "id": "http://vocab.getty.edu/aat/300027388",
      "type": "Type",
      "_label": "Transcript"
    }
  ],
  "content": "Hi, everyone, and I'm very happy to be here today. Hope you are enjoying so far from the different sessions. This session will have two parts jointly given by Szymon Klarman and myself. My name is David Amzalag, and I'm the chief product officer of Black Swan Technologies. Black Swan is a growing startup that specialize in data. We have presences in six different countries, more than two hundred and fifty employees, and different type of customers. Why different types? Because every company, enterprises, for instance, is having a very large number of data sources. Sometimes hundreds of different source data sources and sometimes thousands of different data sources that the organization is relying on and using him for all type of usages. Obviously, regardless the industry, those enterprises are belong to, health care, finance, insurances, retail, etcetera. What is common with all of them is the real and big concern they have on how to effectively manage all of those data sources, keeping them synchronized, accessing them in real time, keeping the infrastructure not so expensive, and very important, how to monetize valuable insight from all of them. Sounds like a big problem. No? Let's make this problem even more realistic. Almost all data sources are feed by different applications. Salesforce Salesforce and other CRMs, NetSuite, AWS, Facebook Ministry of Interior, etcetera. Those probably are geographically spread in addition. Now what we are doing in many of the cases is to build a data lake, one centralized data lake from all data sources that this means that the data will be stored at the same place. No doubt that this might help operationally. However, this is not so cheap solution, and data is not kept synchronized in many of the cases. Moreover, once the data is kept in a single place, different personas of the organization might have different usages of the data. Different queries might be asked, and data from different sources must be consistent and aligned. Actually, a very big problem. Black Swan created technology so no data leak is needed, and access to data source is done directly to where they are. Geography here is not so important. Interface is being done throughout the no code type of user experience, So people from all type of organizations, meaning accounting, hospital data scientists, investors in bank, etcetera, will be able to create applications and queries by themselves actually very easily and really to generate insight from the data with without being, let's say, very good software people. The insight is being generated continuously throughout a large infrastructure of knowledge graphs and its generalizations. Thank you very much for your time and handing over to Shiman to speak about our interpretation of knowledge graph. We call it the knowledge mesh. Hi. My name is Shimon Klarman, and I'm a knowledge architect at Dexcom Technologies. As David already explained, the main challenge we are trying to address is how to facilitate utilization of data within large data intensive organizations who commonly own enormous amounts of data assets, and yet they struggle to get the actual insights and knowledge out of those assets. So a prototypical, starting point for those kind of scenarios could be a use case we could call a three hundred sixty degree entity view. So something, that in practice would be known as single client view, know your customer, compliance, a scenario where we want to gather information about a certain, real world entity or collection of such entities, where this information is actually spread across multiple data management systems. It's well known that in this kind of scenarios, organization struggle with a number of, data challenges such as data discoverability issues. Sources are often based in multiple even multi cloud environments. Because of that, there's lack of uniform accessibility rules. There's a heterogeneity of database models and schemas, lack of semantic interoperability across those schemas in which data is expressed. There's a number of data quality issues such as data incompleteness, granularity, or data normalization issues. Now one, natural way of handling this kind of three hundred sixty degree entity view use cases is by employing enterprise knowledge graphs. I guess within this audience, this is, this is actually a well known solution. We know that we would like to get the clean, the duplicated, resolved, reconciled, up to date view of all the entity data. This kind of entity and relationship centric, view is arguably much closer to the real world or at least the way we think about the real world. It presents all the information integrated as it's a very powerful data integration framework under a single consistent data model. Further, this being a a graph, also a semantic graph, which we can explain a bit later, means that we can apply a range of analytical techniques on top of such structure to get insights, things like semantic inference or machine learning on graphs. So altogether, if we add it up, offers us possibility to get a reliable knowledge, something we can trust, something that gives us a lot of contextual information out of just a plain raw data source. Of course, we know where we'd like to get as the ideal, end of the journey, but how to get there is a challenge in its own right. And knowledge graph construction process, according to some textbook recipes, could be, under some simplification presented as this kind of, ETL pipeline going from the sources to the final knowledge graph by the process of extracting, transforming, and loading the data, most likely to a graph database. The extraction process would most commonly be achieved using type, sort of extractors depending on the specific data sources. So for instance, SQL extractors for structured databases, scrapers for, HTML websites, NLP algorithms for processing text documents. The data extracted from extractor using extractors would then be mapped into a target knowledge graph schema that we want to use in the application, And extraction results would be further reconciled, meaning, quite literally would basically connect the dots, resolve we would resolve entities ensuring that there are no duplicates, which in the same way would resolve relationships between those entities and normalize the attributes. At the end of the day, we would have a a nice, clean, consistently duplicated knowledge graph. This being an ETL pipeline, however, is actually where big problem is. Yes. We are getting a nice, clean knowledge graph at the end, but, actually, it's a knowledge graph that, again, is very hard to maintain and evolve over time, and it doesn't exactly solve the data siloing problem that we wanted to solve to start with. Instead, it just creates yet another data silo. Now why is that? Well, if we compare this ETL approach to some other known ETL based, centralized, data integration platforms, such as based on the concepts of data lakes and data warehouses, we can see that, there's a lot of commonalities. Okay. The final results might might look a bit different, but it's this centralization of this whole architecture And, the the the fact that the process is based on ETL, one directional ETL process, is what causes the issues. And the issues are basically, that we generate a lot of ETL and ELT code, which, actually encapsulates a lot of meaning, with that we attach to this data, a lot of semantics. And this semantic is only accessible and manageable for developers who build this code. And the result of it is that this meaning of data becomes essentially lost, and there is no real end to that end data ownership, that would, allow different, domain focus teams to be able to manage and, govern different data assets over longer periods of time. Now what's an alternative? Following years of experience with single ad challenges, we arrived in architecture, which we call the knowledge mesh, which essentially tries to challenge all the most important pain points of those, previous centralized, data integration architectures. Instead of centralization, knowledge mesh assumes a decentralized data world. Instead of moving data to a single central storage place, we only virtualize the data at the query time, at the, analytic insight time or put it generally at the right time when the data when the integrated data is needed. Instead of developing bespoke ETL pipelines, we provide generic capabilities driven by metadata, which can be managed by nontechnical subject matter experts. And instead of focusing on specific applications of particular data assets, we focus on collecting and maintaining general knowledge pertaining to broader business domains. So overall knowledge mesh is this domain driven architecture of metadata and data management and integration tools, which are aimed to maximize the effectiveness of data utilization in knowledge graph applications in different domains. Let me now walk you through very briefly through, different layers of this architecture, to highlight the most important aspects. So firstly, there's a uniform data access and virtualization layer. Essentially, data fetchers are these simple, services that allow to extract the data at query time. They are not basic they are not just data extractors as in this bay previous ETL, approach to to knowledge we have constructors. They are little database interfaces, essentially, which can take any data source and expose it using a uniform data description language, basically, a uniform schema of the source, and offer a generic query language, again, which allows you to formulate queries that that are needed by the application to satisfy certain, data requirements. We use, GraphQL as this interface, although, obviously, this could be achieved, in number of other ways. The important part is that the source, the the data fetcher that exposes the source is agnostic about the application. It's the mapping, explicit declarative mapping layer that connects the, the schema, exposed by the fetcher to the application , which provides this essential glue between the sources and the application layer. Now, the one of the most important, if not the most important, aspect of this architecture is the rich metadata layer. What is this metadata? It's basically the data about, all the data assets that are involved, in the, in the application. But even more than that, it's about all the infrastructural components that are necessary to efficiently conduct whole data integration and data interpretation process end to end. So it's the information about the data sources, about the fetchers that serve them, about the domains where those sources are assigned to, about the mappings, which map, the data as served by the fetchers to specific domain vocabularies, which are also associated with those domains. All these components form essentially a connected metadata graph, which provides a lot of powerful capabilities to the system. It allows to catalog different kind of resources to increase discoverability. It opens up a whole range of, low code, capabilities, where we can actually, instead of writing the spoke, code support certain functional functionalities, we can offer, generic solutions which are consistently driven by this metadata to achieve certain goals and support certain, use cases. All the metadata is obviously, expressed in standard formats, standard metamodels, which further increases interoperability between humans and machine agents. All the metadata, as mentioned before, is, is, organized in a domain oriented fashion. Now what does it mean? What is a domain read? A domain is a predefined collection of assets that make sense, and are particularly useful in the context of some range of applications which would jointly, collect in a in under an umbrella of some, business domain. This kind of business domain could be a compliance or could be cyber or perhaps could be health care. In any case, a domain consists of a domain vocabulary. So the collection of terms which we use to describe all important, data relevant data within this domain. It contains of a collection of sources which are catalogued for browsing, for discovery. It also contains mappings that, again, generically map the data from this domain to the domain vocabulary and possibly also a bit, a collection of of, domain specific algorithms or basically some intelligence which is already built in into that domain. For instance, algorithms that can that allow you to perform entity resolution on entities, within specifically within this domain. As a result of of of offering those kind of of building those kind of business domains, the process of constructing a specific application and and, developing a a knowledge graph for this kind of application can be, to a large extent, automated. It's, so once the application is being created in specific domains, we can automatically leverage the metadata, the whole understanding of which sources can be used, how they map to this domain vocabulary, and how they should, how the data from these sources should be essentially processed so that the knowledge graph powering, fueling this application, this domain can be, constructed relatively, seamlessly with a reasonably low effort from the application developer. On the way, we collect we are able to collect another layer of metadata. This is a metadata about where each data item comes from. What do we know about the source it came from? With what confidence and with what method, this specific piece of, data has been extracted, and how it was processed later on to, for instance, resolve, one entity against against the other. All these additional, layers of metadata are something that essentially allows us to to offer higher quality guarantees, to the application consumers. So this is essentially the process of extracting knowledge from raw data where the knowledge basically comes into the picture at the moment where you can start building trust and start building confidence that that the raw data was, was was was sourced and was processed in a way that ensures high quality information at the end of the process. Now when we add these two different layers of metadata, which I mentioned, in the previous slides together, the metadata and the data collected from the data sources and the metadata about the whole about the whole infrastructure, data integration infrastructure, we finally obtain this, this complex twofold knowledge graph or as it's sometimes called, a full data fabric. It is exactly the structure that is necessary for efficiently utilizing all available information and automating all the data processes, that we want to, that we need to carry out in order to gain insights and get knowledge out of the originally disconnected data sources. So to summarize, knowledge mesh is this domain centric data integration and virtualization architecture consisting of a range of advanced data management, data processing, data cataloging tools working together, to, to basically help automate and help manage all the steps required to build efficiently build up to date knowledge graphs from disconnected distributed data sources. It, thus maximizes the effectiveness of data utilization in knowledge graph applications. And, for those and perhaps familiar with so called FAIR principles, principles basically, adopted increasingly adopted by a large data intensive enterprise. Knowledge mesh is really nothing else but metadata first architecture that is intended and it allows to make the data findable, accessible, interpretable, and reusable. And with this, I'd like to finish and together with David, take any questions you might have. Thank you for your attention.",
  "language": [
    {
      "id": "http://vocab.getty.edu/aat/300388277",
      "type": "Language",
      "_label": "English"
    }
  ],
  "created_by": {
    "id": "https://example.org/cdkg/Creation/presentation/4",
    "type": "Creation",
    "caused_by": [
      {
        "id": "https://example.org/cdkg/Activity/presentation/4",
        "type": "Activity",
        "_label": "Presentation of \"Knowledge Mesh: From Data Silos to Data Fabric at Global 2000 Enterprises\""
      }
    ]
  },
  "digitally_carried_by": [
    {
      "id": "https://example.org/cdkg/DigitalObject/recording/4",
      "type": "DigitalObject",
      "_label": "Recording of \"Knowledge Mesh: From Data Silos to Data Fabric at Global 2000 Enterprises\""
    }
  ]
}