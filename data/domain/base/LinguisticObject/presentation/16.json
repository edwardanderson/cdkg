{
  "@context": "https://linked.art/ns/v1/linked-art.json",
  "id": "https://example.org/cdkg/LinguisticObject/presentation/16",
  "type": "LinguisticObject",
  "_label": "Textual content of \"Leveraging Graphcore’s IPU architecture for large scale GNN compute\"",
  "classified_as": [
    {
      "id": "http://vocab.getty.edu/aat/300027388",
      "type": "Type",
      "_label": "Transcript"
    }
  ],
  "content": "This presentation will discuss the opportunities for leveraging Graphcore's IPU architecture for graph neural networks. We will review the computational requirements and challenges of graph networks that require both sparse access to memory, a sparse computation, and dense processing, which is associated with the use of neural networks as function approximators. We will then introduce the intelligent processing unit, the IPU, and review the design principles, the processor architecture and the system scale out. Finally we will discuss implementation opportunities for graph neural networks on the IPO. We will start by considering graph neural network processing and its challenges. Graph neural networks have been successfully applied to a growing number of applications based on graph structured data. Applications that range from the small graphs of molecular chemistry and drug discovery to the very large graphs of recommended systems. This application involved tasks including node classification, link prediction, and graph classification. However, graph neural networks come with a number of challenges. The first challenge is related to sparse computation. The implementation of the graph neural network is based on the message passing algorithm, which is implemented as a series of message passing steps or GNN layers. And for each individual message passing step, the embedding of a node is updated using the neighbors of the node. And for each of the neighboring nodes, the computation involves the use of a message function to compute a message associated with the neighboring node, and all the neighboring node messages are then aggregated and sent to an update function, that produces the update, the sequence to update the embedding of the node. And both the message function and the update function are parameterized by neural networks and therefore corresponds to dense computation, where there is at the same time irregularly sparse connectivity associated with gathering the neighbours of each individual node. This GNN sparse connectivity results in lower arithmetic intensity and higher communication costs compared to dense processing. And these computations are particularly challenging for modern, hardware accelerators. Another challenge is related to heterogeneous graphs, which are graphs with multiple node types and multiple edge types or relations. For these graphs, for each of the relation types there is a different set of neural network parameters, in addition to having different set of neural network parameters for the individual layers of the g n n, for individual message passing steps of the g n n computation. This, considering that in general in the order of thousands of relation types for this type of graphs, this corresponds to a significant increase of the number of parameters to train, which can cause overfitting. Another challenge of graph networks is related to the fact that increasing the number of GNN layers, and each each one of these layers, corresponded to an aggregation of neighbors, this causes an exponential increase of multi hop neighbors, which causes an increase of computational complexity and memory cost. At the same time increasing the number of neighbors aggregated over several hops and aggregating information that corresponds to an increasing number of nodes that are part of the graphs, makes the representation of the nodes more similar, which causes over smoothing. Oversmoothing can correspond to a degradation of the task performance. Another challenge is related to dynamic graphs. Dynamic graphs are prevalent in real life systems with applications related to social networks, forecasting, epidemiology and others. And these types of dynamic graphs evolve over time features of their connectivity, have for example additions or deletions of nodes or edges, and have the transformation of the features of nodes and edges. An issue related to this type of graphs is that when collecting a number of examples in a mini batch for training, especially when the there is a large mini batch, the information that is evolving over time related to the last samples in the batch lacks inaccurate up to date information for the early examples in the batch. So there is a staleness not up to date information that again can correspond to a degradation of task performance. A final issue related to training of graph neural networks relates to large scale deep learning. We have already described the issues of deeper GNNs, graph neural networks that have a large number of message passing steps which can cause a neighborhood explosion and oversmoothing. Besides those problems when training very large graphs, there is the issue that training still requires efficient and fine grained access to memory. And given the high level of sparsity of large graphs, when accessing a memory it is possible that only a fraction of the communication bandwidth may correspond to useful bandwidth. We will now review the main features of the intelligence processing unit architecture. The intelligence processing unit has been designed from the ground up to accelerate AI workloads and to achieve the following goals: Fast and fixed cost access to memory, Efficient fine grained computation with low latency. The possibility of efficiently scaling to systems that include a significantly high number of processors. And finally, the possibility of maintaining high performance for low precision compute, when using low precision number formats that deliver high computational efficiency, for low power consumption, and reduced memory cost. This has resulted in the following main characteristics of the IPU process: A very large number of independent cores, and distributed on chip SRAM. Computation that is based on multiple instruction multiple data parallelism and relies on a bulk synchronous parallel programming model, which allows to exchange data between different cores of the processor without memory concurrencies. And finally, in addition to the provision delivered by the hardware of efficient computation for fine grained computational elements. This is also supported by the software with the provision of sparse instructions and libraries. These slides give additional details on the IPU processor, the GC200. A single processor contains one thousand four hundred and seventy two independent IPU tiles, we distributed in processor memory. Each tile consists in other words of both compute and memory, for a total of two fifty teraflops of fourteen point six in compute and one hundred megabytes of in processor memory. The on chip memory can be accessed with a bandwidth of forty seven point five terabyte per second. Four IPU processors can be put together to build an IPU M2000 machine which deliver a total of one petaflops of floating point six in compute and a total of three point six gigabytes of in processor memory. The M2000 machine also contains an IPU gateway and external DRAM, for a total of four forty eight gigabytes of streaming memory. The connection between the four processors, the four IPU processors of the N2000 machine is provided by FastIP Link, that have a bandwidth of sixty four gigabyte per second. This slide compares the memory subsystems of the F2000 machine with the memory subsystem of alternative hardware accelerators. And the LPUM2000 have a larger amount, a significantly larger amount of on chip memory, which is accessed with extremely high memory bandwidth in excess of forty seven terabytes per second, and also have a much larger amount of DRAM with lower bandwidth, which is motivated by the fact that the large amount of on chip RAM, on chip SRAM generally corresponds to a reduced requirement of DRAM traffic. This slide gives typical examples of the use of external memory for training of neural networks. There is on the top the simple case where the entire model fits on in processor memory and there is no need to access external memory for training. In other situations, it's possible to advantageously store master weights and optimized states in external memory, especially for situation corresponding to pipeline parallelism where, there is only infrequent need of the weight update and accessing external memory to use the optimizer state for updating the parameters. And finally, there is the case where there is is useful to have a more regular access to external memory with phase execution using streaming memory and overlapping of communication and IO. And in this case, the periodicity of accessing the external DDR is larger given the provision of a large amount of on chip SRAM. The IPM two thousand and seven machine that we have just described can be used to build even larger systems. The IPOPOD64 is constituted by sixteen IPOM2000 machines corresponding to sixty four IPU processors, and provides a total of sixteen petaflops or fourteen point six in compute, corresponding to fifty seven point six gigabytes of importer memory and around seven terabyte of streaming memory. And these IPU pods can be also used to build even larger systems, aggregating several racks, several IPU pods, to arrive to hundreds and even thousands of processors up to sixty four thousand processors. And these very large systems are glued together by the IPU fabric, which provides connections between the individual IPU processors of each M2000 machine within an IPU pod, as you have described before, and a connection to the host, and connection through the gateway link between different racks between different IPo pods of a much larger system. We'll now consider the implementation opportunities offered by the IPU. Implementation opportunities for graph processing on the IPO are given by the scalable and cost effective DRAM that can be used to store large embedding, but the possible use of large SRAM to store neural network weights and model data, but the use of distributed SRAM rely on the BSP programming model to deliver efficient execution of it for heterogeneous computation and finally the possibility of using the fast APU links for efficient computation in pipelines or for phased execution. The first method that can be used to train very large graphs is based on mini batch training using sampling of nodes with their respective k hop neighbors. Of course this method, for deeper GNN, for GNN with a large number of GNN layers, can face the problem that we have mentioned before, related to neighborhood explosion and oversmoothing, and in general can cause a large increase of computational memory overhead. This is the reason why the earlier implementation used the fixed neighborhood size per layer, reducing the number of neighbors segregated per each message passing step, which of course makes for effective training of very large graphs, but has a downside that corresponds to a reduction of expressivity of the model compared to processing on the original graph, which may correspond to reduced performance, reduced task performance. Alternative approaches have been proposed more recently decouple the depth of the layer, the number of message passing steps with the number of hops for neighborhood aggregation with what is referred to as shallow sampling. This method can compromise the computational efficiency by an aggregating an excessively high number of neighbors with an increased expressivity by training deeper networks. The performance of mini batch training can be improved by sampling of connected subgraphs, which are provided by graph clustering. This method improves performance and also increases the embedding utilization of PDMATCH training. In this case the complexity increases only linearly instead of exponentially with the number of GNN layers, which allows to increase the number of massive passive steps of having the deeper GNNs, which improve the expressivity of the model and correspond to improved task performance. And the performance can be further improved by sampling multiple connected clusters, which reduces the variance across mini batches. Both mini batch training are based on selecting nodes with the corresponding neighborhoods and selecting subgraphs provided by graph clustering can be efficiently implemented on the IPU. Both, in both cases the mini batch, the examples of the mini batch can be stored on on chip memory, which provide efficient and fine grained execution with fast access and for neighborhood aggregation on chip. The implementation can be scaled over a larger number of processor by parallel training, in particular relying on data parallelism and pipeline parallelism. With data parallel training, the mini batch is distributed over multiple processors and using the on chip memory to store the elements of the mini batch allows to continue to have efficient training, efficient access to memory, but increasing the the size of the mini batch and speeding up training. Pipeline parallelism at the same time involves the implementation of a number of pipeline stages which can be implemented again in the on chip memory of a number of IPUs. Different pipeline stages can include one or multiple GNN layers, one or multiple message passing steps, that corresponds to the same computation graph with different parameters. And it's particularly attractive to consider the storing the parameters of the message passing steps on the in processor memory on the on chip SRAM to reduce the amount of communication during training. And both data parallelism and pipeline parallelism for speeding up training can be complemented by the use of low precision number formats, which allow to store larger batch sizes and to be more memory efficient and to store larger amount of computation element, other larger number of samples for node wide sampling or larger subgraphs in the case of sampling of the connected subgraphs. And the lower precision number format not only allow to increase the the size of the subgraphs that can be added, can be stored on on chip, but also, as we said before, corresponds to more efficient computation and reduced power consumption. As an example of efficient computation on the IPO, we consider now temporal graph networks. This is current work in collaboration with Michael Brosteyn and Emmanuel Rossi, who had developed and proposed the original temporal graph network model. Temporal graph networks operate on continuous time dynamic graph that are represented as series of temporal events. That corresponds to the evolution of features or connections over time. The temporal event, as we said before, for a dynamic graph can be can be constituted by the addition or deletion of a note or the interaction between pair of notes, the modification of the edge between between different notes. In the case of the model, of the temporal graph network model, it's the characteristic is that the node, each node is associated with a memory, a state, that is updated after every event. The computation starts by collecting a mini batch of a number of recent temporal events for processing and this mini batch contains the current state of the nodes involved in the in the mini batch, in the events that we consider. And this information is then used to compute a message associated with the the nodes under consideration. And the messages are then used to update the memory of the nodes. The memory update, it's implemented by neural network, typically a GRU, a Recurrent Neural Network. And then the updated states, the data memory of the nodes together with the input information associated with the mini batch is used for providing node and updating node embeddings. Now the node embedding function is particularly critical and is based on aggregating information from the node neighbors. Because this is justified by the fact that in these graphs and some nodes are often infrequently updated. Therefore, it is advantageous to rely on the connected neighbors and information to compensate for the information that is lacking in up to date information that is missing for specific nodes. And this is the reason why the node embedding update implementation relies on, for example, temporal graph attention network or temporal graph sum networks. Once the node embedding has been updated through this type of processing, the updated node embeddings are fed, are sent to a decoder which provides node classification or link prediction in the case of this block diagram. The task is to provide prediction of the future for future interactions. This model had been very successful, improving by a large amount the state of the art in a number of relevant tasks. However, as we have discussed before, when when considering dynamic graphs, they have the issue that for a large batch size using the aggregation of a large number of the temporal events, there is the possibility that the later component of the batch, the later samples of the batch, lack the information from the area sample and have the not up to date information which can cause stale update and degrade the task performance. We have implemented this model on the IPU and the plot on the left hand side of this slide shows in fact that the average performance, the average test precision during training, it's much degraded for larger bite size. During training increasing the number of epochs the performance for larger bite sizes is significantly worse than the one for bite sizes small as forty or sixty. However, using batch sizes as small as forty or sixty typically is connected with a fine grained computation that is not very efficient on the conventional hardware accelerators. But implementing this type of computation on the IPOS is shown here on the right hand side, we have been able to maintain improved task performance at the same time delivering speed up of training. Maintaining a faster training for the smaller batch size corresponding to the case of the large batch size which doesn't achieve the same task performance. In summary, we have reviewed the main challenges of graph neural network processing that requires fast computation, a heterogeneous and fragmented computation with a high memory load. And we have discussed the key features, the key design features of the IPU architecture, which can rely on scalable and cost effective DRAM and can make use of the BXP execution and the large amount of and cheapest RAM to provide efficient computation for fine grained elements associated with the processing of graph neural networks. And we have considered implementation opportunities and methods for training large graphs on the IPO and we have given us a particular example of efficient fine grained computation for a graph neural network implemented on the IPU. And this concludes the presentation.",
  "language": [
    {
      "id": "http://vocab.getty.edu/aat/300388277",
      "type": "Language",
      "_label": "English"
    }
  ],
  "created_by": {
    "id": "https://example.org/cdkg/Creation/presentation/16",
    "type": "Creation",
    "caused_by": [
      {
        "id": "https://example.org/cdkg/Activity/presentation/16",
        "type": "Activity",
        "_label": "Presentation of \"Leveraging Graphcore’s IPU architecture for large scale GNN compute\""
      }
    ]
  },
  "digitally_carried_by": [
    {
      "id": "https://example.org/cdkg/DigitalObject/recording/16",
      "type": "DigitalObject",
      "_label": "Recording of \"Leveraging Graphcore’s IPU architecture for large scale GNN compute\""
    }
  ]
}