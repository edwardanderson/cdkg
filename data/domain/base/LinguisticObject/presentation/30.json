{
  "@context": "https://linked.art/ns/v1/linked-art.json",
  "id": "https://example.org/cdkg/LinguisticObject/presentation/30",
  "type": "LinguisticObject",
  "_label": "Textual content of \"Deep Learning on Graphs: Past, Present, And Future\"",
  "classified_as": [
    {
      "id": "http://vocab.getty.edu/aat/300027388",
      "type": "Type",
      "_label": "Transcript"
    }
  ],
  "content": "It's, my massive pleasure to, introduce Michael Bronstein. Michael Bronstein is a professor at Imperial College London where he holds the chair in machine learning and pattern recognition, and also head of graph learning research at Twitter. He also leads, the, machine learning project, in, project SETI, a TED audacious prize winning collaboration aimed at understanding the communication of, sperm whales. So without further ado, over over to you, Michael. Welcome. Thank you, James. I hope you can see and hear me well. So thanks a lot, for the invitation to to join this presentation today. And I would like to talk about, deep learning graphs and, try to outline what it is about, what is the present state of this science and technology and what the future columns for it. So allow me to start with actually a little bit far away and taking a step back and talking about the concept of inductive bias. So this is a fundamental notion in learning and it refers to the the set of assumptions that, a machine learning system has to do about, the data in the problem at hand. And let's take a very simple machine learning system, some of the earliest neural networks called, multilayer perceptrons. We know that they can approximate any continuous function to any desired accuracy. We call this property universal approximation, and it was proven in the end of the eighties for for these architectures. And, it sounds like a good piece of news. Right? Because we can represent anything we want with multilayer perceptrons. But with the moment we try to apply these simple neural networks to real problems dealing with high dimensional data, they tend to fail miserably. And, let's look for example at the problem of digit classification, one of the simplest examples of a computer vision problem. Essentially, what we want to say here is whether what we see is the digit three. And the way that you can think of this problem when you try to apply multilayer perceptron to this digit classification, you just stack the image into a vector and pass it as the input to the, to this neural network. The problem is that what happens when we have another instance of the same image where the unit is just shifted by one pixel as you can see here. And the input to the neural network can be very different because the neural network is absolutely unaware about the structure of the image and thinks of it as a one dimensional vector. So it will take a lot of examples and very complex architecture with a lot of parameters to learn in variance to shifts from the data. And this is one of the reasons why early attempts to apply neural networks to, to image data, to computer vision problems, failed, failed miserably. Now the breaks will in applying neural networks to images has come from the right inductive bias. And these are convolutional neural networks from the seminal work of Jan de Kann, where the inductive bias is what we call a translation equivariance. Basically, it's hardwired into the neural network architecture in the form of shared local weights. And this way, we have now way less parameters and this idea that you can recycle the same weights and apply them at different positions at the image at different scales is very powerful. That was really what made these architectures so successful, and the results speak for themselves. As you know, CNNs have really revolutionized the field of computer vision in the past decade. Now let me show you a different problem. What you see here is a a molecule, and, this is a molecule of caffeine. I hope that in the break you've had enough of it. So I have a little bit in my my tea cup here. And, we can model it as a graph. The nodes here represent the atoms and the agents represent the chemical bonds. Let's say that we want to predict some chemical property of this molecule, for example, what physicists or chemists would call the atomization energy. So it's the energy that takes to break this molecule apart. And, this is really fundamental problem in, drug design and drug drug discovery to be able to do virtual screening to predict certain properties of, potential drug candidates. So how do we represent this molecule? Again, we can just take the features of the nodes and put them into a vector as we did before with an image. The problem though that we have many, many more ways to do it. Actually, any permutation of the nodes, produces a valid representation vector. And the kind of invariance we want to have here is different from the previous example. Here, we need to account for all the possible permutations. And molecules are just one example of graph structured data. In fact, we see graphs everywhere. Probably the most prominent example are social networks where the nodes are users and the edges represent their social relations and interactions. So you can think of Facebook or Twitter or any other graph that is generated by by the activity of of humans. We also encounter graphs, or networks in biological sciences where we look at the interactions between different biomolecules such as proteins and drugs and so on. In computer graphics and computer vision where we use graphs with maybe a bit more structure that are called meshes to represent three d objects and, in many, many other fields like in brain imaging where graphs can be used to represent functional networks and so on. So if you want the gist of what deep learning on graphs is, it's essentially finding the right inductive bias for graph structured data, which is sometimes also called relational inductive bias. And in two thousand sixteen, I wrote a position paper with Jan de Kann, Pierre van der Geist, Joanne Brunner, and Arthur Schlam, where we connected several attempts to deal with, irregular or non Euclidean structures in deep learning, which we named, geometric deep learning. So I would say that now I would probably write it completely different. So I think there is a much more depth to this idea of trying to geometrize machine learning problems. But somehow this term is now used synonymously with graph deep learning or graph representation learning. And, as I said, there is more to that. We can think of geometrical learning as a framework unifying grids, graphs, curves, and gauges, which is high energy physics term for manifolds. So, in fact, we like calling it the four g of deep learning. So this year, graph neural networks have officially become one of the hottest topics in machine learning. And at least judging from the submissions of the six of ICLR, one of the main conferences in ML, this has been one of the popular keywords. So let me show you some more details and let's look at classical CNNs. So if you look at classical CNNs that take as input an image, which we define as a function that I denote here, by x on a regular two dimensional grid, What convolution does is a form of weighted aggregation of the values in the pixels of a neighborhood. And we can do the same thing in the graph. The neighbors will be the nodes that are attached by edges to any node in the graph. So so far, it looks all the same. But one thing to notice is that when we move to a different location, we still have a constant number of neighbors because the grid is regular. So in this case, each pixel is connected to four neighbors. And on the graph, on the other hand, we might have a very different number of neighbors. We had six neighbors in the previous node and five neighbors, here. And if you think of social networks, these differences can be huge. So a popular user like Donald Trump has millions of followers and other users might have just a few tens or hundreds. And this is what is called the node degree in graph, in graph theory. Another thing to observe is that on agreed, we have a fixed ordering of the neighbors. We can always talk about, a node to the left or a node to the right, and this allows me to always apply the same weights to the first neighbor and other to the second and so on. And this is exactly the idea of weight sharing in conversion on your networks that that I mentioned in the beginning. So if you represent this as a matrix, we actually see that there is a special structure that is called, circulant matrix. And, because circulant matrices commute, we also have commutativity with the shift operator, which is called shift equivariance. And in fact, you can actually derive convolution from first principles of symmetry, of translation symmetry. And, this is the the idea of geometric deep learning where inductive biases emerge from, first geometric principles. So on the graph, the situation is rather different. The ordering of the neighbors is completely arbitrary, so we don't have a canonical way of assigning a fixed weight to a given node. And this actually makes graph neural networks quite different from traditional CNNs in the form of the variance that that we get. So here's a blueprint for how to do conversion like operations on graphs, which have two types of operations. We can aggregate information from neighbors, and we can process it in some way and then update the features of an old. So these are the two operations, aggregate and update. And aggregate has a most general form of a function that is applied to to the neighbor node features. And importantly, this, function is, prohibition environment. And there are some important particular examples of architectures where this can be linear aggregation or an aggregation that uses some form of attention mechanism. So let's now dive more into the details of what is similar and what is different, between graph neural networks and traditional deep learning pipelines and look at CNNs. So if we look at historical developments of CNNs that that that appeared in computer vision problems. So early models such as AlexNet from two thousand twelve were relatively shallow, and they have just eight layers with relatively large filters of up to eleven by eleven pixels. And, as CNNs became more commonplace in computer vision, they also became deeper and used smaller filters. So the VGG architecture had twenty layers and three by three filters. And there are several reasons for, why this happened. First of all, obviously, smaller filters are more efficient computationally. But more importantly, it was shown that in convolutional neural networks, you can construct complex features from simple ones, the property that we call compositionality. So if you look at the features that that that are noted in different layers in the neural network, you see that the first layers have primitive geometric features such as edges or corners. And as you go deeper, you get more complex, complex features. It appears not to be the case in graph neural networks and it's really wishful thinking, composing complex structures from simple ones. For example, it was shown recently that graph neural networks of the message passing type are equivalent to what is called the Weiszler Lemann graph isomorphism test, which is a classical algorithm from graph theory that determines if two graphs are isomorphic by means of some color refinement procedure. And, this is a test that can tell you whether two graphs are possibly isomorphic, but it's necessary but insufficient condition. In this case, for example, the the the test will fail, because, it is known that it cannot count, simple substructure such as triangles. And one of these graphs has triangles, another one doesn't. So there exists higher order, more powerful versions of the the Weisler Lemann test, but they have, prohibitive computational and memory complexity. So another thing to notice that unlike in the traditional architectures, it is actually difficult to to train deep, craft neural network architectures and, they require a lot of, tricks such as regularization and, architectural, changes such as residual connections. And, the bottom line that that even with these tricks, sometimes shallow, baselines work better than than the deep super duper graph architectures. So one reason for this is what is called feature oversmooling. It means that features on the nodes tend to collapse to a single point in the feature space. But probably a more fundamental phenomenon is that, it was described in the recent paper, by Uri Alon that, there is a bottleneck. So in some graphs, where the number of neighbors tends to grow exponentially as you expand the neighborhood size, you get a lot of neighbors, whose features you need to squeeze through a single feature vector. And if you have, this exponential growth of neighbors and you also happen to depend on long range information, you run into the bottleneck phenomenon. And, in a sense, it's not clear for which cases, for which problems, and for which graphs, that helps. So in a sense, deep graph neural networks try to use many layers with small filters, just one hot filters. The alternative is to use fewer layers but make the filters bigger, and that's exactly what we try to do in the work with my collaborators at Peter. We took this idea to the extreme. We wanted to see what we can do with a convolutional layer, just single paragraph convolutional layer. And, an analogy in classical CNNs would be to have a shallow network with bigger filters. The nice thing here is that if we use linear diffusion, linear message passing, we can pre compute the diffuse features and then it boils down to just applying simple multilayer perceptrons to the predefined old features. And as a result, the neural network is extremely efficient, and it scales to to very large graphs. So the surprising finding is that such a simple architecture performs almost on par with some of the much more complex state of the art deeper models. And again, this brings the question when do you need depth for what kind of graphs and for what kind of problems. What is for sure that it's significantly faster by more than an order of magnitude in training and inference, and, it resembles the the inception convolutional neural networks that that were pioneered by Google a few years ago because, we use filters of different size, as was used in that in that architecture. So there are certain graph quantities we cannot compute by means of message passing no matter how deep we make our own neural network. And I should say that this is not fully understood and on the contrary, there are examples of properties such as graph moments, for example, that can only be computed unless the network has certain minimal depth. So it's still an open theoretical question. So what we can do is, to help graph neural networks to count substructures by providing these counts as some peak computed feature vectors. It's kind of positional or structural encoding, and we can do this by, peak counting some structures of size k. This could be, for example, triangles or clicks or cycles or paths of different lengths. And we provide this as node or edge fishes and then do standard message passing. So we call this architecture graph subtraction networks. And, the nice thing about it that it actually retains the linear complexity and the local structure of standard message passing neural networks. And, the, comp the the precomputation is the part that might be expensive. In the worst case, it is as complex as the high order of Le Mans, methods. But, in practice, it can the complexity is much lower. So what we gain in this way is that, the graph's abstraction network is strictly more powerful than the the device for a lemon or the equivalent message passing graph neural networks. And we have problem specific inductive bias, we see that, for example, by by counting certain structures such as clicks in social network graphs, we get significantly better performance. And especially interesting for molecular datasets describe chemical compounds. Cycles are important motif, and, they're abundant in organic molecules with structures such as aromatic rings. And, again, this is my favorite molecule of caffeine. It has two rings of size five and six. So if you use a graph substructure networks with these structures, we get significant gain in performance of predicting chemical properties of molecular graphs. And the experiment shown here is on the TING dataset that is often used for virtual screening of drug like compounds. And I believe that applications of graph deep learning in computational chemistry and drug design and discovery are probably the most promising. Allow me to come back to this point in a few minutes. So in the remaining time, let me share some thoughts on what I believe to be, the next steps in this field. And, I would like maybe to make here a small confession. I'm somewhat disappointed with, the the when I started working on, genetic deep learning probably around six years now, I was expecting something similar to the revolution that happened with the adoption of deep learning in computer vision and we have not seen anything similar yet. And, of course, there is a lot of progress and even some, commercial applications of, craft neural networks in the industry. Well, I could here shamefully put the success of a startup company that I founded with my students, that where we were using graph neural networks to detect misinformation on on Twitter and were acquired by Twitter last year. Yet, I think it has been more of an evolution even though a fast one. And, let me try to explain and maybe highlight some points that are important for future progress in this field that that will make maybe a broader adoption of deep learning on graphs. So there are three things really that sort of made deep learning happen, and these are data compute and and software. So in case of computer vision, data was a benchmark such as ImageNet. Compute was the computing power of graphics hardware, the GPUs, and software was open source tools such as PyTorch or TensorFlow that that have democratized deep learning. Now if you look at the situation, in graphs, well, we also see emergence of standardized benchmarks such as ImageNet for graphs, the open graph benchmark, software libraries such as DGL or PyTorch Geometric that implements some state of the art graph learning architectures. There are problems of efficiency and scalability. So this is really what has precluded so far the application of graph learning to industrial scale and industrial settings. So now we have, several methods that that can really work in production systems of large scale. If we look at, problems such as Twitter and Facebook, they deal with dynamic graphs. So the graph is not a static session, but it's living and evolving in time. Nodes are added and deleted, and it's really, better to think of this graph as a kind of a synchronous stream of events that form it, like edge and node insertions and deletions. And there are currently just few architectures that support these cases. So this is one of the topics that, I'm working, on a Twitter work with or we've developed recently what we call temporal graph networks. It's an architecture that generalizes message passing neural networks to dynamic graphs. Now talking about high order structures, I already mentioned them in the context of our work on graph substructure networks. And I would like to stress again that so far, graph neural networks were primarily focused on simple, structures such as nodes and edges and message passing on these structures. But we all live in many complex networks such as biological or social networks. We have complex high order structures and motifs, and, we want to better exploit them. So I believe that there will be, in the future emergence of methods that take advantage of these more complex structures and their interesting relations to previous works that have been done on topological data analysis such as persistent homologies on graphs. Another important topic is actually the very assumption that we are given an input graph to start with. In many cases, this is not the case. We don't have the graph. We just have some cloud of points and the graph can be just a convenience that can be used to model the underlying data structure. We did first work that we called dynamic graph CNNs. It were we showed that it's possible to design graph neural networks that build the graph as part of the learning process on the fly. And, the graph, for example, can be constructed, as k nearest neighbor graph and updated between the layers, in a way that is optimal for the downstream task. And this brings an important question of whether the computational graph that is used for message passing does necessarily, be the same as the input graph. There are many good reasons why we'd like to decouple the two, one of which I already mentioned, the the bottleneck phenomenon. So you might rewire your graph to make it more convenient. And another reason is that, the graph can actually be, as I as I already said, it can be done in a way that, that is optimal for some downstream task. And sometimes the graph that can be learned in this way, setting that we call latent graph learning, might be more important than the downstream task itself so it can provide some interpretation of the of the problem or the classification results. And we did with my collaborators in Munich, a recent work where we looked at health care electronic records of, different patients. And graph neural networks have been applied to these problems before, but we can craft it graphs. And here we show that learning the graph as part of the process by, by graph neural network provides better results and also better hopes to interpret these results. And, thinking maybe broader in retrospective, this kind of methods is related to what was called manifold learning or nonlinear dimensionality reduction, a class of approaches that, modeled, the data as sampled from some low dimensional manifold that lives in a high dimensional space and, tries tried them to represent this dataset in a lower dimensional space by preserving some structures such as geodesic distance and then applying, machine learning algorithms such as clustering on the slow dimensional representation. So, the problem with these approaches was that the different steps were completely disconnected from each other. And you first had to to create some handcrafted representation of the data, then build the graph that represents the structure and only then apply machine learning. And sometimes it required a lot of tuning by hand of how you represent the data and build the graph. Now with graph learning pipelines, you can, put all these stages into a single end to end differentiable pipeline, and that's why I call these methods meaningful learning two dot o. It brings all these steps into a single, architecture. And probably we'll see more interesting applications of these methods and one, maybe a little bit exotic field where these, matters are already, been shown quite, quite interesting to produce quite cool results is high energy physics where you can think of, interactions of, different particles. And, one of the key problems is to reconstruct the interaction graph. So there are many, open theoretical questions about, performance guarantees, expressive power, generalization, robustness of perturbations, and so on. And, I think what has been done so far is just the tip of the iceberg. Last but not least, and I apologize maybe running a little bit over time, I would like to finish with a few examples of what they call killer apps. So something like computer vision, was the killer app for, for traditional deep learning and conversational neural networks. For, graph neural networks, we see, we see them applied to a lot of different problems since graphs are really very abstract and universal models for systems of relations and interactions. You can find them in particle physics, in recommender systems, in, problems in social networks and computational chemistry. But, if you ask me what would be one field of which I am willing to bet where these methods would probably make a breakthrough, I would say these are problems in medicine and biology, and you can apply graphs on all scales from nano to macro, from modeling molecules and interactions between molecules to entire patient networks. And some of the results here are really extremely promising. I would even say dramatic. So one, on the nanoscale, one application is that we can model molecules as graphs and predict their properties, which is a holy grail of drug design because the space in which we operate is humongously large. We have something like ten to the sixty of possibly synthesizable, small molecules. Whereas, what we can test in clinical environment is maybe a few hundreds of compounds. So we somehow need to to bridge this gap computationally. And at the lower level, we can do, quantum mechanical models and molecular dynamics and maybe some approximations such as DFT. So, craft neural networks, already several years ago were shown by any work by by DeepMind by Justin Gilmer to be, at the level of DFT while being several orders of magnitude faster. And this is really a cheap alternative to, more complex quantum mechanical simulations for predicting properties of potential drug candidates. And one doubt that you always have when you work on this kind of biological or chemical problems that it is too simplified and kind of spherical course in a vacuum from the famous joke, but, probably not anymore and GraphML is already on the radar of pharmaceutical companies. And earlier this year, the group of colleagues at MIT showed the discovery of a new class of antibiotics where graph neural networks were used in the virtual screening pipeline. So in our application also, in drug design, drugs are typically small molecules, but their targets are usually large molecules, proteins. And, proteins are among the most important, molecules in our body. They play crucial role in almost every biological process. And, in some targets, unlike, what we see with small molecules, they're considered to be very difficult target by small molecules because they have flat interfaces. So they're, what is called undruggable. And, it is possible to develop a new class of drugs called biologics or biological drugs where the drug molecule itself is a protein. And it allows to address these kind of targets. And with my colleagues at EPFL, we've used geometric deep learning to predict protein binding properties and then construct new proteins from scratch for this called de novo protein design. And we showed, for example, that we can design proteins that disrupt the programmed death ligand complex that is used as target for, cancer immunotherapy. And, this approach potentially could pave the way to a new generation of biological anticancer therapies. So this was a paper that appeared on the cover of Nature Methods, in the February issue this year. And at the high level of abstraction, we can use graphs to model the interactions between molecules such as proteins and drugs as the the protein to protein interaction drug. And, in if you think of, the drug traditional drug therapy, in many cases, we see multiple drugs that are administered at the same time. Medics call this polypharmacy or combinatorial therapy, and it comes with the risks that some interactions of the drugs can produce bad, or even potentially dangerous effects. And, it is impossible to clinically test all the, millions of possible combinations between all the FDA approved drugs, and graph neural networks were shown to be successful in predicting drug side effects or drug interactions. So the interactions are not necessarily bad. They can actually be synergistic. And, I'm part of the the cover coalition that, tries to to develop graph models for predicting synergistic effects of, combinatorial treatments against COVID nineteen. So I will finish with the last example that takes ideas of drug requisitioning to the domain of food. And itself drugs, we can look at drug like molecules that are contained in food and you you know that many plant based foods belong to the same class of, chemicals that are used as as drugs. So it's not surprising that, for example, many, anti cancer therapies are actually, synthetic analogs of molecules that you can find in plants. And we're reading just thousands or, of such molecules, polyphenols, flavonoids, terpenoids, indoles. I'm pretty sure that most of you have never heard of them. In fact, they still remain large and unexplored by experts. They are never tracked by any, any regulatory bodies. So this is truly the dark matter of nutrition. And, with collaborations at Imperial College, we used graph based, ML techniques to discover drug like molecules in food and then, identify which foods, would work the best and hope to prevent or or treat diseases. This is really the first way this somewhat simplistic attempt to use graph neural networks for this problem of predicting health effects of biologically active molecules in full by modeling their network effect. And, in a longer perspective, our ambition is to provide quantum leap in how we, prescribe, design, and prepare our food. And as a conceptual take on this, we partnered with the molecular chef who used the ingredients we have identified to prepare simple, tasty, and, and cheap recipes that you can actually find online. So I think it's a good moment to end on this tasty note and, well, probably we need to come back and see what whatever I have prognosticated here, how much of this has materialized, in the next, few years. So thank you very much.",
  "language": [
    {
      "id": "http://vocab.getty.edu/aat/300388277",
      "type": "Language",
      "_label": "English"
    }
  ],
  "created_by": {
    "id": "https://example.org/cdkg/Creation/presentation/30",
    "type": "Creation",
    "caused_by": [
      {
        "id": "https://example.org/cdkg/Activity/presentation/30",
        "type": "Activity",
        "_label": "Presentation of \"Deep Learning on Graphs: Past, Present, And Future\""
      }
    ]
  },
  "digitally_carried_by": [
    {
      "id": "https://example.org/cdkg/DigitalObject/recording/30",
      "type": "DigitalObject",
      "_label": "Recording of \"Deep Learning on Graphs: Past, Present, And Future\""
    }
  ]
}