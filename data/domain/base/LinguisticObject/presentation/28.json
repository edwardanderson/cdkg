{
  "@context": "https://linked.art/ns/v1/linked-art.json",
  "id": "https://example.org/cdkg/LinguisticObject/presentation/28",
  "type": "LinguisticObject",
  "_label": "Textual content of \"(DataCatalog)_-[poweredBy]-_(KnowledgeGraph)\"",
  "classified_as": [
    {
      "id": "http://vocab.getty.edu/aat/300027388",
      "type": "Type",
      "_label": "Transcript"
    }
  ],
  "content": "Hello, and welcome welcome back, everyone, to Knowledge Connections twenty twenty. I'm Jozana Diotis, and, I'm your host here on the Innovators track. And, we keep we keep pushing through with more great talks. We're here today with, Juan Sequeira and Brian Jacob from data dot org. Who are here to talk to us about data catalogs, knowledge graphs, and how these two are connected, and how you can use knowledge graphs to to power your data. The floor is yours, Juan and Brian. Take it away. Great. Thank you very much, George, and we're excited to be here. So, this talk when when we wanted to have a talk here, Brian and I were thinking about something to be having a a a different bold point of view, and and that's the goal of this talk. So, just a little bit about us. My name is Juan Cicada. I'm I'm the principal scientist at data dot world. I've been in this, knowledge graph space, which goes back to the original semantic web vision for almost fifteen years. This is where I started doing my research, and I have, I did my PhD at the University of Texas at Austin. We spun out our research into a company called CapCenta, which we did, kind of pioneered a lot of the semantic graph virtualization on relational databases, developed Grapho, our visual collaborative real time knowledge graph scheme editor. And and we we sold the company. We're acquired by by Brian's company, data dot world, and I've been part of kind of bridging the world between academic and industry, being on many standards committees, and and now working on the property graph schema working group. And, I'll pass it on to Brian to introduce himself. Great. Thanks, Juan. I as Juan said, I'm the the one of the cofounders, the CTO for data dot world. I I think, you know, maybe my story is a little bit similar, but inverted to one's. I, you know, grad school twenty years ago in AI machine learning, but really have spent the last twenty years, you know, starting and working for a number of of large companies kinda more in mainstream data management. And the idea behind data dot world was to really bring the power of semantic web and knowledge graph technology into that world. And so, that's how I got connected up with Juan and ultimately, acquired his company so that, you know, we can kinda work on this together. So And this is a side note I always tell people. Brian at HomeAway, there that was a company that acquired over thirty companies in ten years. So if there's somebody who knows about data integration, it's Brian Jacob. He has integrated over thirty different systems in just ten years. That's people spend integrating one two different systems in just two years. So to take that in account. So this is this is the kind of your typical example that you see. Right? You have your sources. Right? And you have your data producers, the personas, like data engineers and data stewards. They have all this data from all these different sources. And the goal is to be able to integrate this data, to be able to provide it to your consumers of data, like your data analysts and your data scientists who wanna be able to go answer your questions. Right? And there's this big gap. And the typical marketing or or architecture slides, it's that you put a knowledge graph here in the middle. But in reality, what you start seeing in here, there's very specific boxes. Right? There's things like we need to do data governance. We need to have a a business glossary, a data glossary. We need to have data virtualization and federation technology. We need to do data quality. We need to do data security, data preparation, data lineage, data dot dot dot data, more data data. Tomorrow, there's gonna be more data things that we need to go do. And all these different aspects are needed to be able to go integrate data. So just simply kinda slapping on that, oh, a knowledge graph is gonna solve us here. The devil's in the details, and there's a lot of details in here. And this is the big question is how do you figure out what you need? Our position is that you wanna start on two important pillars. The first is a pillar on metadata management, and the next is a pillar on data management. So you want to be able to start build your strong foundation on top of metadata management and data management, And that is our definition of what a data catalog should be. Now a data catalog wants need you need to be able to go do things with quality and security and preparation and so forth. But depending on your use case, data quality may be more important than data lineage. Data preparation might be more important than data security. So that's why you don't need to really think about boiling the ocean and bringing all these things together. You wanna start on these very important pillars and then bring in the other tools that you need. And how do you bring in these other tools? The knowledge graph is what really helps put to put this stuff in together. Because what we're gonna go discuss here is that because everything is driven by connections, and and and we can start putting connecting all these different systems together to ultimately be able to create your knowledge graph. So I've introduced two terms here, data catalog and the knowledge graph. A data catalog is a metadata and data management tool that helps users inventory and organize and access the data within their systems. And the knowledge graph, I know there are just so many different definitions and stuff, and and and this is what I call Juan's, nonscientific, non pedantic definition of a knowledge graph. You wanna do data integration where you're integrating internal and also external data sources. And second is that you want your real world concepts and relationships to be first class citizens, And those concepts end up being nodes in a graph, and the relations that end up being edges in the graph. And that's a simple definition of what a knowledge graph is. So with that, let me go pass it on to Brian because I want him to go tell us why your data catalog should be powered by a knowledge graph. So I'm gonna start by talking about the relationship in this direction. The data catalog is powered by your knowledge graph. So a graph data model is, by definition, flexible and agile. It's about entities and relationships, and it can be open open world in the sense that you can bring in new entities and relationships. So that as you move across your organization, acquire new data sources, uncover new data sources, you can bring those into the model in a flexible way. Data quality, security, lineage, all of these kind of use cases of of a metadata management system are represented in the knowledge graph as relationships between the between entities. And it's you know, as with any application, it's not that you can't build this application on top of tabular, you know, relational SQL databases. It's that when you think about the kind of expansiveness and the openness of this problem by modeling it at where the entities and the relationships are first class citizens, you end up putting the data, the actual meaning of what you're representing first, and you build a more flexible, usable, and and and powerful implementation to power discovery, search, recommendations across those data assets. And and one thing to add here, if you look at other companies, like the Airbnbs of the world, right, that they built their own data catalogs, guess what they built the catalog on? They built it on graphs. Sure. So thinking about the use cases that we talked about for for data catalog, for metadata management system. For when when you think about the first thing you wanna think about is cataloging your data assets. What data assets do exist and how are they related to one another? This is this could be represented by, you know, by go back one one slide one. This represented by, you know, thinking about those database tables and databases as nodes in the graph and the relationship of containment as as a relationship between them. Going forward to discovery, you wanna understand what data assets are related to a topic, what physical assets like this table are related to, what logical concepts like the concept of order from my business glossary or from my taxonomy of my organization. You wanna think about governance. You wanna understand what agents and policies are related to those data assets. You know, again, here is a simple example. Reuben is a person who is the steward of this database. Notice how each of these statements are a real world use case of something I'd like to understand, but they are but but but, ultimately, they are represented by modeling the entities and the relationships as first class citizen just like in any graph based solution. Provenance. How are data assets related to one another and and over time and and and in a in a derivation, in a in a, you know, in a state change representation manner. And collaboration. What people are related to all the data and analytical artifacts? If I want to understand all of the data the the the data assets in my system, it's one thing to have kind of all the relate all the all the data represented in the catalog, but you also wanna understand who are the other people who have additional information, who have actually where is the prior art and how who else can I contact who has actually touched this data or has has a relationship These things, these are all concepts in the real world? Your databases, your reports, your your your policies and people, they're all real world concepts. They contain information about other real world concepts and relationships, but they themselves are concepts that have relationships to one another and the information they represent. Regardless of how you actually implement it, your data catalog is a knowledge graph. It's a graph of of of of of knowledge about these assets, and it should be the starting point for your enterprise knowledge graph that kind of speaks in the domain language of your business. So stepping through those use cases again. You know, first, I represent that this orders table is part of this particular database. I understand that that orders table is related logically to the concept of orders. I understand that Ruben is the steward of the sales database and that there's a net sales table derived from that orders table and a report derived from that, which was actually produced by by Sally. I all of these individual statements are things that are collected together by introspecting and understanding the metadata about about these data assets. And these give you when you ping them bring them all together, a graph over which you can reason and analyze your data infrastructure to understand where where the where where the actual data and know and information about your real world entities lives. So when you build this graph, ultimately, you need a model that helps you make sense of this. Your nodes are gonna represent concepts like data assets, databases, tables, and columns, the agents, topics, etcetera. The edges are gonna represent things like containment, things like like logical relation, things like derivation for provenance and lineage. And so what you end up what you end up with is an ontology that describes these that this this interconnected network of of data assets, of your of of the of the metadata about your data assets. What we what we have built internally is not told you we call Dweck, but, ultimately, it is the majority of that of that is vocabulary from these open standards. It uses things like DCAP for the basic catalog structure, Dublin Core for basic, metadata about digital assets, Provo for lineage, SCOS for concepts, etcetera. The this is the this is the ontology that you start with to make sense of the data assets themselves, and it builds into the next section that Juan is gonna talk about where you actually then use that as the foundation to build your enterprise knowledge graph that's in the language of your of your business. And I'll just kind of close with, you know, thinking about one specific use case and how this kinda really makes it makes it makes it real, makes it visceral. You know, a a common use case for a data catalog is to understand lineage for things like impact analysis. And when you when you do that, what you're really doing is saying, I want to go into the graph that represents all the ways that all of my data assets are interrelated, and I wanna pull out a subgraph that looks at the looks at all of these things by along the the derived from dimension. I wanna basically pull out all of the the ways that things are derived from in a prominent sense so that I look all the way off to the right here and say, I have these several Tableau reports, and I would like to understand where the data that powers these reports come from. That is a query against that graph that says, give me transitively everything from which these things are are derived. This is just one quick example that, you know, I think is is is a common visual implementation in a in a cataloging and and lineage solution to show you kind of how you what you're doing is really querying across this graph database and pulling out an interesting subgraph to address a particular query. With that, I'll hand it back to Juan. Great. So we the point here is that your data catalog is about managing and understanding your metadata. And if you're gonna do that, it ends up being a graph. So it ends up being a knowledge graph itself. But I know everybody here is really excited about knowledge graphs, and we're trying to learn about that. And our position is that your knowledge graph should be powered by a data catalog. So let's think about it. When within organizations, you don't know what data you have. This is the common problem everybody has. Right? We We don't know what data we need to go search for data, so let's go catalog it. Let's go literally crawl and bring in all that metadata together. But when you do that, you wanna be able to start connecting it together to understand how things are connected, everything that Brian just mentioned. Once you start cataloging that data, you wanna go find the data, the next thing is, like, I wanna go access that data. I found the data that I needed. Right? Regard I talked to somebody. I figured out how things were connected. I now wanna go access that data. Let it be give me access to the raw data because it's being sent to me, or I wanna be able to go query that data. And at this point is when you start realizing that I I I I can go query it using data virtualization technology, data federation technology. I wanna take a table in one database and the table in another database and go join them together. Right? We wanna be able to provide users with that quick and easy access to the data. But this continues to be what we call the application centric view of the data. Right? When when we talk about real world enterprise databases that you're cataloging, you're cataloging databases that have hundreds, if not thousands, of tables and thousands, if not ten thousand columns. And at this point, you want I mean, our position is that you want people to realize that their data is a mess, and you want them to kind of crash into the wall and realize, wait. My data I don't cataloging also the usage of the people, and you start realizing what are the different the most important data, assets that people are querying. You can start cataloging also the queries that people are doing, and you realize, wow. People are querying for orders in all these three, four different ways. And when you start talking to people, you realize, wow. You use the word order, and it means three different things to different people. So you start realizing these things once you start cataloging the data. And then when you start cataloging the data and accessing the data as it is, people realize, I don't understand my data. And at that point is when they realize, I need to add my knowledge on top of that data. I don't want to go write these really complicated application centric view queries to get what is in order. I would like to just have a concept called order and just access the data through that concept and get it. But if we I think a tendency that we see a lot is people wanna jump into this knowledge aspect first. We call this to crawl, walk, and run. You wanna be able to first crawl, catalog your data, walk, go access the data, understand how that data is today, and realize that there there are a lot of pains, and I understand it. And when you start cataloging the usage, you start realizing which are the aspects most important to people. And at that stage, you start you can go go into the running phase, which is let's go create add the knowledge to it and go create that knowledge graph. And by doing this crawl, walk, and run, it helps us focus on what people are actually using and needing, and that avoids boiling the ocean. Because this is something that we have seen over and over again, that people want to jump in and create their knowledge graph, and they start building this big knowledge graph with having a clear view of success. So the crawl and walk is what we call the the data catalog, and the knowledge graph aspect is when you're starting to go run, create that knowledge graph, and then actually using that knowledge graph. That's why we say the knowledge graph should be powered by your data catalog in order to be successful. So to get a little bit more of an example, when you're starting to go catalog, this is an example of a real world enterprise database. Right? This is something that you can go catalog and understand how things are what things exist in the real world. Right? This is in your real world enterprise databases. But this is what you if you give this to your data consumers, right, your data scientists, your data analysts, your BI developers, They're gonna scratch their head because they don't understand this, and that's why you go back and forth with your data data producers to understand it. So you wanna be able to go create your knowledge graph as adding that layer of of knowledge, the layer of semantics. So imagine viewing the data this way. You see the main the the real world business concepts, orders, customers, billing address, order lines. Right? A per customer purchases an order and so forth. You wanna be able to go access this data. Access the data in terms of, in terms of this knowledge graph view. But when you're creating that first catalog, you're starting to come up, for example, with the business glossary terms, and that helps you bootstrap what this is going to look like. Now the big differentiation between having this kind of, the the the the knowledge graph view versus what I what we call the application centric view is, if I have this particular question. Right? What are the orders and their net sales in a given time period per their status? To answer that question, I would have to write one SQL query over that database, and it would be a very complicated SQL query. Something that's much harder to to to maintain, something that a small amount of people can be able to understand because they need to know understand the application database. But when you're starting to talk about what I what we call the knowledge and data centric view of it, your queries I'll be I understand they're in graph and sparkle that may be a new thing, but just think about that. The queries are now in terms of the knowledge view. This it's a data centric view of this. And this everything now starts to have a complete lineage. So you if you're asking what is an order, I have the connection of what that concept order is all the way back to the original application database. That's why you wanna be able to start with the metadata first. So another aspect that I really want to push here is that we all think it's just a technology problem. It's not just a technology problem. It's a that's a fallacy. It's about culture too, and I think this is the other aspect that we want you to take away. First of all, we talk about tools, but what about the people? Ask yourself, what does success look like? How do you know if you're going to build a knowledge graph you're being successful? Who is responsible for the data? Assume that, you generate data and and the data is being used for to run some machine learning models to go make a decision, and that and the decision that was made was a failure. It it sales went down. Who's responsible for that? On the contrary, what if sales just increased dramatically because of the data that was generated? Who's responsible gets promoted for that? And, additionally, who does the actual knowledge work? Who understands what these concepts are and understands how they're supposed to what all the different meanings and how they're supposed to be connected to those physical databases? I think this is this is this is again, this is not a problem solved just by technology. This is understanding the people who are involved, understanding who is actually going to be consuming this data and asking them what success looks like. Therefore, we need to be able to understand what are the business needs, the business problems, and tie everything that we're doing with the data to business value. Otherwise, we're just having we're we're we're defining success from a technical point of view. And I think this is one of the key problems. We have been in the same kind of rut in in enterprise data management because we define success from a technical point of view and not from a social business point of view. Bringing the data into one place, into one lake does not solve my problem. It solve the problem is if the business users are able to go answer their questions. That's how we need to go define success. Who is responsible for the data? We we treat software with the respect it deserves. It has a team. It has a manager, a product manager that communicates with the with the with the users of the software to get the requirements and translates those requirements to the engineers and defines tests. And we don't do that for data. We need to have this notion of a data product manager, and they're the ones who are managing the this the datasets that are coming out of that. And who does this knowledge work? We all confuse we're all confused, and then we say that the data scientists, right, are the ones who's supposed to do it, and they complain of the eighty twenty rule. Right? Eighty percent of the data scientists spend their time cleaning the data, and then only twenty percent of the time they do the analysis. And that's true. But now we treat this eighty percent almost with disrespect. We say it's a data janitorial work when it's the crucial knowledge work, that knowledge of what things mean in our company. So this needs to be kind of we need to separate it and make it into its own diff own, role and what we're calling the knowledge scientist. Now people work together, and the question is, how do they work together? What are the processes that people need to go to? How do you avoid avoid boiling the ocean? We don't wanna have the old school waterfall approach. Yeah. How do people work together? Right? We need to have agile agile methodologies, what we're calling agile data governance. And one of the things I always tell people is, like, why do we have brakes in a car? And the immediate response usually is is so we can go slow. It's like, but no. We have brakes in a car so we can drive fast safely. And that's the mentality that we need to go think about when we're thinking about data. And how do people work together? The data product manager needs to set up sprints. We need to think about how to document data. We would never dare release code into a master branch without any comments. Why do we do that for data? We don't we don't document it. We don't document how it's being transformed, the transformations, the queries. We don't do that. We would never dare right now to go release code without anybody, code reviewing it. Right? We do pull requests. People go review our codes. Right? We we even do peer programming. Why don't we do that for data? These are the these are the social aspects that we need to go consider. So to take away, frankly, I think it's just time to transform enterprise data integration. The way we have been integrating data has been the same. So your data catalog powered by a knowledge graph. Your first knowledge graph should be of your metadata, of your tables and columns. Why? Because you need to know what data you have. And by know means you need to have a knowledge. That's what we're talking about. The knowledge graph of your first knowledge graph should be should be of your of your metadata. And then your knowledge graph should be powered by the data catalog. To build your enterprise knowledge graph, it needs to be done to solve business pain points. And I know people are gonna be so tempted to run, to jump and start creating their knowledge graph. Please don't do that. Why not? Because you are basically diving into the deep end, and you will most probably drown, meaning you're probably gonna fail. And what's gonna happen is that when people start looking at this knowledge graph, they're gonna ask, where does this data come from? How do I know I can trust it? And the answer to that, it's the metadata. So that's why you should start with the data catalog as a strong foundation of metadata. Crawl and walk first in order to succeed with your knowledge graph to run. And I know Brian has some final words here. No. I think yep. Just just just as you said, your data catalog is a knowledge graph. That knowledge graph is actually going to be a fundamental and critical part of your enterprise knowledge graph of your enterprise knowledge graph. The the knowledge in your knowledge graph isn't knowledge if you can't kind of state where it comes from. And that is exactly what having a model of your of of your data assets, the metadata about your your your data, your policies, your organization, and how it all hangs together. It is it is it is the foundation of your knowledge graph, and it is fundamentally a part of an integral part of having a a real and powerful knowledge graph for your enterprise. And and one final point is we have to be very honest. We're people who are here who are attending this the this this conference, we're all very excited, but we even though we see Gartner and we see where they're putting knowledge graphs and all that stuff, this is frankly still very early. Right? People are not companies are not knocking on their door on on and say, I want a knowledge graph. They do not have a budget item, the budget line item that says go buy a knowledge graph. They're not there yet. So knowledge graph is still new in the industry. There needs a lot of education for it. So we believe that a way to get knowledge graph in is just focus on a data catalog powered by a knowledge graph to sneak in the knowledge graph within an organization. The value of a data catalog is evident right now to the business. Right? There's magic quadrants for data catalogs. Right? There is a line item to go buy a data catalog. The market is asking for a data catalog now. So if you wanna help the knowledge graph industry grow, start with the data catalog powered by a knowledge graph. And with that, thank you very much. I think, George, you're you're on to moderate questions? Yep. Yeah. Yeah. And, actually, you've been quite active in responding while, you were giving the talk, which is pretty unusual, but also a little bit unfair if I compare to the other people who gave talks because there's two of you. So well done. And, CEO also started the conversation with, Dean Alemanc in the comments, which I've been following with with great interest. And so I'll just relay, the insulators comment, basically. So he's saying for the benefit of, the rest of the people who may not, have the chance to, to to follow the conversation. So he's asking, how do Juan's comments about data governance relate to the fact that we'd like to reuse data from outside organization? We don't know if the external data has undergone the governance that he talks about. Does that mean that we can't use it? And he he hopes not. And it's it's a long comment, and it it includes a number of questions, actually. And then it he concludes by asking how do we reconcile the need to manage data in the way Juan is talking about with the desire to use external data. Yeah. I mean, you know, I can jump in on that. That's what I I think it's a it's a great question. And, you know, I think, you know, the answer is you can you can represent that external data as as concepts and entities. You can represent what governance has taken place if you know about it externally and what governance may have taken place on that internally. Right? You know, the so, folks, something we could ask a lot is, like, how do we deal with data quality? How do you kind of know that only data only quality data is within the within the catalog? And the answer is, well, we we don't, but we do in the sense that data quality isn't an absolute isn't an absolute thing. Right? It's always kind of use case and need dependent. So what you can do is you can represent facts about data. What what what assessments have been made? What ascertainments have been made about this data? And then within a particular context, you can assess you can assert policies, which are what ascertainments need to have been made about this data in order for it to be useful for this this use case. And so the same piece of data might be completely usable for some kinds of research courses purposes within your organization. It might be totally unusable if you're putting together, you know, your s a c SEC filings because it hasn't gone through proper proper vetting. That's not two different pieces of data. It's it's not two different it it is two different ascertainments of quality for different purposes. And and, you know, third party data is is absolutely, you know, part of part of your catalog. You just need to understand where did this data come from and what what governance has has been been you know, I would say, though, like, if you know, to to kind of, like, back to the framing of the question, if you just bring in a catalog of here's a bunch of open data that we haven't really looked at or or or scrutinized or made any ascertainments about, you probably shouldn't use that for most for for most critical use cases because you actually don't you don't actually know anything about about the quality. So you have to really kinda have a what factual quality as ascertained to the name and how does that and how does that relate to the policies of use that I have? So so a couple of things I wanna add. One is you first need to go catalog your own stuff to go know what I have before you wanna go bring other things. Otherwise, you're just, like, throwing stuff in the wall and see what sticks. Second, this should be use case driven. Right? Why why are you going to bring in this other data? What is the problem that you're gonna try to solve with that? So I think that's how you wanna be able to go bring understand that. And then once you realize that I have this particular use case, this is the problem I'm gonna go solve, you're gonna bring in that external data inside, and and then you're gonna have to add the governance to it and figure out, well, can I actually use this or not? Right? I don't I mean, we we're so kind of entrenched now with these magic wands of AI and stuff. It's like, you put people in the room, they won't even agree. Why the heck do we believe that AI is gonna come up with the right answer when the humans won't even agree? We need to stop thinking about that for for for I mean, we need to kinda really put our our feet on the ground on that aspect. Now the I think the the way the market is going to get involved, and this is much bigger than knowledge graphs, is a data marketplace. A data marketplace is going to be a data catalog of your internal and your external data. And you're gonna see and so you're you have data catalogs for for folks to go use it internally in their in their enterprise, and we're gonna start seeing data catalogs of open data that are really starting to add that governance. And guess what? So I think the winners are be are the ones are the data catalogs that are gonna be able to do both. So take a look and watch out for data dot world for sure. And I think I'm looking at one of the chats there, from Janine Hill asking about, the somebody was asking about the ontology, that that we that we were talking about. Frankly, the ontology we have is is is out there. Right? It's it's it's DCAD. It's DC. It's it's all that stuff. We internally have taken all that stuff and added some glue together. Right? We need to go connect that stuff. And I I think part of our plan is to, we're cleaning it all up, and we just wanna go release it. So we will be open sourcing that the Datadog World enterprise catalog. But note that it's all open standards plus a bunch of glue that we just added to that. So we're gonna I think that's something valuable that the that the industry needs. Well, we can't We don't have a paper that discusses the glue, but we probably should. That'd be a good that'd be a good thing for us to do and prepare for the engagement. I am looking at that. We'll get we'll have that published soon. Yep. Okay. So thanks to both of you. So thanks once more, and we're wrapping up, and, see you in a bit. Thanks.",
  "language": [
    {
      "id": "http://vocab.getty.edu/aat/300388277",
      "type": "Language",
      "_label": "English"
    }
  ],
  "created_by": {
    "id": "https://example.org/cdkg/Creation/presentation/28",
    "type": "Creation",
    "caused_by": [
      {
        "id": "https://example.org/cdkg/Activity/presentation/28",
        "type": "Activity",
        "_label": "Presentation of \"(DataCatalog)_-[poweredBy]-_(KnowledgeGraph)\""
      }
    ]
  },
  "digitally_carried_by": [
    {
      "id": "https://example.org/cdkg/DigitalObject/recording/28",
      "type": "DigitalObject",
      "_label": "Recording of \"(DataCatalog)_-[poweredBy]-_(KnowledgeGraph)\""
    }
  ]
}