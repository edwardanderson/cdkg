{
  "@context": "https://linked.art/ns/v1/linked-art.json",
  "id": "https://example.org/cdkg/LinguisticObject/presentation/18",
  "type": "LinguisticObject",
  "_label": "Textual content of \"UBS Helix Knowledge Graph: The Power of Information\"",
  "classified_as": [
    {
      "id": "http://vocab.getty.edu/aat/300027388",
      "type": "Type",
      "_label": "Transcript"
    }
  ],
  "content": "Hello. My name is, Natasha Varitimou, and I'm a data architect in UBS. More specifically, I'm the modeler in the UBS  knowledge graph. So, we will discuss today what were the issues and requirements that we had in UBS and how the UBS knowledge graph facilitated the way that we deal with these requirements. So, every organization in order to increase its revenue has two ways. One way is to increase its customer base, which is something that every organization is trying to do, but you cannot do it multi fold over time. The second way is to decrease its debt. So the question that we had in UBS is how do I decrease my debt, and more specifically, how do I reduce my debt in the IT world, in the technical landscape? So in order to do that, we had to answer some questions. What are my applications in my IT landscape? What is the software and hardware they leverage? How much does an application cost me? How can I reduce this cost? Do I invest too much money in applications that I should be decommissioning? There are a lot of questions where the answers were not that straightforward. It wasn't easy. It wasn't we couldn't give answers immediately. They were it was very time consuming to actually search for the data that will actually help me facilitate me to answer these questions. That is because our IT landscape, the view for our IT landscape wasn't very clear. The data that would help us answer these questions were hidden between behind different applications, so they were hidden in sign laws. They were named differently. We had many duplicated information from application to application, and, of course, we had redundancies. And, all these questions, these challenges were not only we don't, face them only in UBS. I have seen this is actually a data definition and data integration problem, and I have seen it in many organizations and independent the size. I've seen it in many sectors. I have worked in governmental, projects, in my previous roles, governmental projects in Brussels. I have worked in oil and gas projects in Norway. I have worked for life sciences and pharma. I've worked for consumer goods. The latest years, I have been focusing in the financial sector, but as I said, independent the sector, independent the size, independent the project, the challenges were always the same. The data questions were always the same. So the solution, in a few words, is federated and model driven layered architecture, and this is what I'm going to present to you today, how we solved how we created this in UBS. So So I'm gonna structure the presentation by breaking down the data challenges one by one and giving and presenting the solution that we actually gave for each challenge. So the first question, and it seems very simple, while it is not is what is my data? We need a consistent description and representation of the data. Sometimes the the same real world concept in the same organization, but in different parts of the organization or maybe in the same part of the organization between different teams, the the same real world concepts are called differently. So some people might call something a system, for example, and then other teams might call it infrastructure, and other teams might call it, hardware. So we don't know that we indeed, we are talking about the same thing. We need common terms to the organization . We need descriptions for these common terms, and we need to know what is the what are the relations between them. So what we actually need is a model. This is what a model brings to the table. So we need an enterprise agreed model so everybody would refer to the same real world concepts the same way, and we need to document it in a model. Now, if this model is a UML model, is a model documented in a picture, in a conference page, in a document anywhere, then people could understand and see this model and maybe see the descriptions as well in the relations, but machines cannot understand it. So machines cannot answer the same questions that people can. So in a question, a person could go and search based on this model, but the machine cannot a software agent, an application cannot search for this, based on this model. So we need machine readable models. So the same way that a person, a human can understand it, a machine or an application can understand it as well. And we also need the data to live together with these machine readable models. This is what Tim Berners Lee called knowledge graph. Machine readable models and data living together so everybody can search for the information they need, not only, as I said, humans, but also applications and software agents can navigate through these models, get the data they want, can discover new information, follow their nose, and, do more complex queries. Right. Some organizations have, reached this level of maturity, but, at least to have an enterprise agreed model, but just a few and probably, UBS is the first that is doing it in such a large scale. We are doing it actually we have a a knowledge graph in the middle of our architecture as you see here in our in this, diagram. So in the middle of this pink part is actually our models, which are we call them upper ontologies, which are normalized, consistent, clear descriptions of our data. Actually, we have gone one step further. So they are not just ontologies. They are Shackle models, which is Shackle is a a state of the art, modeling language where you not only describe in a machine readable way your model, but you actually can use this model to validate the data against this model and report some inconsistencies, if any. So in this middle layer in our knowledge graph, we have our upper anthologies, and we have our data living together consistently. In our upper ontologies, in our upper shackle models, we are trying to utilize as many standards as we can so we will, we don't have to reinvent the wheel, and we'll also, succeed interoperability. So we are utilizing we are using as much as we can Dublin Core. We're using consents from Fibo. We are using, Prov for provenance. We are using TCAT to describe datasets. We are using DataCube and SDMX to describe statistics, metrics, and dimensions. We are trying to reuse as many standards, especially w three c standards, as we can. As I said in the beginning, we began by trying to describe our technical landscape, so we needed a technical ontology. However, this was, surprising, I would say. We couldn't find something to reuse. There wasn't a standard technical ontology out there that we could just reuse and extend if necessary. I see surprising because every organization has an IT department. I I suppose that every organization wanted to wants to describe its applications, its how hardware, its software, all the cloud capabilities they have, how they are all tied together. However, as I said, we couldn't find any, so we created our own ontology. So that is our model layer in our architecture. But then the second challenge is how do I bring my data in and describe it with these models? Where is my data? Where is this data coming from? How do I track their provenance? How do I track how the data flow, in order for me to have a lineage, aspect on my universe? For example, if I change something in my data, what does it affect downstream? Or if I have a change somewhere, what does, if I have something, how do is it affected from any chains they do upstream? This is what we call lineage. So this is a second this was the second challenge. So the the solution to this one is federated architecture. We don't have we we utilize microservices to actually bring the data in from its data sources. The data sources themselves are also described very clearly with taxonomies. We have people and roles around these microservices, so we have a well documented, ecosystem of each data that are coming from this microservice. Again, for the microservices and for our publishing, we didn't create a wrong technology. We didn't use a proprietary technology. There was again, we didn't reinvent the wheel because there was there were technologies out there, very well known ones, tried ones, and guaranteed. It was the HTTP protocol that is used in the web. It was the URI's URI techniques. It was, as I said, vocabularies like d cat where we will describe the datasets as they were coming, as you see here, from the different publishing microservices to our knowledge graph. And, of course, we we also utilize Prove. So every time a bunch of data, what we call dataset, is coming in the knowledge graph, we always track and we see an example here. We always track what this dataset is, what are the persons, and what are the roles around this dataset, and we always track where they are coming from. So a dataset might be generated by an ingest activity coming from an original source, from an application, but it could also be generated by a transform activity derived, as you see here in the example, from two other datasets. These datasets, in their term, are generated by a transfer or by an ingest activity. That way, we, track the lineage of our data with datasets being first class citizens. Third challenge, I have now a very good architecture. I have my data very well published, described, consistent, and, with usability and, guaranteed data quality. How do not, how can I now use this data? Different teams have different needs on the data. For example, there might be, one part of the organization, one team that would, need just a specific glance, a specific lens on the data and not see the whole universe, a knowledge graph can be, intimidating sometimes because it has a lot of information. So someone would just want to see just a lightweight, simple, view of the data. There might be other teams that want some of the data, so cherry pick some of the data, and they want it in specific formats, other teams in other formats. So how do I cover all these cases? We have different ways to deal with these challenges, so our ways vary for facilitating different, extension lighter models or our on our, bigger, fuller knowledge graph to having, different types facilitating different types of queries and rest APIs. So in our architecture, this is described here in the upper layer. So as you see, although this middle layer, our knowledge graph creates the integration platform, we can have multiple ways to interact with it. That will vary. We have a a very nice UI, so it's like a a web browser search on our knowledge graph. So someone could just, put a term in it. And a lot of links appear where you can follow your nose and discover more information just like you would do, in a search engine. We have a a Quibi interface, a sparkle endpoint, so someone could write very, more complicated queries. And when you save them, they are immediately saved as REST APIs, so they could be, invoked from any other application or invoked from any user in the UI and see the results in the UI. We have, what we call guided search where it is again a web browser, just all the other browsers, but you can, you can cherry pick specific properties, specific information on your on your concepts, to see. So, actually, under the hood, that creates a sparkle query, but it this is under the hood, and guided search provide to users that are not familiar with SPARQL. Most of the users are not familiar with SPARQL, an intuitive way of creating complex queries in the knowledge graph. We always we also have, of course, free text search, which will make more efficient, based on elastic search, and we are going to create a faceted search. So someone could search and filter based on specific properties on the concepts. This is the more, direct way to interact with a knowledge graph. Sorry. Go back. But, there will we also have other teams that they interact with a knowledge graph, with a consistent integrated middle layer that we see here. They interact in their own way based on different the different ways that I said before. But, also, there is a way, and I think that in the future, it would be more obvious as we bring more data in the knowledge graph and we have more use cases. There will be the need of lighter, extended models, as we call them, where the data are also described with these lighter models, and we need to transform the data from the fuller, more, heavyweight knowledge graph to these lighter small pods. Of course, the trick here is, always be synchronized and consistent. That is the trick. And, otherwise, you can interact in many simpler ways, but the with the middle integration layer. There are also, though, times, and this is the last challenge, that, you cannot bring your data in the knowledge graph. This might be for time or priority or resources restrictions, but it is not, always possible immediately to publish the data in the knowledge graph. In this case, if I cannot integrate the actual data, what I do is I can integrate the metadata and that is what we call dataset registry. So dataset registry, I have seen it with different terms in different companies. Other companies call it metadata repositories, metadata registries, dataset catalog, data dictionaries. What they all mean and what we actually do in our dataset registry is in order to achieve, again, the integration and facilitate integrated information across our data sources when we cannot bring the data itself in the knowledge graph, we are bringing the description of our datasets. So we are not bringing the datasets in the Knowledge Graph, We are bringing their descriptions saying that this is the physical structure. A dataset actually although you don't see for example, if it is a tabular dataset, maybe we don't see its rows, but I'm going to describe and say that this dataset has, actually, these tables and these columns in it, and I we map these tables and columns, these physical structures, again, to the enterprise agreed logical model. That way, you can query over your universe if you manage to map it to the logical model. You can query your universe just as if data were in your knowledge graph. However, in this case, just because you don't have the data, you will need a resolution service here to say, what is it that you want? Do you want applications in software and hardware? These datasets have them in. We can see it from its physical structure, and this is how you can get it. But you will need now the resolution service based on these instructions to actually go and get the data. So our architecture in our dataset registry is actually to describe the datasets again, their physical structures, the way they map to the logical enterprise agreed model, and, that way, you can actually get the information again that you need. Just want to emphasize again here that dataset registry is a very nice first step to integrate your universe, but it is just the first step. It just just a small part of your entire knowledge graph. And the biggest benefits, the greatest arch data architecture that you can have is when your data is in your universe. But as I said, sometimes you have to do things, step by step, and the dataset registry is, for certain, a very well way to start, getting to integrate your, data landscape. So, we are actively working on our dataset registry, which is the catalog of catalogs. We are bringing we will we will bring all datasets in, and we are going to have an explorer where someone could go and search the datasets, see its usability, and structures, in the same way that, our explorer would be the same way that Google had, its dataset explorer. We are also using d cat, to describe our datasets, but we extend it, as we're saying, to describe their physical structures and their mappings. I think that is, what I wanted to share today, and, hope, I gave you an overview of the federated and model driven layered architecture that we are establishing in UBS. And, looking forward, for stimulating discussions in the context of the conference. Thank you very much, and, thank you.",
  "language": [
    {
      "id": "http://vocab.getty.edu/aat/300388277",
      "type": "Language",
      "_label": "English"
    }
  ],
  "created_by": {
    "id": "https://example.org/cdkg/Creation/presentation/18",
    "type": "Creation",
    "caused_by": [
      {
        "id": "https://example.org/cdkg/Activity/presentation/18",
        "type": "Activity",
        "_label": "Presentation of \"UBS Helix Knowledge Graph: The Power of Information\""
      }
    ]
  },
  "digitally_carried_by": [
    {
      "id": "https://example.org/cdkg/DigitalObject/recording/18",
      "type": "DigitalObject",
      "_label": "Recording of \"UBS Helix Knowledge Graph: The Power of Information\""
    }
  ]
}