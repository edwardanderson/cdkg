{
  "@context": "https://linked.art/ns/v1/linked-art.json",
  "id": "https://example.org/cdkg/LinguisticObject/presentation/29",
  "type": "LinguisticObject",
  "_label": "Textual content of \"DBpedia Databus: A platform to evolve knowledge and AI from versioned web files\"",
  "classified_as": [
    {
      "id": "http://vocab.getty.edu/aat/300027388",
      "type": "Type",
      "_label": "Transcript"
    }
  ],
  "content": "Is one of the biggest and more important knowledge graphs around and has been for the last decade or so, if my memory serves me right. It has been a knowledge graph, before actually knowledge graphs were were to hike or before even they were to think. So they started, scraping knowledge from, from Wikipedia. And to do that they used some structure in Wikipedia and by doing that they extracted a big amount of knowledge and they have been building on that and creating a whole ecosystem around it. And this is actually what Sebastian is here to talk about today, the latest addition to this ecosystem and how they're using it and, I guess, how they you can also use it. So that's that's it, from, from my part. And for the rest, Sebastian, the floor is yours, and take it away. Thank you for the introduction, George. So today, I'm talking about the d DBPedia data bus, in particular, a platform to evolve knowledge and AI from version web files. This is a particular particular aspect which goes into data and knowledge engineering. So it it is it is about the engineering methodology. A part of it is about the engineering methodology behind it. So you would get some some knowledge engineering details and processes here, and then about the platform which provides the tool support for this methodology. So for for all of you who do not know DBPedia so well, I've prepared a short introduction. So starting from the left, you can see that two thousand and seven was this first Wikipedia extraction thirteen years ago with the four partners, and there was a functioning sparkle endpoint, which is stably available publicly till today, and also linked data deployment. Then soon after, basically, in the same year, there was this formation of the linked data cloud around DBPedia. Trend continues, and, so so then there was a major boost in knowledge graph and linking research. We opened the editing of the DVPD ontology in two thousand ten, which was labeled a new type of psych. Two thousand eleven, there was major industry so IBM Watson was built, for example, using DVPD. Yahoo used the the software. BBC included the identifier. Park was given to Unicode. Then, two thousand twelve and two thousand sixteen, we extended this extraction framework to cover all hundred to hundred forty Wikipedia's, comments, and also wiki data. And at this time, fourteen point four billion facts were extracted. In two thousand fourteen, we founded the DBPedia Association, which I'm the CEO of. Two thousand sixteen, we had we were try starting to tackle data quality on a larger scale. That's why we developed the Shackle web standard, which allows test driven knowledge graph development. Two thousand eighteen, this, the linked data cloud has grown a lot. And, starting two thousand nineteen, we really built DBPedia into an innovation platform. So it's not not really about the dataset anymore, but about the connect connecting of the data, linked data technology, and the ecosystem. Now in the future, we we kind of like raise the the productivity. I'll talk about this. So there are twenty two billion facts per month. This is one thing, so it's monthly releases. And we have this huge link data derived open knowledge graph, so we kind of, like, increase the speed of integration very much and are able to produce this huge knowledge graph. And also, we started to do FAIR link data. So FAIR, most of you might know it, but it means findable, accessible, interoperable, and reusable. And I'll talk about these developments in twenty twenty on the next slides. So what does FAIR linked data mean? FAIR originally comes from a scientific consortium. So there are guiding principles for scientific data management and stewardship. Personally, I always wondered these these guidelines, they are very vague, high level, idealistic, also altruistic, and not at all industrial. So what we're doing with FAIR linked data now is that we provide a practical practical implementation of FAIR. So there's lowered effort because you can reuse the implementation. We have measurable automated FAIR tests, so you can be sure that your data is FAIR, and the whole thing is industry driven. Now this, there are many things in this FAIR guiding principles that are industrially relevant. For example, findable, there is also a marketing aspect, but also internal data management aspect that you want to find data in in your enterprise. Yeah. Because it's very distributed across your enterprise. Interoperability is, of course, a great issue and reusability is is a great cost saver. Accessibility, goes along with privacy concerns and who's allowed to see data. So this has a different flavor than the the original FAIR guiding principles. I would like to introduce you the DVPDIA Association members up to now. So we are very fast growing knowledge engineering and linked data lobby. Part of it is to to give the visibility that the EPR has back to the people who contributed and build it and really drive this knowledge graph adoption. Let's I I would like to answer the question whether DBPedia is academic and is industrial is often, quite a misconception. So it's it's neither and it's both. Because we focus on knowledge engineering, and engineering is always taking scientific methods to produce industrial output. So more on the left side, we put the academic and nonprofit and public members, and then on the right side, we put the industrial members. Yeah. So you can, kind of, like, see this this, knowledge engineering methodology here or process here. I'll talk about some, economical concepts just to give a background motivation why why we are doing this. Some years ago, we had an aligned project that was supposed to align software and data engineering. We also published a book, Engineering Agile Big Data Systems. And if you do a system analysis of your data intensive workflow, you have these three properties which are agility, productivity, and quality. And here I've plotted basically the release cycle of DVPDR in two thousand sixteen and two thousand twenty. So two thousand sixteen, we had kind of like this entire pattern. It is called data quality creep. So we were always focusing on data quality and totally ignoring agility and productivity, which means if you if you follow if you go into this entire pattern of data quality creep, your releases will be delayed a lot. And this means you you use agility and you use, net output. Yeah. So productivity. This is an automatic triangle here. We lost a bit of quality because, of course, we we reengineered the whole pipeline. And during this basically, during this refactoring process, I think there are minor new bugs. Yeah. So we lost a bit of quality, but we kind of like increased agility and productivity a lot. So the last release cycle was seventeen months, and now it's monthly, and you can really add fix the pipeline better. Another economic concept is the law of diminishing returns. So if you have an economic background, you already know what this means. This measures the productivity per unit. So if you add more workers to, so it's the production factor is labor here. If you add more workers, then you, at some point, you're you're getting more productive. But then if you add more workers to it, you kind of, like, have diminishing returns. That means each worker is not so effective anymore. And then if you will continue this, let's say you want more and more data quality and you add more and more people checking this data quality, you will even get into negative returns. That the main reason here is that data quality is Pareto efficient. So there's a twenty eighty rule applies. And if you increase in quantity, you lower quality as well. And also the increase in quality makes it harder to find errors here. So as a take home message, if you ever print the slides, you can cut this out with the scissors. If you want, you you should always try to tackle data quality issues with innovation, not with more community or workforce. Yeah. So because of if you double the size of your data curation community or you have more edits or more activity, it doesn't mean that you are more productive in any sense. So this is basically the take home message and we have we focus here on innovation. Now I'll come to the core. I'll introduce some data bus concepts and, let me see what's that. No. Data bus concepts and use cases. It's in the mix, so there will be high level concepts, and also use cases and practical demos, which show these. So what's the data bus? Data bus is a digital factory platform. The basic process on the DataBus is you find data, you consume data, you process data, then you republish it. This is a very minimal, minimal building block. There's this online platform, data bus dot d b p dot org, and it's literally built like a like a data bus in in your laptop, for example. So in computer architecture, bus is a communication system that transfers data between components inside a computer or between computers. And here, of course, we're talking in the in the server. It was inspired by the Maven repository, Steam, GitHub, and also the APT package manager on Linux. So these are all software deployment systems, and we focus you on on data data consume and deployment. What you do with this basic building block is that you can build networks, and not a pipe not a static pipeline. So as soon as you, build pipelines and really hard integrate, let me see. Another question. As soon as you build hard pipelines, you lose a lot of flexibility and agility, and you cannot test the individual data output. So what you see here is that the metadata, on the top the top blue line is the data bars. You query the metadata for the files, and then you download the data from from somewhere else. So it gives you kind of like the location of the files on the web, and then you can load it into your data cleaning service, into a service, into some sort of model. And you produce output data, and this output data, you can upload the metadata on the data bus again. So it's basically orchestration of input and output between software. So how can you imagine a digital black factory platform? On the right, you see a warehouse with a lot of boxes. You kind of, like, need to know what's in the boxes, where they are going, which box you need. So the boxes here are files. So it's the registry of files on the web, which means that, any files reachable via HTTP, can be described with this metadata. Doesn't matter with the whether it's RDF, XML, JSON, PDF. It's the central storage, and the central metadata index is available via Sparkler API. So there's a knowledge graph as well. Right now on the public platform, there are a hundred thirty thousand files and two point six terabyte, and you can download all of it. So there's no restriction. It's all open. It's open data. Maybe you don't want all of it, then you need to select it and drill down with the SPARQL API, and it's also a FAIR implementation. So if you publish files on the data bus, we guarantee that you, follow the FAIR principles. So what is the core of it? There's very strict metadata. So this part is very well, well checked. So provenance, we allow users, organizations, and agents to publish data. And, of course, you have, datasets, source datasets. When your dataset is derived, you can link to the source datasets. Yeah. So this provenance, machine readable licenses is required. Everything is signed with a private key and the x five zero nine certificate. And we have, dataset identity and versioning, which is a bit different from DCAD. So there's a gap in the DCAD standard. Here, you the dataset is abstract. And then each version you have versions per dataset. Yeah. So I'll show that in the next example. Those structure, like the links, are user group artifact version. If you know Maven, you already, you're familiar with this. And I'll show you one popular dataset of DBpedia. So this is just a very tiny dataset. So these are geo coordinators extracted with mappings. That means from the English Wikipedia, you have a lot of from, you know, from all Wiki from from all major Wikipedias, it extracts the geo coordinates and put them into the file. You can see if you go on the versions that so this is the abstract data set. Yeah. And then each month, we produce a new version based on the Wikipedia dump of this particular dataset. You have a a schema, basically, for the files. This is important. So you can drill down on the individual datasets. So now you just want to select the English files. And, if you want to subscribe to it, there is a query generation functionality. So each month now, if you execute this query, you will get this this particular new dataset, just the English version of it. Yeah. If you click on a version here, there's more there's more much more documentation available and also links to the codes, and you can report errors and discuss. So there's interactivity features. There's also the link to download the metadata. So this means you can automate these downloads totally. The next thing is so if this gets more complicated because we are, the the core collection, which I will show, it has several hundred files per month. So here, users data bus users can create their own data catalogs, which we call collections. So these collections are stable they have stable identifiers, so you can put them in a scientific paper, for example, and they are dynamically updated or they can be static. Yeah. So I'm going to show you the latest core release here, which is basically what we'll then load into the main sparkle endpoint, which you all know. So here's a simple batch script to fetch the data. I hope loads loads now. Yeah. But anyhow, the the collection is backed by a query, which is can get quite long. Yeah. So the sparkle endpoint endpoint is quite good to handle this. And let's see. Yeah. So here you can see all the the datasets. Basically, each month or each day, you can check whether there are updates to this collection and then kind of, like, replicate the DBPedia sparkle endpoint on your local machine. And you cannot create your own collections via the the if you are registered. So what can you do with the collections? There is this thing which we call, it's like rapid application deployment because we need to maintain the DBPEDI infrastructure, the core infrastructure. So as I showed before, these collections can be loaded programmatically. And if you pair this with with Docker, you get a low code application deployment. So we have this Docker image, which is a virtual sparkle endpoint quick start. And, here, it is a three liner. You just feed it the collection DPPR, and then you need to restart it and it checks the updates for the collections. Now let's talk about so this was the download and action part. Now I'm going to talk about this data engineering in action. So, this part, I I do not have detailed slides here, but kind of like, if you ask what is an application and, of course, I mean, data intensive applications here. So AI, search, many things that are fed fed by a lot of data, chatbot maybe or question answering systems, NLP as well. So you have an application, that you are developing, and this application here always needs software and data. So these are the two components that you that you actually need to have a working application. Now if you think about the software engineering part, there, you have very good frameworks for it. You have Maven, you have NPM, you have the DBN package manager, you have, Scala build tools, the SPT, for example. And there you can really pinpoint, the exact software dependencies that you need. They're recursively resolved and you know kind of like for each software artifact you include, you know exactly which version it is, whether there are compatibility issues, whether things changed. You have a build tool that runs integration tests normally. So there are many, many tools there. But when you when you think about data, so if you present your application to somebody and I would ask you, can you give me a list of datasets included and where you got them and, like, a record on what's what are the dependencies and everything, then normally, this is done without any tools. Yeah. So and here I see the great, great gap. So I prepared some homework for you also. If you compare your software engineering processes and tools with your data engineering processes and tools, so how you track data and develop data and everything, do you notice a difference? And you can think about it, and you will see that there's a or I assume that you will find a great gap. Yeah. Because from my experience, I was not so happy the last ten years working with data and the tools provided to work with data. Now the main thing also may the main incentive to for us to develop the data bus is that we want to do knowledge graph engineering. This means, in this case, it's a simple example. For us, it's a simple example. So it it mainly shows the dVPDR release process. And we have this fully automated, it's in the Marvin port, so there's fifty five gigabyte per release, around twenty twenty one billion triples per month and five thousand triples per second. There's also a dashboard that tracks the process. And on the bottom, you can see the the the, debugging well, release and debugging workflow. So the the on the left top, you see that we, we take the time based Wikipedia dump each month, and then there's a time schedule extraction. This is uploaded on the data bus. And then there is the state data cleansing step, which is another component. And in the end, there are this downloads it and republishes it as quality control DBPR release groups. What we do then is, there is this large scale validation. So we run yet other tests on the final thing, and this produces reports. And here, this is the kind of like the point of truth is this community reviewing. Yeah. Because we just produce the data, but then it gets downloaded six hundred thousand times per year, and the errors are only shown in the final applications. Yeah. So if you load it in the sparkle endpoint, if people parse it, whatever. Yeah. So and then the the problem here is always to backtrack, the errors and the issues to the specific component. And this is something which is possible in the data bus because we reach this unity between data package and also piece of code and software to which will which is responsible for this particular data part. And you can break down the the tests these data tests. You can put them into the software integration tests. So here, we have kind of, like, achieved this alignment between software and data engineering and the backtracking of the of the data issues, in the opposite direction of the data. Now here, I'm go just going to show this real short. So here's a more complex, knowledge graph engineering workflow. It is more complex because it's iterative. Because here, it's not so straightforward. So it's not like it's not pipeline like a pipeline. So the release workflow is more like a pipeline and then you backtrack for the errors. Here, the individual components use datasets of other, components, but then also improve them, and, you have to reload the other components. So it's like an iterative approach. So the the borders between consumption and publishing are are kind of, like, not so not so linear. Yeah. It's more network like, and this is only internally. What we are what we're doing with this is, we're deriving huge knowledge graph from linked data. So here you can see an entity size comparison of the knowledge graphs we are building. So, the DBPedia you you know and you are used to is on the left side. Yeah. This is the English tiny diamond. Diamond because it's compressed data from several sources. We recently built a Dutch national knowledge graph from Dutch linked data sources. Then you have, as a comparison, you have items in Wikidata. And right next to it, there's this DBPedia small diamond, which compresses all language versions of Wikipedia plus Wikidata in in one, yeah, like a diamond knowledge graph. We'll build more national knowledge graphs in the coming years, like the German one or maybe the US one. If you're interested, please mail me. And, on the then the there comes the largest diamond, which is kind of like all datasets that we integrated up to now, compressed into one huge knowledge graph that you can download. So here, we increased the speed a lot. So we we need to right now, we need around four hours per linked data cloud per linked data bubble, and we are going to crowdsource this so everybody can integrate their data into the PDF. Then we have these two commercial graphs, the Google Knowledge Graph for comparison. So this is log scale and the DeepPort knowledge graph. And then we have, of course, the linked open data cloud, which is the largest knowledge graph on Earth, I think, compared to anything else. There's there's more data available there, but not findable. That is a problem. Yeah. So we need to book up the infrastructure that can really find, the particular data that you need from the LOB cloud in life and also manage the changing of the data. So now I I want to show you a data bus application. So previously, we focused on the consumer, the download, and also developing methodology with the data bus. But you can also build applications which are powered directly by the data bus. And I would say this is a unique selling point because this is not possible with other data repositories. So other data repositories, you can kind of, like, keep the or organize the files a bit, but it's very hard to build real applications on the repository itself as a platform. So this is DBP we call it DBPedia archival. So it's an ontology archive. I will show it real quick. So what we do is we crawl eight hundred. So at the moment, we discovered and crawled eight hundred ninety ontologies, and then we do a star rating. So here you can have a list of ontologies and filter them. Maybe some audience want to see their ontology. Maybe it's we don't have time to wait till somebody suggest one. But, yeah, you can try it out later how this, the star rating is done. So so what it does is, basically, this this archival tool discovers and tracks all ontologies and every eight hours, it downloads a version of the ontology onto the data bus, bus, which is persistently saved. So this means you can now use the data bus to download and, the ontologies and the individual versions. So if you click on let's wait. Let's go for the latest time stamp to see which ontologies have been edited recently. So here, this one, you can see that here every time a change is detected, the ontology is downloaded and persisted, and then we check for parsability here. For example, if you only have one star, then most likely the license is missing. Yeah. And then a consistency check. So these are the the four stars. So these are these are basically stars which are needed. So, you know, your ontology is is fair and available. It doesn't say anything about the quality, just that you can find and access the ontology and that it has a license attached. So there are, in the back end, you have also programmatic access ontology versions. And this is kind of like an application build on the on the data bus itself. Where two things where I have no time, which I just want to mention shortly. So there's also a data bus client, which is quite practically because it converts format and compression on download. So it's a bit different than content negotiation. Content negotiation does the conversion server side. And here, you can the the client kind of, like, simplifies g zed g zed g zed to b zed to conversion or things like that. So it's quite practical to homogenize the collections. And there's also data bus mods, which automate the enrichment of the metadata. So this is like so users don't have to give something like a MIME type because this can be automatically detected or file size or, what many other things, void generation, is done automatically. About the data bus, the usage is free if you register, and we are preparing on-site deploy on-site deployments for the next year beginning of two thousand twenty one, so you can have a more closed architecture. Yeah. Thank you for your attention. Are there any questions? Oliver is asking if there's a user review process. And, for example, what happens if you detect a bug in the Wikipedia data? Okay. So this is about the, the data quality. Yes? I think so. Yep. So, how does it work? So, normally, you would submit an issue in the in the data in the in the GitHub. Yeah? So this issue is kind of like not semantic yet, or actually where the users kind of like, they know already how the data bus works and how to pinpoint the error. But, so let's say you you have you are not familiar with anything. You say, okay. I found this is wrong, and then you post the issue. And what you do what we do then is we tag it with the particular artifact on the data bus. So we say, in this version, we confirm it in this version of this particular artifact. The error occurs. From there, we device tests. So there's a mini dump where we can implement software tests, And these are in the dev branch. So on the dev branch, the tests always fail. And then as soon as one of the tests is fixed, it moves to the master branch, and then it's available next month so it gets better. So this is the overall process. Okay. There's one that I know you're going to love because it's one of your favorite topics. So, someone's asking, like, okay. You know, this is such a great achievement, but, how do you actually manage it? Do you have less funding, or is it because the core team is so dedicated? No. No. It is really, so we have a so we I have to say we have a lot of we have we have a very large community, I have to say. So whenever so it it is it is still, or it comes definitely from a community based models models. So, for example, he, manages the TB, the library now, and gave us three servers. There's some hosting Manheim, open link host domain endpoint. So everybody is kind of like contributing a bit. And we are moving and and you have to see that on the board of DBpedia, there are six of the top ten knowledge engineers according to AI Miner. So we all won a prize, basically, that we are the most influential scholars in knowledge engineering, and also our our members are, very much experts in the field. So you have to there's definitely technology leadership, and this innovation helps to to keep this running. And, we focus the last three or four years to really make it because before that, two people were doing the release, and they needed twelve months or seventeen months in some cases. And this was very expensive, and now we automated it. So it just basically can be done by a very skilled bachelor or master student. Yeah. And this is kind of like a high cost saver because per se, the association is unfunded. So each of the members has a very huge budget. Yeah. Like the libraries, the companies, the researchers, and in this funding, there's maybe always a work package included, or for businesses. They, of course, they want to, show their tools with with the PDF. So there's some interest with the researchers. There's always a work package maybe that is particular to the PDF. Okay. Thanks. There's actually more questions, but since we have to to wrap up, I'll just do a quick one and give you a quick reply. Amir is asking how does the star rating, ontologies work? Is it based on, FAIR principles? No. It cannot be based on FAIR the original FAIR principles because they are too vague. So they are kind of like saying that you should have this and should have this, but they're not specifying it too much. But for ontologies, we're talking about all ontologies, which is a very small and specialized area. And there, you can have clearly measurable star ratings. For the details, you have to go to archival. Tvpr dot org. We spent almost two weeks describing these stars, so there's a lot of information there. Okay. Okay. Great. Thanks thanks once again for, the, for the talk, Sebastian. Thank you very much. Bye. Bye.",
  "language": [
    {
      "id": "http://vocab.getty.edu/aat/300388277",
      "type": "Language",
      "_label": "English"
    }
  ],
  "created_by": {
    "id": "https://example.org/cdkg/Creation/presentation/29",
    "type": "Creation",
    "caused_by": [
      {
        "id": "https://example.org/cdkg/Activity/presentation/29",
        "type": "Activity",
        "_label": "Presentation of \"DBpedia Databus: A platform to evolve knowledge and AI from versioned web files\""
      }
    ]
  },
  "digitally_carried_by": [
    {
      "id": "https://example.org/cdkg/DigitalObject/recording/29",
      "type": "DigitalObject",
      "_label": "Recording of \"DBpedia Databus: A platform to evolve knowledge and AI from versioned web files\""
    }
  ]
}