{
  "@context": "https://linked.art/ns/v1/linked-art.json",
  "id": "https://example.org/cdkg/LinguisticObject/presentation/35",
  "type": "LinguisticObject",
  "_label": "Textual content of \"Protecting vital public health programs with AI and Knowledge Graphs\"",
  "classified_as": [
    {
      "id": "http://vocab.getty.edu/aat/300027388",
      "type": "Type",
      "_label": "Transcript"
    }
  ],
  "content": "Welcome to the final session of the morning, before the lunch break, with, Vanessa Lopez of IBM Research Ireland. Vanessa's research interests are to investigate and envision technologies to better understand human needs and support us as a society to target complex problems in the health and social care domain, in particular using a combination of semantic, natural language, and learning technologies to capture, integrate, search, and query diverse data and apply it to solve real challenges, like for integrated care, to support the care of the most vulnerable citizens, and to ultimately obtain, better outcomes. And Vanessa is the, research scientist and manager, within the, AI for health and social care, area of IBM. So a a very warm welcome to, Vanessa and, and over to you. Hi, everybody. Thank you so much for the nice intro. So, yeah, I am a resource scientist and manager for the last eight years at IBM Research in the lab in Ireland. And people here have, lots of different backgrounds. Mine is actually academic. I used to be in the Open University in KMI. But, since I am in IBM in our team, we have done a lot of work on applying research to the integrated health and social care domain. So I'm really glad to be here to share with you this research work that we have done jointly with my team at, IBM Research Europe and with IBM Watson Health. Both teams are collocated in Dublin, and we have a mighty common mission, which is to bring AI to help transform how the health care systems of the future will grow. The group peak here is obviously from the old days where we could get together in person. And here, I'll talk, miss mostly about my experience and perspective perspectives on using knowledge graph to address significant societal problems in the health care industry in two projects. One that we work on for almost three years to combat fraud, waste, and abuse to protect people public health problems, and a second new or exploratory one to understand the impact of social problems to drive positive health. There are not yet AI strong offerings in the human and health services space. And this is because while everyone thinks that this is a real competitive differentiator, bringing AI innovations is really hard. And embedding AI requires longer efforts than classical product timelines. It requires validating what is feasible, what is the impact of false negatives and false positive, understand the customization effort, and the time to market to market assistance. So it requires a little bit less, and it needs to be driven very much by the expertise from the subject matter experts. Those are our SMEs. And our SMEs in particular, they work with the government, and they bring experience from the Medicare and the Medicaid health programs in the US. So before I deep dive, one may ask why Knowledge Graph. So health care and program integrity in particular, they are very complex domains where science and AI can really make a difference. It has a direct impact on people's life, and huge amounts of money are spent globally. There have been actually lots of advancements on the scientific community by building models of disease, progression, and outcomes. However, the impact of those models has been somehow limited. Maybe maybe because the models are silo in the source of data, they are silo in the specific use case or in a specific disease, and isolated models provide isolated predictions. And they do not support very well the comprehensive insights that you need behind complex decisions and the best practice patterns among health care providers and payers that remain hidden in the data. So analyst graph emerge as a unifying technology that may facilitate creating interconnected models so we get better at characterizing and predicting. Our long term goal is to empower care professionals with augmented AI so they can ultimately improve patient outcomes at a sustainable cost, which is a very ambitious goal. And this talk is about our journey at IBM Research. And as a spoiler, we are still far from solving many of the challenges, starting with proving how well it can scale in a real world environment. So a top priority by the World Health Organization is to reduce disparities on the delivery of health care and to ensure a fair distribution of funds, that there is enough for everyone in need to get the services that they are entitled for. Even more at this COVID time when the needs are so great and the funding is stretched, it is key that the money intended for vital public health programs are not great by intentional fraud or by unintentionally providing services that are unnecessary or they are inconsistent with good medical practices or in the worst case, even harmful to patients. So just to make clear that program integrity is not about limiting funding. It is about making sure that the money goes where needed. At to combat fraud, waste, and abuse is a huge market. In the US alone, health care fraud losses are in the tens of billions each year. These are shocking amounts because of bus toasters and bus providers, and these numbers indicate a significant market opportunity. So in this scenario, the providers rendering services to patient submit claims to the state health programs, such as Medicare and Medicaid so they can get reimbursed. And the payers know how much the providers are spending, but they don't know if this is in compliance to regulations which are described in policy or how much is lost to fraud, waste, and abuse. So the investigation units, they validate the integrity of these claims submitted by providers so they can recover overpayments for claims that infringe policy. This is a labor intense and an error prone task. To build a case, the investigators had to review the policy, and then they had to talk to the data analyst to query the claim data, and they go back and forth till they find what they need. They manually encode some algorithms on top of the claim data, but most of the stuff in the policy, it doesn't get automated. So due to the overwhelming volume of policies and claims, most of the systematic fraud or waste is never surfaced. So the policy is written down in very large text documents that is describing what the providers should be doing, And all information about what providers are actually doing is in the structured claim data. So state of the art fraud, waste, and abuse detection is data driven on top of the structured claims, for example, to identify providers with anomalous patterns. But being another layer does not always mean a provider is fraudulent. And even if so, it is not enough to know a provider is fraudulent. Investigators need to tie the clients for this provider to the policy legislation to build a case. And these efforts are not always successful. The policy may turn out to be too vague to be enforced, and providers are confused about what they should be doing on the recoverables recoverable amounts too little to warrant any action. So we thought it will be very powerful to bring this together, and we are the first ones to automate the process of identifying claims at risk based on policy text. So the unique value lies first on using NLP and AI for the automatic extraction of the actionable knowledge from the policy. It converts the policy text into a knowledge base of benefits rules that can be directly applied to review claims against policy. And these human and machine consumable rules, they provide a common interpretation of the policy, which can be shared across the team and investigators. And that that allows a more thorough, transparent, and consistent review. So this benefit benefit rules that are extracted from policies, they are reviewed by the SMEs, our subject matter experts, before they execute the mobile claims. And this is a significant differentiator. It empowers the investigators to size and prioritize the investigations they may follow based on the evidence and on the likelihood of recovery from funds and as well on the policy by executing those policy rules on top of the claims. And a second big piece is explainability. Each label claim at risk is linked to the benefit rule alongside the natural language part from the policy text that explains what is the violation . So there are many challenges behind automating the claim processing from policy at the scale. And a core challenge is to capture the knowledge from the text and to model the domain. So at the center, there is an ontology, which is an evolving asset that we created manually with our experts to bring to bridge this conceptualization gap on how the experts think about the policy and how the data is storing the claims. The ontology captures the terminology, and it makes explicit complex relations that we are interested on, so it guides destruction. Another challenge is that we didn't have any label data to start with. So an important achievement was engagement and the validation with the users early in the development. So we build basic UI tools to interact and gather feedback and ground truth data from the experts and to learn from that human feedback. Lastly, we need to connect the policy information to the structured claim data in order to reason over claims, to quantify, and explain the violations. So, overall, this is a new way that we are proposing to consume rules from policy. And to do so, we have to work very closely with our policy as person. What are the type of rules most suitable and of high value? And this is just here a simple example from a dental policy on a service limitation type of rule on the number of units a provider can bill for a given service on a given patient over a period of times. So there are many ways ways in which the policy text may describe this criteria, but with the with the same underlying semantics. So after ingesting the PDF policy, the second step is to automatically identify the concepts and relations that are expressing the sentence and to abstract that into common reusable patterns or structure templates of benefits rules as they are modeling the ontology, like capturing for which services, whether they qualify in eligible members, whether the limits, are there any other requirements or exclusions. A knowledge graph is then automatically populated with this knowledge funded policy. And because automatic knowledge extraction is really far from perfect, human AI collaboration is crucial for experts to review and validate the strategic rules. Once that's done, the last step connects the policy information to the structured claim data. This is a mapping and configurable step that maps conditions and values from the ontology to column values on the claims. So it can execute on top of claims and label the claims that are valid or at risk depending on the type of rule. So this is just to impress with some boxes, but no worries. I won't go into detail. Yes. To give a flavor here that there are lots of moving parts behind an AI system. And these are components that we build on top of existing technology, which combines NLP, Now let's reason in so we cannot put the knowledge base of benefits rules from the policy and that the users can review. And from them, we we can learn from that feedback based on deep learning approaches. So the instruction improves over time. Here, as a sample on the knowledge extraction, this is built on complementary different technologies, assistant to perform various NLP tasks, like post tagging, ontology based annotations, and to strat functional dependencies in text. We you we do that based on semantic role labeling functionality, like what is the action, who is the agent, the theme of the context, and also dependency trees. These are very useful representations for relation extraction as they collect entities in a sentence through label edges. In some, different strategies are used to extract those potentially relevant textual pot patterns and expose predicate argument structures from the sentence. And from those, we build a semantic graph based not just of concurrency of entities in the sentence, but how the terms are linguistically and semantically connected. So all domain information is inside ontology, and this is a a sample of the subset of the ontology that represents the scheme of the rule described in the sample, which is meant to model a maximum reimburse amount for a service for certain members during a certain period. And OA is flexible. It does not impose a fixed template, but the main constraints can be defined to ensure semantically meaningful benefit rules. Like, for example, this joint constraint that tells that a service cannot be both a cover applicable service and an excluded service in the same benefit rule. So the goal is to populate the ontology with a knowledge graph with the benefit rules, if any, extracted from the text. So after matching at the level of entities to classes, instances, properties, or data types, the system search for matches at the level of patterns, transforming the text all dependencies into graph patterns in a knowledge graph. By reasoning over the anthology, it can infer implicit relations and a path connecting the entities. And this is a simple example where we have thousand dollars, which is annotated as a monetary amount, and up to, which matches an ontological property, which expected range is monetary amount. So they are linguistically connected in the sentence, and we can also find a path that semantically connects them to build a knowledge graph. So that's the idea behind. Right? And the results is an olive graph that in all information needed for these benefit rules attached to this sentence. Now the goal is not just a distraction, but to empower the investigators to interact with the tool and to build trust in the AI. So we needed to hide the complexity behind this knowledge graph and present an unambiguous flat representation of a benefit rule. And on top of those, that user friendly representation, the user can correct any errors. And through these validations, they are incrementally building institutionally institutional knowledge based on policy rules that we can also use as a ground truth for evaluation purposes on one hand. So we can evaluate in terms of precision and recall and publish results in conferences. But, also, as we work with SMEs and as this shared knowledge base is created, the question becomes, how can we learn from this human feedback to predict benefit rules in a nondeterministic way? So we use the ground truth data to fine tune transfer language models such as PERT. Now learning from a small grown small ground truth data is challenging, so we combine knowledge graph with this deep learning approaches, enhanced with semantic embeddings with two goals. One is classifying with greater accuracy the sections in the policy that are most likely relevant with the benefit rule and also to predict the spans in a benefit rule. So to be able to annotate unseen mentions of entities and relations and to annotate them directly with relevant relation. So it is not just saying that, let's say, an oral evaluation in the sample is a service, but to say that in the context of this sentence, it has two roles. It can be an applicable service payable once every three years, or it can be a nonreimbursable service if given at the same date and other listed reimbursable services. So you can see here, the structure is quite complex. So it is the learning. So this is work in progress, but we are getting preliminary, potentially good results. So now to combat fraud, waste, and abuse is a huge market, and it is hard for humans to scale. But machines can do it if we capture the right knowledge and the sophistication in the models. So to prove the feasibility and validate the results, we build call to standards with our investigators. They manually select it and hand coded a set of twenty representative algorithms that correspond to some of our policy rules. It is a very limited set for now because encoding its algorithm to build the voltage standard, it takes precious investigations investigators' time. As for those, the system can automatically take the validated rules to label claims and quantify funds at risk with the same accuracy, same precision recall of our SMEs on top of a hundred million claims. And this validates the semantics behind the model. See, for example, rule seven on the table, it has a relatively small number of claims at risk, but they accounted for very large amounts of money and funds potentially recoverable. In fact, out of a hundred million claims, we can identify almost five hundred thousand claims at risk. That is point zero five percent of the claims for a total of twenty nine million value. This is pulling needles out of a haystack. This twenty nine million is from a small set of rules selected for evaluation purposes, but we have hundreds of policies containing hundreds of rules across areas like physical therapy, medical equipment, behavioral and mental health, vaccines, you name it. So the good thing is that the execution of claims is configurable and transferable. So there is no need to write a unique algorithm to cover each rule. Given the same claims as schema, the rule is executed on the same way across benefit rules. And that's another advantage. Yes. So the investigators can approach investigation from multiple angles. They can search the policy, relating to suspiciously high bill services, and they can see the payment rules that have been extracted on what policy says. And they can review them and publish them into a dashboard to immediately visual visualize how the data looks like based on those rules and the impact in terms of invalid claims and amount of money. And underneath all that, the ontology, what it does, it provides a shared interpretation of the policy and makes this process of rule creation more transparent and comprehensible for machine and humans. This increases the productivity, basically. It also provides an abstraction that allow us to reuse what we learn in one place to somewhere else and minimizes the gap between the policy intent with inter protection interpretation and the execution, which increase the confidence in the rules and the consistency of how they are playing. So we are aiming to capture more and more diverse rules over time and to interpret more complex requirements, like if a service was medical medically necessary that requires linking health records to claims. And there are many opportunities to apply this technology in other scenarios, perhaps to assist pro assist providers to know what they need to attach to claims before submission so they don't get rejected by the payers because of missing information or to identify gaps in policy so we can improve the communication between payers and providers. And now let me just briefly move to other cool exploratory work in progress. This is on using knowledge graph as a foundation for understanding the impact of social determinants of class. This is a use case example that we published very recently on exploring social drivers of health during COVID by leveraging knowledge graph and population trends. But first, what are social determinants? So those are all the things that are in the fringe of the traditional health care system, but that have a major influence on health and are hugely important to determine the treatments with successful outcomes. Take, for example, homeless of poverty. So they have an impact on admissions to hospital and the complications of a disease and are often a hidden driver of cost behind vulnerable population. There are many examples in the news. Here is million dollar married. He was a homeless alcoholic man in Nevada. And for ten years, he was on the streets, and he ran a medical bill larger than anyone in Nevada. And it is one of the samples why social problems may be cheaper to solve than to ignore. It costs one million dollars not to do something about malware. And despite research showing this strong link between understanding social determinants and health outcomes, there is a gap in the market for orphanings that work with unstructured tests and that cover social determinants of health. Our goal, not there yet, but in the past too, is to bring understanding and evidence on what is the impact of tackling social aspects to improve health outcomes and to reduce the cost on populations at risk. Now when when we think about COVID, we first think about the disease, the symptoms, how it spread, how to cure it, or even the global economic crisis. But there are also significant societal consequences. COVID basically escalates the need of the vulnerable populations, and the social problems are made worse because of it. So what we are looking at here is perhaps the less study social and other health concerns surrounding the pandemic. Take food insecurity. You may think of, several areas where people have not access to food, or you have you have a Spanish mama that will tell you about children in Africa at every meal time. But in the US, food insecurity is also a big issue. People that may live in food deserts where they only have access to McDonald's and fresh, healthy, real food is unaffordable for them. So our contribution in this paper was to build a proof of concept where we monitor concerning trends in social determinants that are arising from COVID and link them to other potential health and social issues to identify those populations that are vulnerable or that are at risk of becoming vulnerable during the pandemic. Now differently from clinical data where we have many standards and terminology, there is not yet a taxonomy for social determinants of health, but there is an ongoing processes. It is really a big effort because the domain is really broad. So for this purpose, for evidence discovery relation evidence discovery, we structured associations from PubMed between social and clinical terms, and we mined those relations into a knowledge graph. To start with, we look at the social determinants that were trending high and which ones are trending higher when COVID. In particular, we look at Google Trends to get a glimpse of the social concerns on people's mind during COVID. We found trends like jobless, food insecurity, and shortage, which coincides with the lockdown and thus hardly surprising. So in a second step, we look at for those social trends of interest, like food insecurity in scientific literature, particularly in PubMed abstract, to mine the evidence that they could affect people's health. So we tag the text in this abstract with a tool called MetaMap to highlight concepts clinical concepts. And once we have those sentences containing both food insecurity and other clinical or social related issue, we use a birth based classifier to annotate the sentence as is it it has a positive association or a negative one or no correlation or something more complex. So for example, food insecurity is positively associated with h b a one c. That is one of the markets for diabetes. So we capture those into a knowledge graph to build our first catalog. And this is this is a graph between food insecurity and unemployment with the most relevant neighbors, like anxiety, obesity, depression, and a bit of noise too. So we verify some of these terms like obesity and coping, and we also find higher trends in Google Earth, which show that we are able to capture relevant relations between the social and the health. Now, current and future work is to keep improving this relation identification to catch more complex relations and not ratios so we can assign weight to the ages. And the value of this knowledge graph is, in fact, to bring it together with clinical records or population records to create foundations for new analytic offerings offerings and to be able to do further research as to quantify the impact of these relations in clinical outcomes and to improve current test risk models and identify, for example, what kind of interventions we could have for those high cost high needs segments of the population. In my experience, using ontologies and knowledge graph, they help us really to develop technologies that are able to work with an open set of features and that they can be reused and they can scale to broad domain knowledge such as including health records, claims, social determinants, and so on. So we can build models that are more human understandable and machine consumable. And just to finish, I hope I convinced you that health care is one of those areas where AI and knowledge can really have major impact and that there are lots of open challenges to keep researchers happy. But it is in particular time that we had a conversation about what real benefits users final users get from the AI systems and to start designing system for the users rather than assuming that a high accuracy is sufficient for them. We need to define and measure impact, and that requires finding this wow moment where you are able to go to a user and tell the story so you engage them since the early stages. In my view, there are many benefits, at least in this domain. Like, for example, care professionals turnover turnover is high, knowledgeable experts are in short supply, and there is a need to share and scale expertise for which knowledge is a good tool. There is also an aspiration to support the experts with explainable and concise concise evidence on the point of care to back up their decisions. Like, whatever it is recommending interventions or whatever is identifying ways or improving policy to reduce health care disparities. Like, for example, you can give fifty blood text analysis to someone per year, but that won't make a difference in any outcome. And the evidence is there in the data. But in some, it is not much about what technology is underneath, but about empowering the health care agencies and the governments to reboot society with a better way to approach health care and a more adaptive post COVID health system. And that's the end of my presentation. I think, there's some questions coming through at the moment. I can see, a couple of people typing. I'll just read the first one out. It appears you use OWL, and you have complex and explicit SME rules. Question, are you using OWL DL? Which rules engine do you use? How do you translate rules implicit in the ontology to those you run against the knowledge graph? Question from people. Yeah. Yeah. That that is, that is a very good question. I didn't cover that. And, yes, we are using actually all, behind, so that's how we are defining the ontology. But then what we do is transform this knowledge graph into a flat representation that we store actually in a post in a Mongo, sorry, in a MongoDB. And it's a simple representation where you show just the conditions and the values so that the SMEs can in a very easy way, with their own terminology that we actually define in ontology, curate those those rules. And then what happened is is these conditions and these values, they are mapped for to claims, columns and to claims values. Right? So and this is happening in odontology as well. That's domain knowledge that we live in odontology saying, for example, that the service about topical fluoride describing the policy corresponds to codes zero zero one and codes zero zero two as they are billed on the claims. So those that linkage information is behind the syntactology, and we use that to basically send a set of queries. So we aggregate the claims. We filter claims based on the service or based on the period of time. We apply some operators like you have to be between this h and this h and so on. So then that executes, and the selected claims, they are marked as valid or as invalid depending on the type of rule. So it's it's a deterministic approach. What we are now investigating as well is to be to use a probabilistic reasoning engine. So to be able to execute that knowledge even in the cases where maybe we are not fully sure or fully confident on the rule. So there may be rules that we have a very high confidence that they are accurate because they were validated with by the users, all the stuff are not. So a probabilistic engine will be a good tool to to handle those cases, but that's that's currently work in progress. So integrating a rule engine is is work in progress, and we'll see what results we get. Cool. Interesting. Thanks for that. Whilst there's a couple of people typing questions, Keith says thank you, just whilst that's happening, one of the questions that came up in, some of the workshop shop sessions was around I don't know whether you you may be able to answer this is, if someone was starting to look at open source biomedical ontologies, I guess, what common ontologies there are that you would recommend working with? Yeah. So we have, work a lot with ICD ten. It's a very medical medical terminology for, you know, all these, like, symptoms, diagnosis, and so on. We work a lot as well with UMLS, which is a unifying medical language terminology that gives you also a lot of information about health care services, a lot of information about diagnosis. Like, for example, you are a high risk of cancer or highest risk of something else, information about treatments and so on. There are also ontologies for procedure codes as they are bill on the provider claims. And those are the ones that we are mostly working on, but, is, yeah, it's it's great to see all these anthologies that are appearing there every every day, basically. There are more and more. Cool. Thanks, Vanessa. That's really helpful. So we'll just give it a second. I can see Douglas Moore is typing. So just to see if we can make for his questions to come through. And then if that doesn't come through, then, we'll probably break for lunch. Of course, any anyone that has any follow-up questions, please do, link up with Vanessa on the on the Slack channel. You can you can ask them there. Here we are. Couple of questions. So was the claims processing, a point solution for validating claims, or has the effort to map source data to greater benefits? Not sure if So it seems to be a question about the claim processing. I'm not so sure if I'm not gonna answer this question correctly. But, so the claim processing is something that is happening already on the state of the art, and in particular for fraud, waste, and abuse detection approaches. Right? They look at outliers. They look at calculating the risk of providers and so on. So the claim data is usually in a very well structured format. Right? And in some cases, it's also linked with health records that give you extra information. Like, you can look at the care pathway of a person, the outcomes, and so on. What we try to do here is a bit different because we want to understand the claims, not by looking only at the claim data, but by understanding the policy. If the policy says you can't do this and you cannot do that, and then to see if we can execute that on the claim of data to see actually what claims they don't seem to be doing that. So that gives you an indication to the investigator to have a look and to explore more and to create. If that is the case that they are really, infringing policy, they can build a case. And in order to build a case, they had to tell. This is a policy lay legislation that these claims are breaking. I'm not sure if I answered that question well. Yeah. I think, I I think that that, answered that. And then there's another comment, from Frederick Frederick, Landquist, using mesh UMLS ICD, etcetera, medical vocabularies. But have you also created one context specific for the speaking about the domain and mapped this with the other large vocabularies? Yeah. Yeah. That's correct. That's the case. So we actually created an ontology manually with the domain as per us. So we can this ontology is the the one that is guiding the knowledge extraction, and it's basically focusing on things that are actionable. So I care about these entities and these relations because these are the things that I can then go to the claim data and see if something is compliant or not. So that ontology was direct was created within that schema. And then, however, there is a lot of instance data. And for that, we use other sources. There is a lot of tabular tabular data. For example, it thinks about describing places of service. Right? You can give this service in the of in the hospital, or you can give this service in this condition. So all those codes and all those sources, we list them into the ontology, just by creating some mappings as in as in fancy. And we also, through the ontology, links to those other terminologies. So for example, there may be an eligibility criteria that says, in order for you to get topical fluoride, you need to be a high risk of caries or cavities. So to extract this high risk of caries, we use UMLS. We say that we are expecting something there that may be a medical diagnosis. Right? And then that is the domain of that property, and that's some part of the eligibility criteria. So this is how we do the linking to UMLS. It's based also it's based basically on the semantic hierarchy that UMLS has. Perfect. Great. Thanks, thanks so much for that, Vanessa. Really fantastic talk. Thanks to you, and enjoy your lunch. You too. Thanks. Bye.",
  "language": [
    {
      "id": "http://vocab.getty.edu/aat/300388277",
      "type": "Language",
      "_label": "English"
    }
  ],
  "created_by": {
    "id": "https://example.org/cdkg/Creation/presentation/35",
    "type": "Creation",
    "caused_by": [
      {
        "id": "https://example.org/cdkg/Activity/presentation/35",
        "type": "Activity",
        "_label": "Presentation of \"Protecting vital public health programs with AI and Knowledge Graphs\""
      }
    ]
  },
  "digitally_carried_by": [
    {
      "id": "https://example.org/cdkg/DigitalObject/recording/35",
      "type": "DigitalObject",
      "_label": "Recording of \"Protecting vital public health programs with AI and Knowledge Graphs\""
    }
  ]
}