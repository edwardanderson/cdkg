{
  "@context": "https://linked.art/ns/v1/linked-art.json",
  "id": "https://example.org/cdkg/LinguisticObject/presentation/0",
  "type": "LinguisticObject",
  "_label": "Textual content of \"Neural Algorithmic Reasoning: Combining Classical Algorithms and Neural Networks\"",
  "classified_as": [
    {
      "id": "http://vocab.getty.edu/aat/300027388",
      "type": "Type",
      "_label": "Transcript"
    }
  ],
  "content": "Hello everybody. I'm Andreea Deac, PhD student at Mila University of Montreal. And today I'm happy to present a talk that I think will, will be interesting to all problem solvers among us. This is called neural algorithmic reasoning. And we'll we'll talk about how to combine classical algorithms and neural networks. This is, as you might think already, there are two well known approaches to problem solving, each with their own benefits and drawbacks. So algorithms are trivially strongly generalizable. You can apply them to different sized inputs, and they will still work. They you can compose them if you have different functions, subroutines. You can you can use them together, change them. They have guaranteed correctness and they are interpretable. They have interpretable operations. We always know what they're doing. However, their inputs must always match a specific format, and they're not robust to task variations. On the other hand, the the more recent, go to, the the neural networks, they can operate on raw inputs. They can generalize on basic conditions. So even if, our inputs are are not precise, they will still work. They can be used across tasks. But a a downside is that before getting a neural network to perform well, we usually require a lot of data to train them, to get them to a good performance. They are also unreliable when extrapolating, and they lack interpretability, although that's actively being worked on. You might notice that these two approaches have attributes that are complementary, and what we, what we might want is to get the best of both worlds. So this is what today's talk is going to be about. And I'll start by talking about a problem that is, quite simple. Most computer science students meet it, at some point or another. So if we have a weighted graph with a specific source, node of interest, and we want to compute all shortest paths from this node to the other nodes in the graph. We can do this by using some dynamic programming algorithm, for example, Bellman form being one of them. And recently, there, there was the, the paper called neural execution of graph algorithms, where, a team of researchers tried to, to propose a neural network that can achieve the same goal. So this was a graph neural network in particular that was supervised to on, intermediate outputs of the Bellman Ford algorithm. So this was this is also known as trunk supervision of at, at every step of the of the iteration. And you can see then the what what we the the operation in the dynamic programming algorithm, aligns quite well with the message passing update rule, in a way. So this observation was, was something that was, explored more in, in a concurrent work, where where they they discovered that graph neural networks align well, in general with dynamic programming algorithms, not just Bellman Ford. And some other, findings were were were pointed out. So, for example, this strong supervision I was talking about is actually quite important to a good performance of the graph neural network in imitating the algorithm. We it requires things like, a different aggregation, the max aggregation to perform well, which aligns, again with the with what we know the dynamic programming algorithm, does. More recently, there's also been a lot of work on proposing better architectures, for example, for for, achieving some computation classic computational goals, and the some even more recent insights, for example, linear algorithmic alignment is highly beneficial. And if we learn multiple algorithms in a multi tasking way, this can also improve performance. So this is also very useful, especially if we want a component that learns multiple algorithms, not just one. So this led to a blueprint being proposed by Velich, Komichi, and Glanton, in in patterns. So under the the name of neural algorithmic reasoning, we have this blueprint, which starts from from the assumption that if we have a task where we know an algorithm could be could be useful to solve this task, Then we can pretrain the processor on some abstract inputs to learn this, how to execute this, how to simulate this algorithm. And then we can take this processor and plug it into our natural pipeline. So in the, in the case of navigation where we're interested from, from getting, in getting from point a to b, c, or d, we might want to use something like Bellarmine Fund, which computes surface path to see what is the best way to to get somewhere, to to do, to to navigate. So keeping this blueprint in mind, you might be wondering, does this work in practice? Has this been used in in some tasks already? So in this talk, I'll talk about, how this was used in reinforcement learning. Our our proposed agent, Excelvin, was done in collaboration with Petar Velish Velishkovich, Piotr Bakon, Jan Tang, and Vlad Nikolic. And even more recently, explored this blueprint was also explored in a self supervised learning setup, in the paper named reasoning modulated representations about which I won't have time to talk, but I recommend you you go check it out. So to jump straight into it, in a reinforcement learning setup, we have an agent that, acts upon a word and receives observations back from this word. This agent can decide to build a plan, and update this plan based on observations, a plan that can then be used on deciding which actions to take. So formalizing this a bit, this is usually found under the Markov decision process framework, where we have a state space, an action space. We have transition the transition matrix. So what's the probability of being, of getting to a state s frame given that we're in a current state s and we take a specific action a. Also rewards. So what's the reward that, the agent will receive by, doing action a in state s? And the goal is to find the policy, pi that tells me what what is the sequence of actions that I need to take in order to optimize the discounted cumulative reward. So get, as much reward over a longer time horizon as possible. So policies that act purely through adapting to rewards are called reactive. And in many cases, to to get them to work well, they require a lot of data and are quite slow to adapt. And planning is something that was proposed to ameliorate these issues by maintaining an explicit model of the world. So what this means is that we would have a state transition model and the reward model, which would usually train from some observed trajectory. So having the agent, acting to the world and gather some data. And using these motors, a planner can simulate the effects of actions before taking them, which is very important and comes with some important benefits as as we are going to see now. So some of these benefits well, the first one is data efficiency. And this is very important if we're planning to use our agents in real world where we don't have access to millions of frames as we do in the case of games. So if if we have the good model, that would mean that we'd have to interact less with the environment in order to get, a good policy. Then strong models will also allow us to adapt to unseen situations. So that is very important, if if anything is changing in the environment, for example, And also very important for safety reasons because if we have a strong model predicting if some action would be would lead to a unsafe outcome, that that means that we can avoid taking that that action to to learn, that would happen. So, this would this is quite important from a safety considerate. It will also allow us to account for, external factors such as human interactions. And in fact, this was already used in game training, alpha go, which you've probably heard about, and across the sciences, for example, for chemical synthesis. Planning has some, theoretical, results that that are very encouraging. So if we had the perfect model, this would allow us to plan for perfect policies, and this is quite important. One algorithm that is very useful for planning is value iteration. This is a dynamic programming algorithm, which is we'll see why why it's important in a bit, that can allow us to perfectly solve a reinforcement learning environment. So what this does is it, it computes values of states based on the values of neighboring states. And it starts with, for example, a random estimate and gradually improves this estimate until it reaches an optimal solution. So this is guaranteed to happen, which is very convenient. And if once we complete this optimal solution, then the optimal policy can be found just by taking the actions that maximize these values. So once we now now that we know about this very useful algorithm, well, we we we could think, what what can we do with it? So could we actually use it in a reinforcement learning environment? That would be a a great thing. Assuming for now, we'll assume that we have the reward function and the transition matrix. And in a bit, we'll talk about what what to do when we don't have these two things. So we assume the transition matrix is fixed and known. By known, for example, each state has no neighbors, up, down, left, right. The actions are deterministic, and computing value doing value iteration would amount computing sums of neighboring values. And this might already remind you of something that's that's familiar. So convolutional neural networks are quite well suited to computing this convolution like, computation. And this idea was, leveraged in value iteration networks, which got the best paper award in NeurIPS in twenty sixteen. So to to summarize, they assuming the MDP is discrete, fixed, and null, they perform value iteration computation by stacking, shared convolutional errors. And while this is great, it has the the downside that this is quite restricted in the sense the MDP needs to be discrete, fixed, and no. And Givens already take a step towards, more general cases extending to to the case where the the MDP doesn't have to be agreed for it. But it is still fixed and known. So in that case, like, so far, we didn't need to estimate the transition model. We didn't need to to find one that the the p would be, and we didn't have to deal with continuous state spaces either. But what would happen if we did another mdp? Well, we can think a bit, what would the human feature engineer do to use validation, which you've seen with a very useful algorithm. So the human feature engineer would take the game frame, the, which we call the natural input and compute some abstract input from it on which it they would be able to run the value iteration algorithm to get abstract outputs. However, we've seen that deep learning does a very good job when we don't have to manually compute features. So we'll we'll use some of this, some of this insight and have an encoder, that, bypasses this part of, human engineering. So this would stick our natural input, to an embedding. And that is great, but that that will still not be everything we need to run validation if we didn't have the transition matrix. For that, we can use a transition model, to build this local MDP that where we need in order to run value iteration. And this was already studied, quite a bit, and there exist many popular methods for for training this transition model. I'll just name one of them, which is what we used in our in our proposed agent. So contrastive learning, is is one way of training t. And this would be this would mean training t to discriminate between true next states and some fake next next states, some randomly sampled, s tilde. So if we have a traditional model that learned how to how the next state s primes will look like, this transition model then can then be used in a in a in a generative manner in a way to to build the the tree over which we had run value iteration. So if we have this tree, we have the p, but we still don't have the reward model. So for now, we'll assume that this is given. We'll assume that the nodes in it in the in the tree I I've previously shown are no. And that would be everything we need to run value iteration, directly over this tree. And this is exactly what was done in pre q n and value prediction networks. And to summarize their architecture, we have the state and the encoder, which is then, is then used repeatedly to build a tree. And from industry, the values are estimated in each of the nodes, and then this then this can be used for value iteration to to do the backup and and use this. So to to summarize a bit what I've talked about so far, we mapped our natural inputs to the space of abstract inputs using an encoder. Using a transition model, we build a local MDP, and we estimated the rewards in in each of the nodes in in the case of GQN. And this allowed us to run value iteration directly on this tree. And this is quite convenient because it aligns in the direction of implicit planners. That is a very neat direction. I I recommend you read more about, if you're if this sounds, like something that would be. However, doing this running value iteration directly on this tree, presents us with the algorithmic bottleneck. So real world data is often incredibly rich, and we still have to in the previous case, we still had to compress it to scalar values. So the the value iteration solver committed to using the scalar the model predicted and assumed it was perfect. And if there is insufficient data to estimate these scalars, then the algorithm is running on incorrect inputs. So incorrect outputs will very likely go out. So this means that we need to have correct inputs for the value iteration to do what we want, and we hit data efficiency problems again, which is precisely what planning was supposed to solve. So trying to avoid this bottleneck of the algorithm gives a perfect solution but in a suboptimal environment, we propose our agent exfiltrim, which is aimed at breaking the bottleneck . And, the main, idea behind it is that neural networks derive great flexibility from their late representations. They are inherently high dimensional, which means that if any part of that of that high dimensional vector is predicted poorly, then the rest of the vector can be can be used to to compensate and still lead us to a correct answer. So to break the bottleneck, we replace the value iteration solver, like, by we we train the value iteration directly on the scalars with the neural network. So that combination I was, I was mentioning earlier on. So what we'll do is we'll have a neural network between natural inputs and abstract inputs in that encoder, and we'll also have a neural network between our abstract inputs and abstract output instead of having the value iteration algorithm. How do we do that? We do it by pretraining a graph neural network to perform value iteration style computations, on synthetic graphs. So something we can easily generate. And then we freeze this. We keep this as a trained reasoning component and deploy it within our planner. And you might remember as value iteration is a dynamic programming algorithm, this is quite easy to do, thanks to this algorithmic alignment that, was pointed in in the in the I clear twenty paper. So putting all together, our Excel agent looks like this. We have the the state, our neural network, encoder taking to a latent representation, our transition model that is used to build that local MDP that in the case of value iteration networks was assumed no, but now we don't have, as as we won't have have in most of the reinforcement learning environments we're looking at. Then after we build this tree, we use our pretrained executor, that we pre trained on synthetic graphs. And we use this to predict values and actions and plug this into your reinforcement learning algorithm of choice, in our case, BPO. So to look a bit on the on the empirical side, our our agent is in green. We compare it with HCC, which is the the the model I talked about earlier, which, actually runs value iteration in scalar space rather than, in latent space, as Excel VIN does with our pre trained executor. And we also compare with a component that, is just model free, doesn't have, any planning component. So we see that usually, in in a lot of the cases, the XAV and the ATC do better than the than the PPO baseline. And something else that is very interesting to notice is the Excelfin does better than HCC, particularly in the first half of the training. So it does well in low data cases, which is one of the main things why we're looking at culling in the first place. So this is quite an encouraging result. To draw some conclusions, we we looked at we presented this this blueprint, that is applicable if we know a specific algorithm will be of interest for solving a natural, a real world task. So we, we see that real world solutions can benefit from combining classic algorithms with neural networks, and this is one way of going about it. Then we build on the on the findings that graph neural networks are particularly well suited to imitate some algorithms, in particular dynamic programming algorithms. And one example is backward forward and other is value iteration. Value iteration, which is, a very useful algorithm in reinforcement learning as it allows allows us to plan, and it can lead to to perfect plans. And using this, this component that learns the immediate value iteration in a, in a reinforcement learning agent, led us to find that it can, it can have great benefits. For example, having good performance in low data cases. Thank you for listening. Some of this, work was already presented in in some some nice articles. So, do check them out if you would like to hear to hear see more about this. And, of course, these were presented in in research papers, neuroalgosmic reasoning, again, neuroalgosmic reasoning as an inclusive planners. I'll also link them here if you'd like to read more about them. Last time, lastly, but not not least, I'm happy to take questions. And in general, if you if you have thoughts, feel free to send me an email. Thank you.",
  "language": [
    {
      "id": "http://vocab.getty.edu/aat/300388277",
      "type": "Language",
      "_label": "English"
    }
  ],
  "created_by": {
    "id": "https://example.org/cdkg/Creation/presentation/0",
    "type": "Creation",
    "caused_by": [
      {
        "id": "https://example.org/cdkg/Activity/presentation/0",
        "type": "Activity",
        "_label": "Presentation of \"Neural Algorithmic Reasoning: Combining Classical Algorithms and Neural Networks\""
      }
    ]
  },
  "digitally_carried_by": [
    {
      "id": "https://example.org/cdkg/DigitalObject/recording/0",
      "type": "DigitalObject",
      "_label": "Recording of \"Neural Algorithmic Reasoning: Combining Classical Algorithms and Neural Networks\""
    }
  ]
}