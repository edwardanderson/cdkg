{
  "@context": "https://linked.art/ns/v1/linked-art.json",
  "id": "https://example.org/cdkg/LinguisticObject/presentation/12",
  "type": "LinguisticObject",
  "_label": "Textual content of \"Graph Analytics vs Graph Machine Learning\"",
  "classified_as": [
    {
      "id": "http://vocab.getty.edu/aat/300027388",
      "type": "Type",
      "_label": "Transcript"
    }
  ],
  "content": "Hello, and welcome to this connected data world talk on graph analytics versus GraphML. Probably, most of you have heard about, the recent uptake in graph analytics and GraphML for a large number of use cases. And I actually truly believe graph is the future of AI. Just if we look at what companies like Uber and DeepMind are doing, if we look at, what's happening in drug research or computational chemistry. So, also, if we look at Gartner trends, there is a clear uptick in, how we can leverage graph. And I think here, this audience can probably truly appreciate this uptake, which many of us probably have seen coming from the last years. I'm Jorg. I'm the CTO over at RankoDB, a graph database. So I actually do this on a daily basis. And, also very passionate about, graph machine learning and GraphML. Also, for example, teaching an a weighted course on that. So but let's actually dig into it. Let's also see what this talk is about. So if you're talking about graph AI, we can often mean very different things. So I always try to kind of classify the complexity of queries we try to answer as follows. So the most simple things we can look at are graph queries. So those are queries which should be able to, be answered from any, graph database. And it can simply be like, who can introduce me to another person in our social graph here on the right? So in the end, basically, what is the path going from one note to another in this graph? If we want to get more complex and, as I said, often from, like, an algorithmic level, we first we have to keep some state. We have to make sure, to actually, find something globally in a graph. And then we actually come to the realm of graph analytics or graph algorithms. And, for example, questions we can answer here, who has the most connected persons in this graph? And you can already see we have to, a, look at the entire graph, b, we have to keep some state, like, how many direct neighbors, how many direct connections does a person have. Then kind of see next level, and this has really been taking up over the past year, is graph machine learning. So graph machine learning, allows us to deploy, yeah, statistical machine learning to alter our graph or to add, for example, edges, to add node features. And with that, we can do solve tasks such as follows, predicting potential connections. So we can see that here on the right, who should actually be connected to another person, but we can also predict other node features that can, for example, be the churn probability of people. So here in this graph, imagine just each node having a little feature saying how likely is that person to churn. So with that, I often get the question asked, like, what is actually the difference between regular ML and GraphML? I really feel the value of GraphML comes in from certain properties of real world datasets. So a default assumption in more traditional machine learning is, I independent and identically distributed data. Meaning, if we here on the right again, our social graph and we will try to to churn, imagine how likely someone to leave our favorite social network, we would treat each person independently, and then try to predict those probabilities. But if we think about it, what just what will happen if all our friends have just left the same social network? So we are very likely to leave that as well as we'll just be left with, like, a so lonely node out there without any connections. That is the one part. Second part, probably, we'll also have similar reasons to lease the network as all our friends set. This is actually an assumption which is true for many real world datasets, and it's called homophily. And that means that neighboring nodes are similar. So in this case, we would have a similar likelihood to leave that networks and all our neighbors, or at least it wouldn't be totally independent. Of course, in traditional machine learning, you could create an extra feature such as how many of your direct connections have recently left that network. What you see is this is an explicit feature we have to create, we actually have to consider upfront. So, let's dive a little bit more into what we can do or what actually graph analytics is about. Graph analytics sometimes also refer to network analysis. Feel free to ping me about the exact difference. Happy to get into the debate about that. But, this is actually analysis applied to graph based data. It's, used across many fields from fraud detection, marketing, supply chain management, recommendation systems, law enforcement, cyber, etcetera. And if we drill down, we can also treat it as a way of unsupervised learning. So, let's take a look at what we can do with that. So for example, who is important or influential? We saw that already in the previous example about, our network, who is the most connected persons. But we'll also see shortly that there are many other measures of what it means to be important in a network. And, again, here, you can already see this is kind of like unsupervised learning task. We just have to graph. We don't necessarily have a training target, but we can still identify, who would be important or who not till we kind of get an outcome. Other things, can be if we have, a chain of events, what is the connection or also topological sort, what is kind of a valid order in those partially sorted graphs. And An important question, especially in computational chemistry, is, for example, are two graphs the same, which isn't as easy to answer as you might think. And as there exist a large number of algorithms for that as well. And then if we go into fraud detection, what is unusual in a graph? So where are certain patterns, which might indicate, fraudulist behaviors? So for example, in money laundering, certain chains of money going around, being transferred, back to that same person, or just, also large aggregates of, money being transferred from, like, one account split up and then eventually arriving at, the destination account. Maybe just as the last example here, also very important, what are distinct subcommunities in the graph? Again, many real world datasets have certain subcommunities. If you think back of our social, graphs there, they often were like those independent components, or, like, independent subgroups, more or less independent, of people usually centered around one or more influencers. And often, it's quite important to identify those communities. For example, again, this can also be used in fraud detection. Where we try to identify, for example, similar items, similar offerings, in into subgroups. Usually, graph algorithms are specked by a number of graph algorithms. And so here, just a very crude approach of kind of classifying what what is all out there. So on the one hand, we have search or traversal algorithms. So that can be, Node or Edge. This can be breadth first search, depth first search, which probably most of you already covered, in some kind of, computer science or mathematics class. Otherwise, pathfinding, and that would be the simple example we saw also in the beginning. I want to find, like, how can I get from HEP or in a social network? Who can introduce me to a specific person? And then we come to, the more fancy term for what's actually important out there, and that would be centrality. And so, for example, what are important nodes in in a network, in a graph? And, those are also referred to as central nodes, hence the notion centrality. If we now, take a look on the right, so here we have the sample graph, and, I try to identify several different notions of centrality because it really depends on the what you want to achieve. So, for example, the most simple one, and that was also the question we had in in the beginning in the introduction slide, what is the most connected node? And this is referred to degree centrality because the degree, the number of edges a node has, is basically that criterion here. The other one, is closeness centrality, and that refers to which nodes are, on average, close to all other nodes. So, for example, if you're trying to build some kind of distribution center, which from where you can reach all nodes on average, in the fastest, that would be the closeness centrality. Last, there's also betweenness centrality, and that's, referring to, what is how important are certain nodes in connecting multiple subgroups. And you can already see if you drop this node here as in between as the, purple node, you would actually, split that graph into two subgraphs. There, of course, exist, even many more different centrality measures, but just to give you feeling for it that this is a very important task with also many important, different algorithms. And next, quite common task is cycle detection, And this is, for example, given my database background, as this is often used in database systems to identify, cycles or a deadlock detection. If one transaction depends on another, depends on another, then probably we have to abort one transaction. And, if you want to imagine that, that's actually a cycle in the dependency graph of those transactions. And it can also be used in general network analysis. So if you're trying to plan, like, a routing or telecommunication networks, also cycle detection, can be an important step. And then, last, we already touched upon that is community detection where we want to identify distinct subgroups in our graphs, which might have different behavior. And, again, this could, for example, also be in a telecommunications network, where, which helps us to generate our routing or in cases like a fraud detection where we want to identify distinct subgroups, which we then can classify as either fraudulent or not. Let's next move to, GraphML. So GraphML is machine learning applied to graph based data, and this means we're actually coming up with a trained statistical model. So as you might know from traditional machine learning, when we we are training a deep neural network or something else, this comes in two steps. So there's first a training approach where we are training our statistical model, and then the second step where we actually leverage it to do some productive work. This is called the inference part. And, this is really split, and this is also distinct from analytics where, basically, this is like one task. I have my graph. I'm doing my analysis, and this is then my outcome, my my inference in one step. Graph eval typically is a supervised machine learning step, not not always, but typically, where I have kind of one target I want to train for. And the challenge with graphs and machine learning is graphs are unstructured data. So the question is how can our favorite, neural networks, or other also support vector machines deal with that? And this is where we come into the realm of graph embeddings. So graph embeddings, often, or you usually try to represent our graph in some form of a vector. So here we have one example for note embeddings. So we try to embed the notes in a in a graph form, and, we just choose a two dimensional embedding. And if we already plot that, we can see, often, this is already very helpful, to distinguish certain subgroups. This is the Karate Club example. Feel free to beat up about that or visit our, workshop, on graph machine learning where we'll dive more into this example. But we can already see often by just embedding it, in in a, vector space. We can easily apply here linear separation between those two groups. And, again, this is due to the fact that, real world datasets, real world graphs often have those properties such as in this case, again, homophily that neighboring nodes will inhabit similar behaviors. Then, usually, this is done, with graph neural networks. So graph neural networks, what do they do? They basically take a graph, go into the embedding space, turn it into another latent representation of set graph. So, that transformation step. Which you're using high level frameworks, you often don't really see that, step in the background to go through the embeddings. But, basically, we come up with a new latent representation of set graph, which can then be used for node classification, graph classification, or for example, link prediction output. There are a large number of use cases and, I would say this is still growing, this field. For example, Uber Eats is using, graph machine learning, graph neural networks for recommendation use cases, which food should you order next? Molecular or, computational chemistry is using, graph machine learning for practice covering. And, for example, also DeepMind, if you think about, maps, this is in in the end also a graph. So they are using graph machine learning to improve CTA predictions, for Google Maps. If we take a look at these use cases, they are actually overlapping quite a bit. I mean, there are certain things which we would only do in GraphML. For example, link prediction gets a bit challenging in Graph Analytics, And there are other tasks, which are clearly part of the graph analytics domain. But for a lot of tasks, we often have a choice of what we want to do. And this is actually what the remainder of this talk about is is about when should we choose in this middle space here when we actually have options? What are our trade offs between choosing a graph analytics solution and a graph ML solution? First question for, for this, is probably performance. So how long does it actually take to get a result? In graph analytics, this really depends on the algorithm. But as I said in the beginning, typically, we have to look at the overall graph, and we also have to recompute that or rerun that algorithm for any new tree and any updated graph. So overall performance, it can be really fast if we have a simple algorithm, but especially on large graphs, often performance is is a challenge out here. In graph machine learning, this performance or the performance cost is actually split across the two parts. So training, we need to train our statistical model and then, inference. Modern, graph machine learning is often using inductive techniques. So we don't actually need retraining if we have a new query or or an updated graph. And, also, with that, the inference part, which is what we typically care about if we need an a quick result, is also constant in graph size. So, in the end, the in graph machining, we kind of we pay an upfront price into training, but then often, again, it it slightly depends on the model complexity. We can achieve pretty fast inference times, which is, as I said, then also constant in terms of graph size. Also, we can leverage specialized hardware for that. So, for example, GPUs, they are really good in training and also doing inference on, on those, neural networks. And, so in the end, if we really care about fast inference performance, graph machine learning can give us this edge here because we train a statistical model. And then once we have some, we don't have to go across the entire graph but can just look at the narrow field of that graph. This comes at a trade off, and that trade off is, a curiously, or if you see the vice versa side, it uses an approximation. So the next question would be, does, the use case actually require exact results or are approximations sufficient? If we take, for example, a look at, health care. In health care, probably, we really want exact results and approximations depending on what we are doing, but, that might actually be a challenging thing similar in finance, but it depends a little on the use cases. Whereas if we do the Uber Eats product recommendation, if we only use an approximations there, that's actually okay because, overall, we won't get, like, really exact results anyhow as we're still dealing with humans. And so having an approximation in our algorithm is actually fine. The other aspect of when we talk about those approximations or at least statistical model aspect of, graph neural networks also comes to reproducible results. Imagine, you're in finance, you're retraining your model. You would expect at least similar results. So the question is how reproducible are the outcomes in the end. Graph analytics, we have an analytical model, so it's, reproducible, and also accurate to our to our algorithms because we are just really following the algorithmic steps. But keep in mind, it might actually be too expensive to run on large graphs. So, accuracy doesn't really help us if it comes down to too large cost, and we can't fully, leverage that in the end. Graph machine learning, on the other hand, is using a statistical model, so an approximation. And so the accuracy depends on data and model. So how much data do we have available? Often, I've often seen machine learning, trying to be trained with just too small or too unqualified data. So I think this is really then crucial also for your accuracy. And, on the other hand, we can also control the model complexity of our graph machine learning models, but keep in mind, it's still an approximation out there. And this also relates then, or leads over to the next item, and that's explainability. So, the ability to reason about the inference results. So understand why a certain decision was made. Why was this, why was this graph classified as a? Why was it classified as b? Or going back to our social graph from the beginning, why was a certain person classified to be likely to churn? Graph analytics, again, it's an analytical model which we can reason about. Graph machine, on the other hand, we have a statistical model, which is often a black box. So trying to reason about our graph neural network probably will be, challenging, but this is actually an active line of research trying to understand how can we make, how can we make machine learning models more explainable? How can we more understand why a certain decision was made? Scalability. So the ability to really scale to large graphs, large datasets, which might even, require a distributed computation. So graph analytics, the scalability depends on the complexity of the algorithm. But as we talked about earlier, often or in most cases, we have to look at the entire graph. So, there's already, like, some kind of limit. And, the other aspect here is that, growing or updating datasets is, challenging. This is also called trans activity, and it refers to imagine the we are Facebook. We are LinkedIn. Our social network graph is isn't going to be stable. There are changes all the time, new people joining or people churning. And, the question is now, if I want certain results, what does it mean for for my graph? In graph analytics, often, as I said, this is, trans activity. We have to recompute the results, from scratch, which, of course, is also costly in terms of, scalability, especially in terms of large graphs. Graph machine learning, on the other hand, it, again, it depends a little bit on the model, how how much it scales. But, again, keep in mind, we have this, trade off between, training and, inference. So training, it can often be a really long but offline process. And then when I want to do the inference , I can rely, or utilize some results I have out of the the training process. Furthermore, again, a kind of a complexity precision trade off. So less complex models also would evaluate and train faster. And so I I have kind of, like, a knob to control that. About growing and updating datasets, graph machine learning or modern graph machine learning as, for example, GraphSAGE, they are actually inductive. So we can update our data set, and we don't have to retrain, the entire model from scratch, but we can still utilize that trained model to classify new new nodes or change parts of Deepgram. So, Zan, if we go to, like, a direct comparison, so we took, actually ran the number of algorithms. And, you can see there are certain things which we would just solve in the, in in the space of graph analytics such as sort shortest paths, etcetera, connect components. There are other things where we actually have options such as vertex or graph similarity. And there are other things which we would only do in the GraphML space such as edge prediction. And especially in that realm here where we have two options, let's take vertex and graph similarity, we we can see the, trade offs. Graph analytics, basically, doesn't need, any train time, so it's great there. But on the other hand, it will be pretty, bad or we will incur that performance cost at inference time. We do the same with graph machine, we can actually have the trade off between training time and inference time and split it into one offline and one online process. Trends, or inductivity slash transductive notes, in, using graph machining, we can utilize a trained model also if our graph changes. But, of course, as Tom said in, cost of, explainability and precision, it's going to be less explainable. First, we have, a generated model. And also the precision, it will be an approximation, of those results. How to choose that? That was a lot of information, but the simplified version or the questions I would ask, first of all, should always be, is a simple model sufficient? Graph analytical models are usually simpler and, keep it short and simple. So I would always choose a simple model over a more complex one. Next question would be, is is there a need for, like, an exact answer, or are we approximations? The if we are approximations, this could be an enabler for graph machine. And if we really always need very exact answers, we'll probably stay in the realm of of graph analytics. But keep in mind, this might actually be unfeasible, depending on the data size. And this is exactly the next question. The smaller the data sets, the more likely, we are happy in the data analytics space. The bigger the data sets might actually mean we need We we are, a, able to train a a larger model, but, b, also, might require, graph machine learning. What are our performance requirements? And, again, this can be split into two parts. Are we okay with a large offline training process, and then faster serving times, or, are we actually okay with just running graph analysis from scratch? We have a need for inductive learning. I said, like, since this is a subpart of, three and four, but, also refers to, like, how dynamic is our graph, how often is our graph changing. And then last, do we have a need for explainable results that we can easily look at, explainable machine learning, techniques? Or, we, again, might have a tendency to use, graph analytics over graph machine learning. So, that was a lot of information. I tried to capture some information here, where to go next if you want to learn more about it. So first of all, if you want to learn more about graph databases, graph databases in action, if you want to learn a bit more about how to model data, the practitioner's guide to draft data, then, graph algorithms, is nicely covered in this book. Graph machine learning, also kind of as a lead over from, graph analytics is covered in graph powered machine learning. And then if you really want to dwell, deeply into, graph machine learning and, also refer to, like, the embedding processes graph representation learning as this is a really great free ebook, which is out there. Furthermore, there's a a really great course at Stanford about machine learning with graph and a number of blocks out there. Having that said, thank you so much for listening. Also feel free to, visit us. We have a number of, other information out there, both on the side. Otherwise, there's also talk from Data AI Summit about graph powered machine learning. And if you're really interested, there's, kind of, AZ workshop tomorrow. And, then there's a longer version of the graph powered machine learning first steps, over at O'Reilly, also, taught, by by me. But I would first recommend visit the course tomorrow, and, then let's see about that. Thank you so much for listening, and, feel free to ask any questions. If you don't, feel, open or incentivized to ask them now, also feel free to reach out. Thank you so much.",
  "language": [
    {
      "id": "http://vocab.getty.edu/aat/300388277",
      "type": "Language",
      "_label": "English"
    }
  ],
  "created_by": {
    "id": "https://example.org/cdkg/Creation/presentation/12",
    "type": "Creation",
    "caused_by": [
      {
        "id": "https://example.org/cdkg/Activity/presentation/12",
        "type": "Activity",
        "_label": "Presentation of \"Graph Analytics vs Graph Machine Learning\""
      }
    ]
  },
  "digitally_carried_by": [
    {
      "id": "https://example.org/cdkg/DigitalObject/recording/12",
      "type": "DigitalObject",
      "_label": "Recording of \"Graph Analytics vs Graph Machine Learning\""
    }
  ]
}