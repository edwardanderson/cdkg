{
  "@context": "https://linked.art/ns/v1/linked-art.json",
  "id": "https://example.org/cdkg/LinguisticObject/presentation/37",
  "type": "LinguisticObject",
  "_label": "Textual content of \"The game plan for your Knowledge Graph-driven FAIR data platform\"",
  "classified_as": [
    {
      "id": "http://vocab.getty.edu/aat/300027388",
      "type": "Type",
      "_label": "Transcript"
    }
  ],
  "content": "This is, the final day for the Knowles Connections conference brought to you, by Connected Data London and the knowledge graph, conference. This is our second track, innovators, and, have the great pleasure of welcoming, here today, Sebastian Schmidt, who's the co CEO of MetaFacts. Co CEO is actually not very familiar, title, not one many people have. Sebastian does. And he's here today to talk to us about, how knowledge graphs, can power FAIR, platform. And by FAIR, well, it's an acronym you may be or not be familiar with. Sebastian will explain it. I'll give the floor to him. Thanks a lot, George. So also welcome from from me on the presentation on your game plan for building a knowledge graph to a fair data platform. So, a few words on myself. My name is Sebastian Schmidt. As as George mentioned, I'm co CEO at MatterFacts. So I'm, running the company together with Peter Hasso, who founded it in two thousand fourteen. I actually come from a cloud and and, infrastructure background, but I've always been a big fan of knowledge graphs. And, I'm happy for being about one and a half years part of a truly digital business and working with a lot of knowledge graph experts. Keep people behind, for example, the the owl draft or also pushing on on new topics like, RDF style and sparkle style. So, who is MetaFacts? As I said, we started out in two thousand fourteen. We are based out of Waldorf, Germany, and, we have an international team really across multiple locations from Australia, Germany, Russia, Ireland, all over the globe. We have one product which is Meta Factory, our knowledge graph platform And we use that product, to drive digital transformation with our customers. And that's specifically around, unlocking the value of the data assets using knowledge graphs as the underlying technology stack. And what I want to present today is an approach, we have implemented with a number of customers in in different verticals. So we are working with customers in in cultural heritage and digital humanities and engineering, manufacturing, finance, insurance, pharma, and and life science. And we have built knowledge graph based applications and and tools from, a bit of material management to a digital museum, solutions like for for drug discovery and and drug repurposing. So, really a lot of different domains where we bring in the the expertise of how do you build a knowledge graph and, how can you do that really fast. And our customers bring in their specific domain expertise. So, what is it about digital transformation? Where do we see this demand coming from for building such knowledge graphs? We see that a number of drivers are behind why companies wanna establish a digital transformation strategy from, really just enabling innovation, bringing new products to the market, driving additional business, to increasing engagement with, the employees, internal customers, partners, to improving operations. So, very often, it's also just an an internally focused effort to make sure that internal processes once mover access to relevant information is available at the time it's needed and, is complete and is reliable. When talking about how to get to digital transformation, this is something you might have seen before. So there's this approach of, first talk to people, talk to your customers, talk to your partners, talk to your employees, and understand the gaps and the needs. Then it is finding the right data, using the data in the right way to drive digital processes and with those, digital processes to support them with digital tools. The approach we are taking on that is is a truly agile approach, and I'm gonna, highlight that a few times throughout the presentation. So it's all about planning, building, testing those individual steps and, iterations on each of those steps as needed. So wherever we are in that process, the idea is you can at any time go back to the previous steps, fill gaps that you identified later on, retest new hypothesis, and and make sure that you are actually achieving the the goal that you have set out. And it might even be adjusting that goal, as you are going along. This is a lot of ground to cover. So today I'm gonna focus really, purely on the the data aspect of this. So how do we build FAIR data? FAIR data that stands for findable, accessible, interoperable, and reusable data. And I will highlight three of the key steps we see which is describing your data, modeling your data, and then using your data. As mentioned, we are using the semantic web knowledge graph standard for that to derive knowledge from this data and to also further drive prediction with artificial intelligence and machine learning. So when we talk about data and digital transformation, most of our projects actually start from the point of looking at what kind of data is available. And I think there's no enterprise that has a lack of available data but the challenge is usually about, what can I actually do with this data? Is this relevant data for what I wanna achieve here? Who created it? So who can I reach out to to get more detail about it? Which processing steps have been applied to this data? Is this, in some way modified? How do I interpret that correctly? Is that, reliable data? So do I have contradicting data available or other data that is, supporting the information I would derive from it? Where and when was it created? So is it still relevant? And what was the intended use? So with what, idea was it initially created and and that's that properly applied to, what I'm planning to do with it. And there are a number of approaches we see that our customers have taken to answer those questions and get a better understanding of their data. The most successful approach at describing your data is is really taking all of your data, all of those digital assets in the same way. Meaning, that could be any kind of private data or public data that is not yet cataloged, that is just existing in in some way in your enterprise, or existing catalog solutions you might have already established, which we very often see are building on proprietary standards, are building mostly, as a siloed solution. So, most customers I'm working with have actually two or three catalog solutions in place. None of them covering, let's say, a very significant part of the data. It's it's usually in the range of twenty, maybe thirty percent. And the idea is bringing in documentation provenance and lineage. And what we are proposing for that is is building on those w three c standards. So building on the RDF data formats to build a semantic data catalog and using, vocabularies from from DCAD and BRAVO to, on the one side with DCAD model really What kind of data that is, where it comes from, and and how you can reach out to get more information about it, how you can get to the actual source of that data and combining that with Provo to understand the kind of transformations modifications that have happened to the data and, what happened before this data actually reached, the the system where you are interacting with it now. So to make it a little bit more visual, I have a demo. I have a fallback version as a video, but let's try if we can maybe have the actual demo system. So this is our, meta factory tool and, we have a number of asset management interfaces built in here. And I wanna start out from the data cataloging interface here. So what we can see is all of those different properties defined in in DCAD to filter the available datasets we have in the system. We, for example, can look at the link Nobel Prize dataset here. We can, look at that. And what we are doing is we we populate, a page in, some kind of a linked data browser kind of concept. So we are now on the on that individual dataset. We get information about this dataset. We can see the URI and the type of this dataset we have here. It is possible to modify the dataset. And all of those configurations are directly loaded from the DCAD model. The model that's defined with that DCAD standard, that open standard is directly driving the user interface and, how information is presented. From here, I can also explore the graph that's behind it. So I can, for example, see all of the incoming and outgoing links for this data set. I can see the the publisher. I can also look into the the model information of type data set. I can see there are number of other resources that are of that same type. So we can take one of those in as well and and see the the different kind of datasets we have in our knowledge graph and really navigate from one resource to the other and and get a better understanding. We have, auto populating knowledge panels that show you context information about everything in that knowledge graph. And through that, you can really browse your datasets and build a central system driven by an open standard by an open data model where you can learn about all the different datasets that are available to you and that you are working within your company. Now the next step is modeling your data. So now that we actually have information about where this data comes from, what kind of data that is, and, where we get to to more information about it, it is very important to put a model on. And I'm not talking about a classical relational database model or something like that. So this is not to describe the structure , the data types of my data, but this is about describing my domain model. Because it's very important if you wanna be effective in using the data at hand that you describe, what kind of data it actually is and how the different concepts and relations are interacting in your specific dataset. So what we are seeing here is a case from from pharma domain where we have a number of data sources in our data catalog and we are showing here how those public available data sources actually interlink on a on a higher level, model. So we see that we have genes and and proteins and and transcripts which are all, subclasses of biomolecular entities. And we see how a protein connects to a pathway. So without actually being in the pharma domain, I can get a basic understanding of of what kind of things are modeled in this data. What kind of information I can extract from that. And that's the idea of this approach. We are using, the OWL and and Shackle standards for that. So so most of what you're seeing here is actually modeled through OWL, But we see that, you also wanna model things like constraints. So you might wanna define, this relationship between gene and protein. So this encodes relationship we have here. How many, proteins can be encoded by a gene. And to define that, OLL doesn't give you any way to do that. So that's why we are combining it with Shackle to allow you to model those constraints as well. So, again, how does that work? So from the the perspective of metafactory, that's just another asset. So, in this case, we're looking at an ontology asset. You can also see that we have, an integration with with git here. So those are all not versioned in git right now but we have again the Nobel Prize dataset here. We see this is versioned in git with no active changes on this. And we can go into that now and we can really modify our model here. So what we see are, again, the the core concepts of this dataset. We are seeing, those constraints like there's an exact one relation between a word file and a file type. And one common step you would wanna do is, after you have actually built the ontology, which you can do in here, so you can can modify parts of the ontology. You can add relations. You can add additional classes. But you can also tie it into, other ontologies. So we see a lot of customers building kind of a more higher level ontology where all of the specific domain ontologies can tie into. And that's also why you're seeing such a lot of classes in here. So if you, for example, would only look at the local ontology you can see it's it's a lot less here. So let me quickly jump back to to all ontologies and, pick out something that does not come from this ontology itself but comes from another ontology. So for example, we pick out the the agent and, the document. We can see that for them, there are already relations defined. And now we can go here and say that an award file and take the subclass relation is actually a subclass of a document. Lawyer is actually a subclass of an agent. And now we can save this change. And we can also now do validation. So we can actually see how much of our data is conformed to this model. So if I'm going back to my, ontologies, I can do a validation step here and we should now get a new entry here for new data quality report. And we can see that, with our latest report, we have a few elements which are not conformant, with our model. So we can right away go in here and we can learn that the minimum cardinality for for file type and for year is not, correctly set for some of those elements. And we can now learn also which specific note does not provide it. So for example, here we see the device file. And that leads back again to our linked data browser. So the idea is really that everything is in the knowledge graph. The catalog is in the knowledge graph. The model is in the knowledge graph. And also the data validation information is part of that knowledge graph. And everything is interlinked and can be accessed from every point, of where you are in your dataset. The deontology or your schema, your model, that's one piece. The other one is, bringing that together with a controlled vocabulary. We are using SCOS for that. So you see an example where we are modeling, the the controlled vocabulary for diseases in this case. And then, we have built that system so that you have interfaces for specific user types. So we are seeing more and more customers adopting the the ideas of a semantic modeler being responsible for that overall modeling process, kind of the the gatekeeper to the model. Domain experts actively participating in modeling as well as, building out the vocabularies and data stewards that have some, interaction with those those models or ontologies, but are mostly active around building the data catalog, bringing the data into the system, mapping it into models and vocabularies, and making it available. To support that process, and and I don't really have time to talk about that a lot, but we have also implemented the W3C reconciliation service API draft. So you can use that to have a standardized interface to look up what is the idea the the IP for one of my, resources or entities. So every new dataset and all the new data you are bringing into your knowledge graph will correctly map into what's available already. So the third and final step is using your data. And that's interestingly one that's often overlooked when knowledge graphs are built in the beginning. And that has been, one of our focus points all along. So we, together with our customers, built, wireframes. So really discussing the the requirements from from data, people, processes for the user experience. And then take that into Meta Factory, which is also, a model driven local platform. So that that linked data browser you saw as well as all the interfaces, they are highly customizable which means they are just HTML pages, that are connected to a number of of templates where we use our, predefined and graph enabled components to put those visualizations and interfaces together. And everything that you would like to have in a different way you can modify. This all sits on top of a graph database where the data is stored. And all of the configuration like you saw it for the model, but also for those template pages is stored in a versioning system. Which makes ultimately the the platform itself a stateless component that's just putting the things together and creating the interface to interact with it. Or in the same way providing that through APIs to a larger ecosystem of analytics prediction, AI, ML solutions. We do have a public demo system based on Wikidata. And, if you if you open that system on wikidata. Matter of fact dot com, there's an example gallery of some of the components we provide. And I just wanna show you, two examples quickly. So there is one option here to combine a keyword search with a a tree visualization. So we are seeing the, the tree of, diseases here and and how that that connects in this taxonomy called MeSH. And here we can look behind the cover of how this is actually built. So what you see is it is our tree component. It has a query to find, parents and it has a query to do the the keyword lookup. And interestingly, you can also see that we are also federating here over a number of repositories combining mesh with Wikidata. So these few lines give you that user interface, that you're seeing here where you can freely interact with this data. Which means if you are, able to write a sparkle query, you are actually able to write a front end interface in meta factory. And if you are not able to write a sparkle query, we have an interactive query builder. So you can actually say I'm looking for a person connected to an organization, where that organization is the employer, and where that organization is related to a place, which is the headquarter location. And that might be London. And now we select the London we are interested in. And now we get a result and and we can extract the sparkle query from it. We can also combine it with and and or clauses and make it obviously a lot more complex query. But through that, I can derive my results that I can derive, an actual spark agreement that I can use then in those visualizations or I can even provide that as a query builder to my end users to interact with the dataset. So we have built a number of systems as I said. It's probably around one hundred applications we have built before our customers in the last years. Just some few examples here. And, as I mentioned it before, all of this is following this agile approach. So, that's also true for all of those assets, datasets, mappings, ontologies, vocabularies as well as the the UI templates. For all of them, we are following an agile approach with versioning in in Git and relevant governance processes around it. And the idea is that with that approach, you can really go from a proof of concept, in just one to two weeks to an an MVP, and a production system in a month or even less. So we have successfully implemented that for for some use cases in in just two weeks, getting something to the end user, being able to iterate in weekly sprints, getting feedback in getting additional data sources in expanding the model, expanding the vocabulary and so on. So, yeah. With that, I think I'm at the end of the the twenty five minutes. That that was the end of the twenty five minutes, and that was also me muting. What I wanted to do was actually enable my screen, Sebastian, for for setting up your your screen. At least it it happened after you had finished your your talk. So, it's a good time to open the floor to questions, and we already have one, actually. We've had that almost since the, the beginning of your talk. It's, one from Dean Aleman. It's a very specific one. So if, Dean will allow me, I will, broaden in the the scope a little bit. So, Dean's question is, if someone's already using Collibra for their data catalog, is there an easy way to migrate that into a knowledge graph like the one, you showed in your slides? I would expand the scope a little bit because, the question, excuse me, the question is specifically about one data catalog, solution. There are there are many in the market. So while it's generalized, like, is there a way for someone who's using some whatever other data catalog solution to to to transition, let's say, to what you're using. So we we are seeing that specifically, specific one quite a lot, actually. So I would say it's, it's part of, like, eighty percent of the solutions we have built around, data cataloging. And we are not yet where we wanna be with that. So, as we, for example, have also enhanced capabilities around the the visual ontology building just with the last release to the the point that you saw just now. We are also looking into how we can make that, onboarding of existing catalogs smoother and and easier. Right now there's there's some manual process still involved with that, but we we see that, over the I would say next year, this becomes a highly automated process to to speed up those projects, even further. Okay. Okay. Thank you. We also have, again, a rather specific question from from Daniel. I don't know if it's one you can answer, but I'm gonna pass it on anyway. He asked about pricing and whether we provide licenses for research purposes. Yes. So we, we are ourselves very active in research. So we also wanna support, research. And and therefore, if you are interested in an academic license or something like that, reach out. We have, under matter of fact dot com slash get started. We have a a public trial that's accessible to to everyone. You can run this in the cloud or on premises. And, if you have a research academic need, just put that into the, the comment field or reach out via email and we'll give you, directly access, to that. Okay. Actually, I also have a question for you, because at some point, you mentioned the agile approach and, iteration and versioning, but it's for data. And this is something that I see coming up actually a lot, increasingly, I would say, beyond the the confines of data cataloging or knowledge perhaps or what have you, it's something that's that's that's there is a need for, and it's getting more widespread. So even for people doing machine learning or any dealing with with data and data science in any other capacity, let's say. Because people are realizing it's a bit like code. So you do have iteration. So your data set will change. Your data set will evolve over time. So you need a way to keep track of that and different versions and which version everyone has and all of that. So how do you approach and resolve this issue in your platform? So for for UI templates, for ontologies, for vocabularies, it's fairly straightforward because the the amount of data is usually quite controllable and and the approach with using a versioning system like git for that is what's implemented most of the time. For data, it becomes a little bit more tricky because sometimes that's just not the approach that's that's relevant or interesting there. So we have a number of ways how we can do the the versioning there. We are, for example, working with, different, named graphs. So depending on the technology that's underneath in the database available to provide kind of a staging area for datasets, or other approaches around maintaining specific user changes. So we also have the the capabilities of providing those forms and every change is then tracked with provenance information from the user and versioning information. So you can actually play back all the changes that that users have entered entered into the system. And in the same way, you can also do that for for other ways how data might get into that system. So for for data, it's still a little custom based on the specific need. I think there's there's no one way to do it right, but we have, implemented that from just small scale to to large scale changes. And we we have approaches to to help with that. Okay. Yeah. I think the, the named graph solution that you mentioned makes makes sense. But as you also pointed out, there is can currently, to the best of my knowledge, at least not the standardized way to, to move between different name graphs. So I guess you have to somehow add some proprietary elements to that. It's it's actually we're not really applying a proprietary element, other than, you know, maybe a kind of a an extension to the vocabulary that we specifically use for that. It still stays RDF. It still stays, open and standardized. And what we can, for example, also employ for that is is LDP containers. So that's that's another technical approach, to help us, enable that. Okay. Okay. Yeah. I was just specifically interested in that because, yeah, as I said, and you also mentioned it's it's an issue, that many and many more organizations are facing. So I was just curious how you deal with it. Okay. So what yeah. We'll we'll have to be wrapping up so that, people can move on to, the next session. Thanks, everyone. Bye. Bye.",
  "language": [
    {
      "id": "http://vocab.getty.edu/aat/300388277",
      "type": "Language",
      "_label": "English"
    }
  ],
  "created_by": {
    "id": "https://example.org/cdkg/Creation/presentation/37",
    "type": "Creation",
    "caused_by": [
      {
        "id": "https://example.org/cdkg/Activity/presentation/37",
        "type": "Activity",
        "_label": "Presentation of \"The game plan for your Knowledge Graph-driven FAIR data platform\""
      }
    ]
  },
  "digitally_carried_by": [
    {
      "id": "https://example.org/cdkg/DigitalObject/recording/37",
      "type": "DigitalObject",
      "_label": "Recording of \"The game plan for your Knowledge Graph-driven FAIR data platform\""
    }
  ]
}