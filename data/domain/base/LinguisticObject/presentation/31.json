{
  "@context": "https://linked.art/ns/v1/linked-art.json",
  "id": "https://example.org/cdkg/LinguisticObject/presentation/31",
  "type": "LinguisticObject",
  "_label": "Textual content of \"GraphEDM: A Unified Framework for Machine Learning on Graphs\"",
  "classified_as": [
    {
      "id": "http://vocab.getty.edu/aat/300027388",
      "type": "Type",
      "_label": "Transcript"
    }
  ],
  "content": "Hello, everyone, and welcome to our third, session in the innovators track for knowledge connections twenty twenty. It's been really interesting so far, and we're pretty sure, you'll be, satisfied with, this talk, as well from, Ines Sami, who's a PhD researcher in Stanford University. Ines will be sharing with us today, Graph EDM, which is a framework that, they have developed with her colleagues. And it's about graph machine learning and neural networks and different classifications. Hello, everyone. My name is Ines Chami. I'm a PhD student at Stanford, and today I will present a recent framework for buffer presentation learning. This is a joint work with my collaborators from Google and Stanford. So this talk will be split into three sections. I'll first start by reviewing the problem setup for graph representation learning and discuss different variations. I'll then introduce our graph EDM framework and use it to describe some graph representation learning methods, both supervised and unsupervised. So let's first start by reviewing the graph representation learning problem setup. As you probably know, graphs are a universal data structure used to store relational data. In this representation, node represents objects and edges represent relationships between them. These graphs are prevalent in many real world applications. So for instance, knowledge bases such as Wikidata, which contains, the Wikipedia, data store knowledge about the world in the form of the knowledge graph. Form nodes are world entities and edges represent the relationships between them. Molecules can also be represented as graphs where nodes represent atoms and edges chemical interactions between them. And another important application is in the domain of biology where the study of evolutionary relatedness between species is done using phylogenetic trees. Note that there are many other applications not listed in this slide, such as web and Internet graphs, recommendation systems, economic and transportation networks. And so the summary here is that these graphs can be used as a shared vocabulary between fields. And if we can develop methods that operate on graph structured data, then we can have potentially impact on many important applications. To apply machine learning methods and neural networks on graph structured data, we need representations that a machine learning model can operate on, and these are usually feature vectors. This is the goal of GRL, which aims at learning representations for each vertex in a graph while preserving the graph information. So for instance, if two users have many friends in common in a social network, then good representations should preserve this information by making the known representations similar in the embedding space. And these representations can then be used as input features in downstream machine learning applications. However, graphs are sparse, discrete, and high dimensional objects, and so it's not clear how to get a mapping from a graph to a low dimensional, dense, and continuous feature representation while preserving all the important graph information. And so we'll see next how to design models that solve these challenges. So there are two main learning scenarios for graph representation learning, supervised and unsupervised. In the unsupervised case, all we have is the input graph structured data, and we wanna get representations that preserve the graph information. Examples of unsupervised tasks include link prediction, where we wanna predict whether two nodes are likely to become connected in the future. For instance, in social network, we may wanna predict future friendship relations. Other examples include graph visualization and graph clustering, where we wanna put vertices in a graph into different clusters, for instance, to detect communities in a graph. In the supervised setting, the goal is slightly different. And while we do wanna get some vertex representations, the goal here is to get representations that are predictive of some graph properties. So for instance, in node classification task, we wanna predict the attributes about nodes in the graph. For instance, we could try to predict fake accounts in a social network. And note that in contrast with the clustering task, node classification is supervised in the sense that we do need the label classes beforehand, and we need some training data to train the model. Finally, another important supervised task is graph class classification where we may wanna predict an attribute about the entire graph. So for instance, in chemistry, we may want to predict whether a molecule is toxic or not, and this is a graph classification problem. So let's now introduce our GraphiDM framework to describe methods for unsupervised graph representation learning. The input in unsupervised URL is a graph, which is simply a collection of nodes and edges, and we store the edge information in the form of a sparse matrix with zero and ones, which is basically the edges in the matrix. We may also have node features representing the vertices in our graph, and we'll talk about these in more details in the supervised learning step. In unsupervised GRL, we set mappings from vertices in the graph to vector representation such that graph similarities are preserved in the embedding space, and we'll see next how to define these similarities. So let's now describe our GraphiDM framework. As we've mentioned before, the input to GRL models is a graph that comes in the form of an adjacency matrix decoded to construct an embedding similarity matrix. And these embeddings are then decoded to construct an embedding similarity matrix. Decoder network can be as simple as taking outer products of the embeddings or be a more complex neural network. The parameters of the encoder and decoder are then learned via gradient descent using an unsupervised objective function, which essentially tries to make the embedding similarity matrix as close as possible to the graph similarity matrix. The graph similarity matrix can be defined in many different ways, which gives rise to different embedding methods. So for instance, some methods may wanna preserve the first order graph proximity, namely the AGSNC structure, while other methods want to preserve higher order graph similarities. After optimization, we can extract the word embeddings and use them as features for downstream machine learning tasks. Let's now go over a simple example of encoders, namely the shallow encoder, which is a simple embedding lookup. We can get the embedding of a specific node by doing a matrix multiplication of the embedding matrix by the one hot vector of the corresponding node. The parameters in shallow encoders are the embeddings themselves, and there are no functions computed on top of these embeddings. In other words, a shallow encoder is a simple vocabulary lookup which stores a single vector per node in the graph. Note that there are many other possible encoders such as graph neural networks, which we will describe later in this talk. So let's now look at an example. We focus on the graph factorization model, which is a popular graph embedding method that preserves first order proximity in the graph by learning essentially a lower rank factorization of the adjacency matrix. In this model, the encoder is a simple embedding lookup as we've seen in the previous slide, and the decoder takes the outer product of the embedding matrix. That is, the embedding similarity between two nodes is simply their inner product. The embeddings are then optimized by minimizing an unsupervised loss function, which encourages adjacent nodes to have a high embedding similarity and non adjacent nodes to have a lower similarity score. Note that taking the embedding's outer product defines a symmetric similarity function, which may have, which may be a limiting assumption when working with directed graphs as some nodes can be strongly connected in one direction and disconnected in the other direction. There have been follow-up works for directed graphs which basically define two embeddings per node, a source and target embedding, depending on the direction of the relation it appears in. Another line of work seeks to preserve higher order similarity in the embedding space using random walks. In skip gram GRL, the decoder function is also an outer product but the graph regularization term is computed over random walks in the graph. So for instance, DeepWalk draws analogies between graphs and language by generating sequences of nodes visited during random walks and treating them as words in the sentence to get representations. All these extensions are described in detail using GraphiDM in our paper. So let's now move to supervised URL methods. In supervised URL, the input data is the same as in the previous case, but the goal is now to learn representations that are predictive of some vertex or graph properties. In this setting, we have some additional information about the graph, namely vertex or graph labels, and we want to learn mappings from vertices in the graph to some target label space. One possibility is to apply unsupervised URL methods to get representations and then use them as input in a simple neural network. However, an important limitation of this two step approach is that unsupervised representations might not preserve important information about the graph that would have been useful for the downstream supervised task. Supervised URL methods overcome this limitation by learning representations and solving the predicted task end to end. Finally, note that node features play play a critical role for predictive tasks. So for instance, in citation networks where nodes represent papers and edges represent the citations between them, we could have had feature vectors representing the paper such as what you're making for all the words in a given paper. And this information can be critical in some supervised settings. So for instance, if we wanted to predict a paper topic, then knowing that some keywords are mentioned in a paper can significantly improve the classification accuracy. For supervised methods, we need to add an additional branch in the GraphoDEM framework to account for the label supervision. Essentially, we add another decoder, which predicts labels from embeddings, and we still keep the unsupervised decoder, which can be used to regularize the embedding and account for the graph structure in the embedding space. But note that some methods discard this unsupervised decoder, and we'll see next some examples. So let's first talk about the label propagation model. Given a graph that is partially labeled, we want to infer labels for the other nodes in the graph based on the connectivity pattern. And, essentially, if two nodes are connected, then they are likely to share the same label. The labeled propagation algorithm uses the graph Laplacian to propagate labels in the graph. And to provide more intuition, the graph Laplacian can be thought of as a discrete analog of the Laplace operator, which happens to model the propagation of the heat equation in the Euclidean space. In some sense, labels can be thought of as heat sources on the graph, and applying different iterations of label propagation diffuses the heat or labels to unlabeled nodes. While label prop is an iterative algorithm, it can also be equivalently written as a an optimization based method, which allows us to describe labelprop in graph EDM terms. Here the encoder is shallow and embeddings are directly learned in the label space, so the label decoder is simply the identity function. The supervised loss term is simply the l two distance to the labels of labeled nodes, and the unsupervised decoder uses l two distances between labeled vectors to compare pairs of nodes. And the unsupervised regularization loss compares these distances to the true graph aj s and c matrix. Minimizing this graph organization term essentially will encourage adjacent nodes to have their labels closed in the embedding space. And we can show that minimizing the sum of the supervised and unsupervised losses recovers the iterative solution from the previous slide. Finally, let's look at a recent and very popular approach for graph embedding. All the methods we've covered so far used shallow encoders and were therefore inherently transductive, in the sense that they could only get representations for nodes that were present at training time. The graph convolution model generalizes convolutions to graph by learning parametric encoders using node features. The total number of parameters in this model is independent of the number of nodes in the graph, and the learned encoder networks can be used to get representations for many input graphs that have the same input feature domain. And this is basically the inductive setting where the models can generalize across different graphs. One challenge in designing convolutional neural networks for graphs is that graphs have arbitrary structures, while usual neural network architectures operate on regular data such as sequences or images. So for instance, in computer vision, CNNs are used for automatic feature extraction by applying small localized features which activate in the presence of relevant panels. And due to the regular structure of images, these filters can be shared across many different locations in the image, making CNNs computationally very efficient. With graphs, this is much more challenging because graphs don't have this regular grid structure. So for instance, the number of neighbors varies from one vertex to another. And so to solve this challenge, GCNs make many simplifying assumptions by adding some weight sharing constraints in the model and also letting nodes interact with their direct neighbors in these small filters. So this is basically the message passing framework where at each step, nodes aggregate information from the neighbors. And as these iterations progress, each node embedding contains more and more information from further nodes in the graph. This message passing allows to preserve both structural properties of the graph, since nodes communicate only with their neighbors, but also feature information since these messages are computed from input features. The GCN model can also be described using GraphiDM. In this scenario, the encoder uses both the adjacency matrix and node features, and no unsupervised decoding is needed since the structural information is already leveraged in the encoding step. The label decoder is simply the identity mapping as GCNs map input features directly to the label space. And the encoder functions can be written in matrix form using the adjacency matrix and the degree matrix. It first transforms input features to compute messages and then aggregates messages via the graph ID as in its structure. Finally, GCN stack many of these layers using nonlinearities and the model parameters are learned by minimizing the cross entropy loss over the labeled data. This is the end of this talk. If you'd like to learn more about GRL, feel free to check out our silver paper, which has many more detailed descriptions of the different methods. In particular, I mostly focused on methods that learn presentations in occlusion spaces, but note that it is also possible to embed graphs into non occlusion spaces to better match the graph geometry. And so if you're interested about this topic topic is perfect to reach out to me or check out our first. Thank you for listening. K. Great. Thanks thanks for the presentation, Ines. And, good to have you, here with us in the flesh as it were, even though, you know, it's it's the closest, it's the next next best thing, actually. So Thank you. Our pleasure. I see that people are just now starting to to type in some some questions. So, I'll give them some time. And, I have actually written down a few of my own until we have, some from the people. So one question for me. You mentioned at some point towards the end of the presentation that one issue that, you have when using graph neural networks as opposed to, the the classic ones that says, precisely the lack of structure, and that forces you to you you some some some tricks, basically. Could you expand a little bit on on that? What does the what precisely is the issue that the lack of structure causes? Because, you know, this is not in any way my my expert graph machine learning, but based on on the cube, my the little knowledge I had I had the impression one of the benefits of using that is precisely the ability, to to add more information and, represents all kinds of data structures, basically. So can you expand can you explain a little bit what we what that issue causes, basically? So just to make sure I understand correctly, because the network is is breaking a little bit, are you asking about the lack of, regular structure in graphs and, how it makes this issue? So if you think about an image, it can be part of it as a graph in some sense, which is a grid graph. So just, two directions where every node is connected to its top left and bottom right, neighbors. So this structure is very irregular in the sense that if I move and I translate, do some translation along the image, then the neighborhood structure is gonna be exactly the same wherever I go. So each node is gonna have, again, a top left, bottom right neighbor. And so because of this regular structure, convolutional neural networks on images, can do many simplifying assumptions like, sharing filter weights across different locations in the image, which makes them very computationally efficient, but also allows them to extract, local features at different locations. With graph, if we wanted to do with a general graph, not necessarily a grid, if we wanted to do a similar method, we would have to design a way to translate those little features across different graph locations. And because these graphs have an arbitrary structure, so one node may be connected to three other nodes and another node may be connected to only one other node, it's unclear how to move those filters across locations in the graph. So that's essentially the challenge in designing methods that operate on these graphs, for, feature extraction and pattern recognition. And so, essentially, the GCN makes, some simplifying assumptions. So for instance, the weights, that are gonna interact, from, different neighboring nodes are all gonna be cool, whereas in, standard CNNs, we have the flexibility to have different weights for different neighbor interactions. So we usually make those simplifying assumption to make these, CNNs work on arbitrary graph structures. So I'm not sure if that under that answers the question. Yeah. Yeah. Yeah. It does. It does. It also brings up a follow-up question, actually. So these precise, assumptions that you mentioned, how do you make them explicit in some way, or should you perhaps make make them explicit in some way, like adding some kind of metadata basically to, to to your network so that you know, like, okay. This is my assumption and this is the basis I'm operating on. The assumption about sharing the weight constraints, is that, what you're asking about? Yeah. Yeah. Yeah. Exactly. Yeah. So when we design the models, we, explicitly have those inductive biases. And so we're gonna design, convolutional layers that account for these assumptions. So, yes, when we design the models, all these assumptions are taken into account, in the implementation, so that they can work in practice on on real graphs. Okay. We have a question from the audience here from Roberta, who's asking if you can please contrast the impact between shallow and deep neural networks. In other words, what drives to increase the complexity and number of layers and convolutions of the network? The kinds of problems or the accuracy? So I think there are two threat of, like, research progress in this field. One is more driven about state of the art. So we have some datasets, and we wanna improve the, as as Roberto said, the accuracy on these datasets. And so in this dataset, it's really driven by, architecture choices, and we're gonna make small tweaks to see whether, architectural change can make improvement. And then I think there's the practice where, maybe those architectural changes are gonna give some small improvements. But then in complexity, they're adding a significant overhead. And so we're gonna go for simpler methods that, can work in practice and, work efficiently. So there are those two threads. And depending on whether we're trying to get a paper and, like, make some research progress or whether we're trying to use these GCNs in applications, to do some prediction about real data, then we're gonna be, choosing one method or the other. Okay. Okay. That's that's actually something I also wanted to to ask about. If there are if you you, specifically, personally are working on applications like, industrial or real world applications of, of this framework. So we've been working, with knowledge graphs, a lot. So knowledge graphs are, specific type of graphs where the relationships have, edge types. And so the application we're considering is link prediction in knowledge graphs. So how can we use the, how can we use the knowledge graph structure to predict missing links, about, entities? And so for instance, in Wikipedia, I guess people are familiar, in the context of this conference. But so, like, trying to predict missing facts in, in knowledge basis, to complete, the the knowledge graph structure, and we're also using them for natural language processing applications. So because these knowledge graphs have, a lot of knowledge about real world entities, we can get representations that capture this knowledge, and they can be used, as input to, neural networks that do any language task like question answering. So we're working in this. Like, for instance, name entity disambiguation. We have, we may have, like, many entities in the sentence that, conflict with each other, and we wanna know which entity sentence is referring to. So having these knowledge graph, embeddings that give us, some more additional context about entities can be, useful in in those natural language task. Okay. That's that's interesting. Actually, I just recently, so discussion, which seems very relevant to to what you're saying. And it's been actually an ongoing, a long standing, let's say, issue in the in the knowledge graph board, precisely what you described. So being able to infer additional, additional links, basically, between these these different entities. And so far, the the general assumption has been that doing that is is a good thing. But, recently, I saw a discussion with people some people, including actually, one of my mentors, Frank van Harveen, like, one of the leading scholars in his domain, was was kind of questioning this notion. Like, the the idea was and some people were agreeing. Actually, the idea was that, well, it it all depends on on, you know, how how accurate you can be in this, link, prediction. If if it's not accurate enough or even if it's not very, very accurate, you may be doing more harm than good with with this section. Yeah. Yeah. I guess this is the precision recall trade off. So the model is gonna return many answers, and we have to choose a cutoff, for when we consider the answers are, not confident enough. So, yeah, this is probably a challenge with these link prediction, models. Like, we don't know when, when to stop adding links in the in the network. Is there are you aware of any, any methods that you can you can set this this this boundary to to be just about right? I'm not sure because in my work, I haven't worked with, like, explicitly the completion. So we use completion to get representations, but then, like, during test time, we don't actually, complete the knowledge graph, but we just get the, representations that we've learned for other machine learning tasks that require the knowledge. So we don't have to make those explicit choices for when, to stop adding links in the knowledge graph. Okay. But that's the interesting point. Okay. Yeah. Fair enough. We have another question from Matt who's asking how are different meta features on Edge data managed in the adjacency matrix? For example, has a or belongs to or is a in a knowledge graph. Quite relevant question. In the talk that I presented here, I only considered graphs that don't have edge types. These were only whether two nodes are connected or not. In the general knowledge, graph application that I mentioned, we've worked with graphs that have edge types. So, usually, the edge types are just, categories. So we know that, has or belongs to our different categories. We haven't tried, adding edge features in the model, but I know this is something that, people have worked on. So, like, maybe some relationships are very similar to each other, and so having some representations for the relations rather than treating them as completely different categories, can improve the model's quality. But, I haven't worked with edge features, in in my work. Okay. Would that be perhaps one, future, future direction for this research? Yeah. Definitely. And even in the just in the graph, neural network area, I think there's been a lot of work in trying to incorporate those edge features. So, essentially, the GCN update, aggregates messages from the neighbors. And, in the work in the talk I presented, the aggregation is just an average, of messages from the neighbors, but we can also extend that to incorporate edge features. So when we aggregate information, we also have some transformation that accounts for the type of relation that the message is coming from. So these are definitely, ongoing research directions, especially for, knowledge graphs that have, different, edge relations. Okay. Other future directions for for your work? So my personal research is mostly focused on, nonaclegian representation. So I've, briefly mentioned that at the end of the, the presentation. Essentially, all the all the methods I've, talked about in the in the talk were, occlusion in the sense that the representation slide in occlusion vector spaces where we can take inner product l two distances. And my research is focused on nonacledian representation learning. So, essentially, here the representations lie in, curved spaces. And so these spaces don't have a vector space structure, so we can't just take two points and add them. So for instance, if you think about a sphere, it's also it's a space of positive curvature. And if you have two points on the sphere and you do the Euclidean addition, then the points are gonna lie within the sphere, which is not valid, does not respect the geometry of the space, which is this curved surface. And so the challenge is in, with this, with this, spaces is to design, operations that can, that are valid in these, in these curved spaces to, do machine learning in, so for instance, hyperbolic spherical spaces. So this is the focus on my research. And the high level motivation is because, tree like data and hierarchies in general can be represented much better in hyperbolic spaces than in occlusion spaces because the hyperbolic space is like some sort of continuous version of trees, and it has a polynomial volume growth. And so because it has this growing structure, it's more suitable to represent, your archical data. And so that's why, we've been working with those, alternate geometries to represent your archival data. Okay. Thank you very much. It seems, we don't really have many more questions. Thank you so much for having me. Thank you very much. Bye bye. Bye.",
  "language": [
    {
      "id": "http://vocab.getty.edu/aat/300388277",
      "type": "Language",
      "_label": "English"
    }
  ],
  "created_by": {
    "id": "https://example.org/cdkg/Creation/presentation/31",
    "type": "Creation",
    "caused_by": [
      {
        "id": "https://example.org/cdkg/Activity/presentation/31",
        "type": "Activity",
        "_label": "Presentation of \"GraphEDM: A Unified Framework for Machine Learning on Graphs\""
      }
    ]
  },
  "digitally_carried_by": [
    {
      "id": "https://example.org/cdkg/DigitalObject/recording/31",
      "type": "DigitalObject",
      "_label": "Recording of \"GraphEDM: A Unified Framework for Machine Learning on Graphs\""
    }
  ]
}