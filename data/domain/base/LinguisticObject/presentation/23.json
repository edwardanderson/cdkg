{
  "@context": "https://linked.art/ns/v1/linked-art.json",
  "id": "https://example.org/cdkg/LinguisticObject/presentation/23",
  "type": "LinguisticObject",
  "_label": "Textual content of \"Systems that learn and reason\"",
  "classified_as": [
    {
      "id": "http://vocab.getty.edu/aat/300027388",
      "type": "Type",
      "_label": "Transcript"
    }
  ],
  "content": "Good morning or good afternoon, depending on wherever you are on the globe when you are watching this. My name is Frank van Harmelen, and I would like to discuss with you in the next half hour AI systems that learn and reason. So first, let's take a, a broader look at at where I where modern AI is is going. So here are are just two quotes from from recent blogs and from the web page of recent AI centers that have been initiated. And they all talk about making artificial intelligence more human centered, making AI systems that support people and that are competent partners. So the emphasis in modern AI is is less on replacing, people by AI systems, but more on AI systems that collaborate with people and support them. But at the same time, here's another quote from a popular blog, current AI systems are often incompetent because they lack background and, and contextual knowledge, and they cannot even explain their actions. And so, clearly, current AI systems with their lack of background knowledge and and their lack of contextual knowledge and their lack of ability to explain themselves, they are not very human centered, and they cannot support people, and they cannot be competent partners. Right. So, what's holding AI back? Right. So why are we in the situation where, on the one hand, we would like to build systems that support and collaborate with people, but on the other hand, our systems are somehow incompetent and they lack background knowledge and contextual knowledge about the tasks that we ask them to do. And, a a common analysis these days is that what's holding AI back is that for a long time, AI researchers have locked themselves into one of two towers. Now we all know the famous story of the two towers. Right? And, in the case of AI, we could call these the symbolic AI tower and the statistical AI tower. And as as always in a kingdom with with two towers, one tower thinks that the other tower is evil and that they are wrong and that you should not really discuss, with with them. You should even fight them maybe. And in fact, maybe there's even another analogy that is appropriate here, which is that AI was not only divided as two towers, but maybe, you know, people even had sort of different religions about what AI should be. You could either be of the statistical AI religion, in which case you would build neural networks, and train, machine learning programs, or you were of the symbolic AI, religion, in which case you would build knowledge bases, you would build very large large models graphs, and you do inference over them. And just as with religions, you know, statistical AI and symbolic AI, they had their own books, they had their own meetings, and you didn't really need to talk to each other because, you know, the other people were wrong anyway. And fortunately, I I think in the last few years, we are beginning to see in AI an increasing movement that these, these two different streams in AI, they are beginning to talk to each other, and they are beginning to understand, that we actually need them both because our brain uses both. Right? Our brain does both the the stuff that that machine learning is good at, and it does the stuff that knowledge representation is good at. So here is, a very rough sketch of what's going on in our brain when we look at an image. Right? So here's a a cross cutting of somebody's brain. They're looking out towards the right. It's their eye. So what happens when, light hits the retina? Well, when light hits the retina, of course, there is the optical nerve that go to the visual cortex. Right? We call this the the bottom up signal. Right? And this this bottom up signal, when it hits the visual cortex, the visual cortex analyzes, forms, shapes. It it recognizes separate objects. It recognizes motion. It recognizes object of conclusion, and, it interprets the scene. And these signals then go to our prefrontal cortex, where we have, memory, where we do planning, where we do problem solving. But interestingly enough, in our brain, there is also a downstream signal. Namely, the prefrontal cortex is continuously telling the visual cortex what to expect. So it's not just a matter of the pixels feeding the symbolic representation, it's also a matter of the symbolic representation telling the pixel processors, what to expect and how to interpret all these pixels. And in fact, without such a downstream signal, our visual cortex would never be fast enough to interpret all the visual signals that come in. And, and actually, the downstream nerves, are actually larger than the upstream. So there is a a constant traffic up and down between the symbolic and the statistical parts of our brain, if you allow me that, that freedom. So now we could call the upstream signal data driven, as in neural networks, machine learning, and we call the the downstream, signal knowledge driven. And, actually, the history of AI also shows this. It it is not the case that the history of AI in the past, we did symbolic stuff, and now we realize that the symbolic stuff was, wrong or somehow mistaken. And now we are seeing the light and we're doing neural networks and and, we're making good progress. The actual history of AI has been a constant pendulum between these two streams, and frequency of this pendulum is about a decade. And, and certainly in the last decade, we have seen large major breakthroughs in the the the connectionist data driven statistical machine learning, part of AI. But we have also seen important, scale up, in the the symbolic, and knowledge based logic driven and reasoning part of AI. And, we are now seeing the pendulum swinging back to the middle of this spectrum, and, hopefully, we can this time, we managed to stop the pendulum somewhere in the middle so that we can combine these two. Now to put a little bit more flesh on this comparison, now let's compare, these two streams of AI, and and here is a a table that that very briefly compares the strength and and weaknesses of AI. So let me go through this table. So first of all, the the construction costs. Right? So as we know, symbolic AI and and for this, audience, you are all very familiar with with very large model graphs. These very large knowledge graphs, they are, not they are large. They contain billions of facts and rules, but they are expensive to build and expensive to maintain. So either through crowdsourcing or through corporate, investments. Right, but they are expensive to build at midday. But the connectionist approach also has its construction costs and here the construction cost is called data hunger. So many of you will be familiar with the Nvidia benchmark where Nvidia scraped, millions of, faces of people from the Internet and trained the neural network to generate new faces. Right? Now the neural network is so good that that humans can no longer distinguish between real faces and and generated faces, but it took ten million training samples to train the network for this. And, of course, NVIDIA was very clever in choosing this domain because for human faces, we can actually scrape ten million training examples from the web, but now try to do this for, say, rare cancer tumors. Right? So images of rare tumors. You cannot obtain ten million ten million training sample. The same holds for for AlphaGo. You know, the famous, DeepMind program that, that beat the best, human Go player in the world, it took four point eight million training games, mostly AlphaGo training against itself. And, again, game playing is a domain where you can basically, you have infinite amounts of training material. And, again, this is not true in in most of the the business cases, that you would meet in practice. So I'm always a little bit annoyed when, machine learning people tell their customers, well, we can't really help you because you don't have enough data. Well, it's not the case of the customer not having enough data. The problem is that the machine learning algorithms need too much data. So you see that, they have sort of complementary strengths and weaknesses here. And, as we know, these symbolic methods, they suffer from the combinatorial explosion. Right? So they they get worse with more data. Right? This is called the combinatorial explosion. But the connection is the neural network approaches, they get worse with less data. Right? So, they they suffer from this data hunger. So again, very complimentary strength and weaknesses. Another, you know, by now well known complementary strength and weakness of the two systems are their explainability. These symbolic systems, they have the advantage that their vocabulary is very close to the vocabulary that people, everyday people or experts, use. So here is a rule about a particular medical system, sim a medical symptom, dysphagia, which, then causes, some, a particular conclusion, namely a medical diagnosis. Right? So we have a variable, we have a condition, we have a conclusion, and this is in an understandable format. And, as we all know, these neural networks don't have this. Here's a famous example from the literature where researchers took a neural network that, with reasonable confidence, recognized this picture as a panda, and then they added noise to the picture, noise that is invisible to the human eye, and suddenly, the picture the the neural network recognized the picture as a given. Now you would want to know what went wrong, what happened between this one picture and the other picture when we can't even see the difference. Now what the the neural network engineers did is they carefully added noise from the picture of a given, and they carefully drove the network down into another local minimum in the high dimensional, fitness landscape where the network training does gradient descent. Okay. Now that's clearly not an explanation that you can give to to any reasonable user of such a network of of such a neural network. So this is known as the black box problem. And finally, as we well know, symbolic systems suffer from what's called the performance cliff. Think again of a knowledge graph. If you query the knowledge graph for facts that are in the knowledge graph, you get very high quality, you know, both precision and recall. But if you start querying it for stuff that is outside the knowns graph, basically, you get no answer. Right? So there's a very steep performance cliff. But, also, these connectionist approaches, they suffer from a a performance cliff. Right? Here's a famous example from, an an an image labeling competition a few years ago, and the question was, what is this lady wearing on our head? And the answer was with near hundred percent certainty, she's wearing a shower cap. Now, we all know that this is not a shower cap, because we recognize this lady and we know that she's the queen, and we know what queens do with shower caps, they don't wear them in public, so so this cannot possibly be a shower cap. Why does the system think this is a shower cap? Well, probably in the training set, there were not very many, training examples of ladies wearing tiaras, wearing crowns. And maybe there were examples of of people wearing shower caps. So this is a typical case of an out of sample very poor out of sample generalizability. That's the performance cliff of connection systems. So if we then see that these systems have complementary strengths and weaknesses, can we somehow get them to collaborate? And and just to emphasize, you know, this is this performance cliff is is not an an an artificial example. Here are all reasonable stop signs as you would find them in practice. Some funny guy has put a sticker on it or there is funny shape shadows on it, and these are all recognized as a maximum speed limit sign. So this is clearly a realistic problem. So can we solve such realistic problems with the machine learning systems by getting them to collaborate with the symbolic systems? Right? That's what what the rest of the talk is going to be about. Can we get the symbolic systems, the systems that reason, and the subsymbolic systems, the systems that learn, can we collaborate, can we get them to collaborate in a system that both learn and reasons? And I will give you a number of examples of systems that do such a collaboration. So maybe to this community, the best example, the easiest example is using a learning system to enrich a knowledge graph. This is often called knowledge graph completion. So imagine that we have a knowledge graph about music, and we have many links between artists and songs. We know that, Michael Jackson published Beat It, and we know that the Beatles published Yesterday, and we know etcetera. So we have many links between artists and, and their songs, and we that that has a particular label, namely that that artist has published that song. Now if we take this knowledge graph and we, somehow transform this knowledge graph into a high dimensional vector space where, similarity in the knowledge graph corresponds to similarity in the vector space, then Michael Jackson would become a vector. The song Beat It would become a vector. And the relationship between Michael Jackson and beat it, namely the relationship publish, is also a vector, from the between the two endpoints. And now we might recognize that if we have a vector for the Rolling Stones and a vector for Angie, that the distance between the Rolling Stones and Angie is very similar to the distance between Michael Jackson and Beatit. So this may cause us to learn that, the relationship between Rolling Stones and Angie is the same as between Michael Jackson and Beat It, namely, it's the relation published this all. Right? So in this way, we can learn new facts to add to another graph by using machine learning methods in this high dimensional vector space. So, essentially, now what we are doing here, we are are taking a graph representation. We are turning it into a real vector representation, a high dimensional, real real number vector representation. We do machine learning in the vector space, which leads to new predictions of links, and we translate these new links back to the, to the symbolic space. Right? So sometimes we call this this this pattern, we call it from symbols to data and back again. So we started with symbols, the the knowledge graph. We produced some data space. We did machine learning on the data space, and we translated it back to the symbolic space. So this is one combination where we use learning systems to improve the symbolic systems. So there, we were using, learning systems to rule only facts. Right? So we could learn new facts, but there are also systems where we learn new rules. And these, learning systems are called inductive logic programming systems or rule mining systems. So, here's an example from inductive logic programming. Suppose that we have a knowledge base, a knowledge base in symbolic form, this could be a knowledge graph about parental relationships. And we know that, and we can learn, from this parent and this parent, father, and mother relationships. From this, we can learn the rule that when that somebody is a parent, if somebody is either the father or the mother of that person. Right? So parent x y if either mother x y or father x y. So we can learn these new rules. Now we can confront the system with a new fact, namely father, Carrie, Andy. And together with, this learned knowledge, we can now derive that not only is Carrie the father of Andy, but, Carrie must also be the parent of Andy. And this can also be done on on knowledge graphs. AnyBurl, from, Manheim, is a system that, learns, rules over knowledge graphs rather than just simple facts. And this doesn't only work on, toy examples like the one that I'm showing you here, but this has also been shown to work on realistic examples where we take a, knowledge graph about chemical facts, biochemical facts, and the scientists this is an example from University of Leuven, where the scientists, the biochemists wanted to know which parts of a chemical structure makes a, a molecule biologically active or not. And so some molecules are biologically active, like, and some are not. So, for example, Petrol is biologically active, but plastic is biologically very inactive. But they are both made of very similar, molecules. And by feeding the system examples of molecules and labeling them as biologically active as as in and inactive, according to what chemists know, the system could learn the rule that this substructure that you see here on the right, this substructure is responsible for a a a molecule being biologically active or not. So in all of the examples we've seen so far are examples of where a learning system improves a symbolic system. But we can also turn this the other way around. So we can also use a knowledge system to improve a learning system. Here's an example. Let let's take the previous example. Now where now why is this a crown? Right? Why is this a and and not a a shower cap? K? Now if we have a neural network, that, neural network will generate a whole set of hypotheses, ranked with confidence. And because of bad, out of training, set, generalizability, the shower cap ended up with the highest probability. But if we now feed all these, hypotheses, these possible predictions into a knowledge graph, and the knowledge graph can tell us that there's a very close relationship between queen and crown with a very long distance relationship between queen and shower cap, then the knowledge graph will help us to select the right hypothesis and tell us that, well, if we have to choose between, a crown and a shower cap, then because this is the queen, it's more likely that this is a crown and not a shower cap. So here, we are using a knowledge based system to improve the performance of a machine learning system. And we can use almost the same, setup, not only to improve the output, but also to improve the explanations. So suppose that by, you know, whatever improvement we have managed to get the right output, why is this a crown? Hard for an image labeling neural network to explain why this is a crown. Right? This is just a a large box, a black box of neurons, adding up to the conclusion that this is a crown. So maybe we cannot give a real explanation about the content of the neural network, why this is a crown, but we can come up with a reasonable justification by using an honest graph. Right. Well, because this is a queen, what she's wearing on her head is most likely a crown. Right. So we can use a knowledge graph to come up with a justification for why, machine learning neural network, produced a certain element. Let me give you another example. Well, look at this picture. What do you see here? Now if you are an image labeling neural network, how would you label this image? Well, you would say, this is a flower. Maybe you would say it's a rose. K? Well, actually, you would have been wrong, because if you would know the context of this picture, you would see that it's not a flower or a rose. It's actually a cushion. Right? It's a cushion on a chair. So it's the context that matters here. Right? So what we can do is we can take this picture. Now let's assume that we have already classified the object in the blue bounding box, namely that it's a chair. Then we can inject a little bit of ontological knowledge. The ontological knowledge will tell us, well, a chair is made up out of cushions and an armrest. Right? So, this is a simple bit of of ontological knowledge. If x is a chair and y is part of x, then y is either a cushion or an armrest. Right? Now given this bit of ontological knowledge plus the fact that the blue bounding box contains a chair, that suddenly increases the the prior probability of the cushion as a as a an output, and reduces the prior probability of flower as an output. Output. So the the prior probability of cushion given a chair is much higher than the prior probability of a flower given a chair. I mean, a flower is not impossible. It could still be a flower lying on the chair, but it's much less likely. So here, we are using a bit of ontological knowledge to give symbolic priors to a subsymbolic, object recognition, system. And and, actually, this is not a a a a rare example. There is a a very good survey paper, by, colleagues in two thousand nineteen who, surveyed more than a hundred, different machine learning systems that use symbolic priors to improve, the behavior of a machine learning system. Now this is also known as a symbolic loss function. So so what is a loss function? A loss function is the function that you try to minimize during machine learning training. Right? It's the difference between your output and what your neural network tells you. And that difference between the the between the desired output and what your neural system tells you, you're trying to minimize the difference between the desired output and the actual output. Now it's part of the black art of machine learning to formulate a correct loss function. And there are many of these systems in this survey paper that, use semantic loss functions. So you write down what you know to be true about the world, in this case, or the parts of a chair, and you are training your system so that it minimizes the violation of this background knowledge. It minimizes the conflicts between what your system says as output and what you know is likely in the world. So these semantic loss functions are a great help to machine learning systems. Here's another nice example of, of of the combination of symbolic and, and machine learning systems. So this is an an exercise we give to our first year machine learning students. Right? Train a neural network so that it can read digits and then produce the sum of the two handwritten digits. And you can do this end to end training. But somehow this is not how we end up, how we as humans add up in, digits. We don't add up the visual images. We abstract the visual images into a symbolic representation, namely a number, and then we add the numbers. And, again, an example from our colleagues in Leuven, in Leuven, this is what they do. Instead of adding up the two, handwritten digits, they first abstract the handwritten digits into a symbolic representation, namely, in this case, the three and the five, and then they add the symbols, rather than training the system to add up the how to add up to handwritten digits. So, yeah, this is really cool, and it colleagues in London from, UCL and, and Google DeepMind to train a reinforcement learning agent in a very, artificial world. So in the the the agent was moving around in a world of objects, but rather than making it move in a world of pixels that are perceived by a neural network, these pixels are abstracted into objects and then the agent moves around in this world of objects, rather than in this world of pixels. And what happened in their experiments? Well, this was a very simple world where, you know, the the the big symbol in the middle there, the plus, is the agent, and it had to move around in this rectangle, this grid like world. It had to collect the crosses and avoid the zeros. Right? And, a reinforcement learning agent, the blue line, could could perfectly learn this task in this rectangular world. And they also trained an an agent that moved around in a world of not in pixels, but abstracting these crosses and nodes into objects and then move around in the world of objects. And that agent didn't quite do so well. But then they did something interesting. Then they they changed the world. They said, well, now these objects no longer live on the grid world, but these objects can live anywhere in the world. And then the performance of the agent that was trained to move around in a world of pixels completely degraded. It it fell from a hundred percent to fifty percent or less. Right? But the agent that was moving around in a world of objects, it was stable in its performance. Now it could, adjust to the fact that these objects no longer lived in a grid world, but that these objects could be anywhere in the space, and it was much more robust to learning, to adapting to this new world. Right? So, again, a combination of machine learning and a symbolic representation about the world. Just as we don't move around in a world of pixels, we we move around in a world of abstract objects that we construct out of the pixels. The final example is about, rule learning, where in the traditional world, we would, this is in a medical example where we would feed patient features about age and and, and body mass index and blood pressure and so on, and then try to predict, diabetes. You could do this with, with a a learning system, and then the system would come up with rules that are are very good, almost hundred percent correct, but that will be totally non understandable by medical experts. And as a result, even be though these rules were correct, the medical experts don't really like these systems because they can't understand what they do. So instead, what we did in an experiment jointly with, University of Munich, we injected, not only data driven rules, but we injected rules that we obtained from experts and from textbooks. And these rules weren't very difficult to, obtain. We talked to experts. We read some textbooks. We read some medical guidelines. And the corresponding rule set that you get out of combining the human expert knowledge with the data driven rules is almost as good, interestingly enough, not quite as good, but almost as good as the purely data driven rules, but the rules had became much more human understandable. So instead of a perfect system that nobody uses, you now have a slightly less perfect system, but that experts are willing to use. Let me for the sake of time, I will skip the the mathematics of this. Right. So in the last minute, my, concluding remarks. So what I have tried to convince you of is that machine learning systems can really benefit from injecting symbolic knowledge in in in many cases, symbolic knowledge in the form of knowledge graphs. Right? We can produce better explanations. We can better rank hypotheses. We can do transfer learning. We can get a better sample efficiency, so learn from fewer data. And we can also do this the other way around. So we can have our knowledge based systems benefit from machine learning systems. And the required symbolic knowledge is available at very large scale. Right? It is no longer true that symbolic knowledge is expensive and we cannot obtain it. All the very large linked data knowledge graphs are a witness to the fact that this symbolic knowledge is very well available. So it is no longer, necessary to learn what you already know. We can inject the stuff that we already know into our machine learning systems, and by combining these two types of systems, produce much more robust, much more efficient, and much more explainable systems. Thank you very much.",
  "language": [
    {
      "id": "http://vocab.getty.edu/aat/300388277",
      "type": "Language",
      "_label": "English"
    }
  ],
  "created_by": {
    "id": "https://example.org/cdkg/Creation/presentation/23",
    "type": "Creation",
    "caused_by": [
      {
        "id": "https://example.org/cdkg/Activity/presentation/23",
        "type": "Activity",
        "_label": "Presentation of \"Systems that learn and reason\""
      }
    ]
  },
  "digitally_carried_by": [
    {
      "id": "https://example.org/cdkg/DigitalObject/recording/23",
      "type": "DigitalObject",
      "_label": "Recording of \"Systems that learn and reason\""
    }
  ]
}