{
  "@context": "https://linked.art/ns/v1/linked-art.json",
  "id": "https://example.org/cdkg/LinguisticObject/presentation/2",
  "type": "LinguisticObject",
  "_label": "Textual content of \"Data Observability: How to Eliminate Data Downtime and Start Trusting Your Data\"",
  "classified_as": [
    {
      "id": "http://vocab.getty.edu/aat/300027388",
      "type": "Type",
      "_label": "Transcript"
    }
  ],
  "content": "Hi, everyone. Great to be here. My name is Bar Moses. I'm the CEO and cofounder of Monte Carlo, the data reliability platform. And today, I am stoked to have a chat with you all about how to eliminate data downtime and start trusting your data. Hopefully, you all can see my screen, and we're gonna get started here. So just a little bit of background about myself. As I mentioned, CEO of Monte Carlo. Prior to Monte Carlo, I worked at a company called Gainsight, worked mostly with companies to help them become data driven, to actually make use of their data, for various, purposes. I, am also in the process of writing the very first book on data quality in collaboration with O'Reilly. Super excited about that. We just prereleased, so the very first chapter. So check that out if you'd like. And then another fun fact about me is that I'm a very big, fan of Bruce Willis movies. So you'll you'll find me, watching those in my spare time, when I'm not thinking about data downtime. So that's just introduction about myself. Again, stoked to be here and to speak with you all about data downtime and data observability. So let's get started. What is the problem that we're all here today to discuss? The problem is what we call the good pipeline's bad data problem. And what does this actually mean? Everyone on this call probably has some experience working with, data lakes, data warehouses, ETL, reverse ETL, BI models, ML models. Whatever you'd like, we have all the acronyms. In short, everyone, typically most companies, have invested a lot in setting up world class, best in class data infrastructure and data systems. And so we're all sort of ingesting, processing, transforming, modeling, making great use of the data. But here's the problem. We've invested so much in having amazing pipelines and amazing data infrastructure. However, so often the data that's actually powering those systems is often wrong. Not sure if anyone here, today, you know, the example here is familiar. You know, you kinda wake up Monday morning, you come into the office or maybe you log into your office, and you suddenly get sort of slammed with this, you know, this, like, meteor, shower of pings from Slack asking, what's wrong with my report? Or, hey. It looks like the data here doesn't quite look right. What am I missing? Or, you know, get pinged. What what type of, what data what, table should I be using? What what is right for this question that I'm trying to answer? And it's always at the very last minute. It's always, like, four minutes before a meeting. Or, you you know, if you get lucky, it might be on a Friday night, as well. In any case, this sort of situation is really widespread across the industry. There isn't anyone that isn't hit by this. For me in particular, I experienced this when I was managing a data team, at Gainsight, and we were, you know, sort of hit by lots of questions. Know, I felt like, oh, man. We had, like, one job to do right. It was get the data right. And we, you know, we weren't able to do that. It was really, really hard to do. And I think there's many reasons for that. Right? There's, you know, way way more data that we're managing today. There's way more stakeholders, and and data is, a lot more distributed, in different systems. And so let's talk a little bit about this problem and how it sort of emerged emerges over time. We sort of call this the data data downtime. So data downtime refers to periods of time when your data is wrong, inaccurate, otherwise erroneous. And the interesting thing about data downtime, again, something that everyone here is likely experienced, is that the impact of data downtime actually is worse as you kind of progress across different stakeholders in the organization . So if you think about a couple of years ago, maybe three, five, ten years ago, there weren't that many people who were actually working with data in the organization. And today, there's data engineers, data analytics, data science. There might be ML engineers, software engineers, definitely you know, teams that are working in in business functions. There's way, way, way more people that are actually making use of the data. And so if you think about sort of the impact of data being wrong, at the starting point, maybe when the data engineering team actually catches that issue, that is what you'd call sort of under control. However, there might be a couple weeks later that an issue goes unnoticed, and then that is discovered by the data analytics or the data science team, which might be frustrating that we as data engineers didn't catch that issue first. Maybe, you know, worse over time, that issue lingers or actually starts to have an impact across the business. Those can actually have material impact on companies, on organizations, on people. Right? And so thinking about companies in media or in retail or ecommerce companies who really sort of, are at material risk in terms of compliance if the data is wrong, if you, you know, mistakenly get credit card information wrong. You might be overcharging customers or undercharging customers. You might be powering the wrong marketing campaign with incorrect data. The list is long. Right? But the bottom line is that data data downtime impacts all of us. Its importance is rising in organizations because the impact of data downtime, is is severe over time. And so, you know, I come to you today with a lot of bad news. I realize this is a little bit of a depressing start, but I have some good news for you all. And the good news is, actually, in order to think about how do we build reliable data products, it is helpful to look at, a corollary or look at our friend software engineers who who have been building reliable products, for decades now. Right? So how do engineers actually approach building reliable products? You know, what you can see here is sort of, various categories and various tools that have built around each of the categories that ultimately help data help engineering teams build products that are reliable, secure, and scalable. Now in data so far, it's kind of been the Wild West. Right? Like, you get up, spin snow spin up, Snowflake, spin up Looker, and you get started. Everyone has access to everything. There's no test. There's no monitors. You put things in the wild, and you hope that they're okay. Right? In a lot of those instances, data downtime rises and starts to become more prominent in organizations. So what can we actually learn from our friends in software engineering? At a very high level, there's sort of three core categories here that, that emerge from this that we would like to actually implement for data and to take as best practices as well. And these three categories at a very high level, the categories of discovering problems as they arise, resolving problems as fast as possible, and then preventing them to begin with. So, again, these are all concepts that are taken from software engineering, but what do they look like when we apply them to data? What does it mean to actually discover, when an issue is detected when an issue, sort of arises in data? You know, I'll say most often, many data teams that we find that we work with, we find that they are the last to learn about issues arising in data. So, you know, they might sometimes hear about it from their internal customers, say, the marketing team or the product team, or support or customer success team. Oftentimes, they also learn about this from real customers, external customers, if you will, not real external customers. And in those cases, they're caught blinded, if you will, learning about an issue from others. So how do we actually turn this around and develop the tools so that data teams are the first to know and the first to discover about data issues. A lot of that goes into actually setting up monitoring, alerts, strong anomaly detection, and different things that that we'll touch on. Once you've actually identified that there is a problem, how quickly does it take you to resolve it? Many data teams that we speak with, sometimes it takes them weeks or even months to resolve an issue. What if you had an approach to actually resolving data downtime issues, in a way that will allow you to do it in minutes or in hours instead of weeks or months? And what does that look like? What are some of the root cause analysis that you need to do in order to identify, what exactly is the issue? And then finally, prevention. Right? If we're doing this right, we can actually reduce the number of incidents, the number of data downtime incidents to begin with. If we're actually getting smarter over time, if we're learning, what are the, you what are the main sources or the main pipelines that tend to break? What are the main issues that are causing data downtime that are sort of draining a lot of our time? If we continue to collect insight and information about this, we should ultimately be able to prevent these issues to begin with. So I very much believe in actually adopting the sort of three three step framework, that we that, you know, is tried, tested, and proven in software engineering and now applying it to data engineering. So today, we don't have time to double click into each in each of these, but I will double click into discover and talk about some of the methods that we've used, that we've identified as sort of best in class and that some of the best organizations, some of the strongest data teams are actually adopting in order to improve the ability to discover. Now, you know, one of the questions is, how do we actually measure ourselves on these three? How do we know that we've gotten better? And so we're seeing more and more data teams develop bet better methods of actually measuring themselves on these. To be honest, most data teams don't measure themselves today, but, actually, just starting to measure is a great step forward. So on Discover, folks actually look at, time to resolution. Sorry. It's time to detection. So how quickly I'm thinking ahead here. So how quickly, is there actually what is the time lapse between when the incident actually happened when we and when we detected it? That's sort of the first metric. The second metric is around resolution. And so once we have detected an issue, how quickly until we resolved it? And obviously, you know, you sort of expect that, you know, incidents will happen, but that over time, we are improving in both of those metrics. And then the third metric is prevention. Are we able to reduce the number of incidents overall? So actually starting to measure how many p one, p two, p three incidents do we have? What is, what is the the time to detect and time to resolution per issue, and are we able to improve over time? So that's just to give you a little bit of sense of how you can measure yourself as an organization to improve this. Now let's double click into the discover part here, the first portion, and talk a little bit about what are the methods that teams are using to, really operationalize that as part of improving, their data operations and minimizing data downtime. So continuing to borrow a little bit some concepts from software engineering, there's a concept called observability in software engineering, which is yet again a very well understood concept, which, you know, if you check out this quote, also has a lot to do with, with monitoring as well. And there's a lot that's been written about the difference between observability and monitoring. But in software engineering, it's sort of like a no brainer to have something like this. Right? And so the most common things, you know, talked about, like metrics and traces and logs. And there's sort of a lot of kind of solutions that enable software engineers to do their job in making sure that applications are, reliable. So solutions like PagerDuty and Splunk and New Relic and Datadog are all solutions that we are very accustomed to work with. And in software engineering, they make a lot of sense. And so the question is, in the data space, why are we flying blind? Why don't we have something like this that will allow us data engineers to actually do our job properly? And so imagine that you're actually taking the concept of observability and now applying that to data. What does that look like? So let's actually define the term data observability here so everyone is on the same page. Data observability is an organization's ability to understand the state and health of the data in their system with the objective of mitigating data downtime. Again, the the measure of success here is if we are able to prevent data downtime to begin with. We probably won't reach a hundred percent. We will not be perfect. But if we can actually minimize it and reduce this, we will all be able to spend time elsewhere on other revenue generating projects, or other things that, you know, we are curious and excited about. And so data observability, barring again from the concept of observability from software engineering, actually allows us to understand, to discover data downtime issues, ultimately increasing trust in the data. So continuing down this path of data observability, what does data observability actually mean? So in software engineering, it's super well understood and used. In in data observability, it's a new term. Right? And so what we did at Monte Carlo is actually we spoke to hundreds of data organizations ranging from small startups to large organizations like Netflix and Facebook and Uber and, actually, sort of created a sort of or collated really big dataset of what are all the reasons for why data downtime happens to to organizations, what are all the symptoms that, folks see when data downtime happens, and then all the reasons that folks or all the ways in which folks actually resolve those issues. And so we actually codified all of that good stuff into what we call the five pillars of data observability. And in our experience, we see that when you actually sort of put these to work, when you, instrument, monitor, analyze these, data teams are actually able, to to start this sort of journey towards operationalizing trust in data. And and having this combination of these five actually provides a very strong view into the health of your data. And for the first time, data organizations are actually able to see, to understand the health of their data and understand where are the sort of vulnerability spots and what are the areas where some corrections need to be implemented. So let's double click into each of these briefly to give you a little bit of an example of what these data observability pillars mean. So starting with freshness, which is the first one, which, freshness means a lot of different things. This is a particular example of it. But really sort of it comes down to the timeliness of the data. Right? So in this example, what you see here is a specific table that, you know, the the lines here indicate the table getting updated, over sort of this, axis of time here from from November tenth onwards. And you can see that the table is getting periodically updated once or twice a day, a couple of times a day. And then suddenly, there's a period of three or four days with no updates at all. In this instance, this might indicate that there is a potential issue with the data. And so actually, by using machine learning to observe the data over time, we're able to identify these in an automatic way. And now, oftentimes, data teams find themselves manually specifying a lot of these thresholds. I'm happy to say with some advances around automation machine learning, it is actually possible to do this, not only for a particular table, but rather for hundreds of thousands of tables at scale. And so you can automatically sort of turn on a slew of metrics, around sort of the freshness of data, this being one example of them. The second important pillar is the concept of volume. So pretty straightforward, as you can see here, an example where you see a number of rows. And in this particular incident, there's an unusual number of rows that were removed. You can see there's sort of a nice trend. So slowly increasing the number of rows and row count. And then suddenly, on September thirtieth, there's a very high number of rows that have been removed. So this is an example of one way to track volume. And, again, another sort of reason for why or another indication for data might be, that might be wrong. The third sort of pillar of data observability is what we call schema, and schema is often the culprit of data, going wrong or going down. Oftentimes, schema or actually or schema changes, they happen quite a lot, but they're not communicated about. So one team might make a change, but then the other team is unaware of that. And oftentimes, in that breaking point in communication is where sort of lies the data downtime problem. And so what we see data teams do is actually have an automated way of tracking schema changes. And so in this example, you see there's a bunch of fields that have been completely deleted and removed. There's probably someone downstream that has a report or other tables or another model that actually relies on these fields and needs to be notified of this. You might actually have also a specific field change in type, which might impact downstream. And so having an automated way to track these the changes in schema and a way to communicate about them is a very, very important part of this fourth pillar. And then the fifth pillar is is, or sorry. The fourth pillar is distribution, and distribution really has to do with metrics and health at the field level. So as an example here, you can see there's a field called count pets one five years old, and we're looking at percentage null. And there's a specific average, a seventeen day average, so a little less than three weeks average. And then today, the percentage of knowledge just spiked. It's way higher than the average in the last, two to three weeks. And so in this instance, you know, this might be a field where, it's it's it is quite bizarre, to have so many, so many null values. And so this might actually, be an incident, that you'd wanna look into. And so this is an example of a type of distribution issue, if you will. And, again, there's a whole you know, under distribution, you could look at null rates, negative rates. You could look at, average, min, max, a whole slew of different metrics under distribution, everything at the field level. And then the fifth linea the fifth, pillar here is the lineage pillar. It might be my favorite. And this pillar actually allows us to bring the whole story together. So what you can see here is, a table level lineage, where you have, tables from a data warehouse and then views downstream in Looker. And there's a particular incident that's been identified in a particular table called recent metrics. And you can see all the tables downstream and the views and explores, and dashboards downstream that have been impacted by this. Now the importance of lineage, both at the table level and field level, is that lineage by itself is quite useless, to be honest. So if you just have lineage, there's not that much that you can actually do with it, or it's mostly sort of eye candy, for for most teams, where we do find lineage being helpful when it's applied to a specific use case. And in this particular use case, it's around data observability. And so I think when you take lineage and you combine that with the sort of other pillars that we talked about today, that's really where the magic lies. So, you know, let's say, for example, there's a particular table that has a freshness problem. So there's data that's loading once an hour at any given day and then data stopped loading that table. It hasn't been loaded for three hours. And then downstream from that, there's another table that might actually, as a result of that, have a problem has a distribution problem. And as a result of that, there might be a dashboard downstream that might have incorrect data now. And so lineage helps us piece all of that together and help us make sense of, Okay, there's a problem somewhere, but should I care about that problem? Maybe there are no dependencies. Nobody's using this table, so nobody cares about it. So maybe, you know, we don't care about that specific problem. However, maybe there is a specific incident with hundreds of tables downstream, key reports that your board and execs and customers are using, in that instance, you probably want to give that incident a higher degree of attention, a higher degree of severity. And so lineage is incredibly powerful in helping us sort of bring this together and help us understand both on the one hand impact assessment. Should I care about this? Who cares about this? And then on the other hand, it helps us understand root cause analysis. Right? So if there's a particular incident somewhere, what are the upstream sources of that we can learn from around what might have gone wrong and where? So being able to answer all of these questions is really fundamental to our ability to restore trust in data, or to have more trust in data, if you will. And so finding or sort of developing ways to automate the instrumentation and the monitoring of these and bringing them together in one view is really what helps us in building towards this new paradigm, if you will, this new data reliability stack, which includes sort of the three steps that we talked about. And so when you think about that first step, the discover step, going back to kind of the initial prompt here, most of our reality today is a reality where we created some tests to test the things that we know can go wrong with our data, and that's a very important part of our job. And then eighty percent of our life is really kind of getting angry Slacks or emails from colleagues that are really, really unhappy with some of the data going wrong or reports getting messy or, model drift. There's all these things that people can get really angry at, and then the finger pointing, game starts. You you know, who actually owns this table and whose fault it is. Right? That's the reality that we all live in. However, I think there could be a better way, and the better way has to do with observability. With observability, you know, you still have tests that we've created ahead of time. However, there's broad automated coverage that we can achieve, with observability for eighty percent of the incidents. So we start out with a good base of issues that are detected in a way that we don't need to specify manually, but rather we can have this broad coverage for. Of course, we will always need to have tests. There's always going to be things that, you know, us as sort of folks closest to the data would know best. And it's very hard for any machine to actually learn or for anyone to do for us. And those will always continue. But with observability, you can rely on sort of broad base of coverage to help make sure that you're sort of starting from a, from a good place. And then I wish I could say we could eliminate the angry text completely, the angry Slack messages, but, you know, I do think that there's probably a one percent that will always remain with us. We we can try to minimize as much as as possible and maybe even laugh about it. But that's sort of, with observability, the benefits that data teams see. And so going back to kind of the three core components, we talked a lot about Discover and sort of what that looks like. We talked a little bit about Resolve. The five pillars of data observability fit nicely into both of these and really help us have a starting point for the data that we need to do our job well and to actually sort of generate results here to reduce time to detection, time to resolution. And then ultimately, we'll talk more about this potentially in a future talk, But, there's a lot of work more for us to do on sort of prevention, as an industry overall. I think we've made tremendous progress in some of these areas, but I'm really excited for developments in other areas as well. And so with that in mind, data observability is just getting started. It's very much a new term. It's taken the world by storm. We're seeing more and more data organizations, implement this as part of their, part of their operational work. You know, team syncs to actually review some dashboards of how we've improved in time to detection, time to resolution. What are the main, areas where we where we're seeing a higher incident of data downtime issues that we want to work through? What are the areas where we're seeing an improvement? What are the different teams that are working with us that we can actually have contracts and SLAs with in order to develop a stronger organization overall, sort of going beyond the data engineering team, and including all the stakeholders that we work with. So there's a lot more work for us to to be done here. I hope this gave you a little bit of a taste of what data observability actually is. I hope you left with a better sense of some examples, you know, maybe something that you can implement, and get started on your journey. If you'd like to reach out, feel free. This is a topic that I'm most passionate about. And if you have an example or a story, or a bad data horror story to share, would love to hear from you. Thanks so much, for having me, and I'm looking forward to the q and a portion.",
  "language": [
    {
      "id": "http://vocab.getty.edu/aat/300388277",
      "type": "Language",
      "_label": "English"
    }
  ],
  "created_by": {
    "id": "https://example.org/cdkg/Creation/presentation/2",
    "type": "Creation",
    "caused_by": [
      {
        "id": "https://example.org/cdkg/Activity/presentation/2",
        "type": "Activity",
        "_label": "Presentation of \"Data Observability: How to Eliminate Data Downtime and Start Trusting Your Data\""
      }
    ]
  },
  "digitally_carried_by": [
    {
      "id": "https://example.org/cdkg/DigitalObject/recording/2",
      "type": "DigitalObject",
      "_label": "Recording of \"Data Observability: How to Eliminate Data Downtime and Start Trusting Your Data\""
    }
  ]
}