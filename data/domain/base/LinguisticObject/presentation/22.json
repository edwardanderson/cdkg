{
  "@context": "https://linked.art/ns/v1/linked-art.json",
  "id": "https://example.org/cdkg/LinguisticObject/presentation/22",
  "type": "LinguisticObject",
  "_label": "Textual content of \"RDF Leveled the Advantages of LPG and Keeps 3 Key Benefits: Standards, Semantics & Interoperability\"",
  "classified_as": [
    {
      "id": "http://vocab.getty.edu/aat/300027388",
      "type": "Type",
      "_label": "Transcript"
    }
  ],
  "content": "Hello. My name is Atanas Kyriakov, and I'll provide you an overview on how RDF overtook the advantages of property graphs in the recent years and what are the key benefits that it still has and bears for knowledge graph applications. To continue with a quick introduction, I'm the founder of OntoText. We, Samantha back in year two thousand as semantic web and semantic technology pioneer. Now we are best known as the developer of GraphDB, one of the most popular database engines. We are an enterprise knowledge graph top leader, and we enjoy being the center of an ecosystem of more than twenty partners all the way from portfolio partners which complement our technology to, consulting and and delivery partners. We are profitable in growing. We have, among our customers, yeah, the leaders in many fields, like in financial services, in in, yeah, media and publishing, in government, aerospace, health care life sciences, or infrastructure management like Johnson and Schneider Schneider Electric. We are also involved in all sorts of standardization bodies that have something to do with it because we we care about the future of this of this technology and, this this trend. To give you, yeah, a graphy, introduction to what we are doing, We are here to connect the dots of your enterprise knowledge. We do this by, yeah, basically combining and fusing and and, making your proprietary information smarter. Our special way to do this, special sauce, a special ingredient is that we use deep domain knowledge to enrich your proprietary, information. And, the way in which we do it is that we we put together, we create, we craft, we help you craft, rich knowledge gaps that enable unified data views and this way, more more more deeper and more more more fruitful analytics. The way in which we create these knowledge graphs is by, yeah, linking and, linking data across different sources, reconcile reconciling data, converting strings to things, the usual things to to to end up with the with the body of body of knowledge that is easy to query and to explore and and and to deal with. Well, this requires a fair amount of text analysis, data analysis, machine learnings, all the fancy things. And that's what more or less everyone does. Again, our special associates that we we we've been maturing for twenty years how we can use, almost exhaustive domain knowledge, big bonus of domain knowledge, to to to provide context, and to to to help us better interpret, better recognize, better better classify things. And that's that's from the bigger picture, the way in which domain knowledge, help. Get connecting to those of your enterprise knowledge and that's first very important ingredient of talking about knowledge graphs at all. We store this knowledge graphs. We manage this knowledge graphs with our semantic database and search engine, Graph DB. And what's still missing on this picture is why we do this. Well, we want to give you better insights, and reach results for for for results as much as possible in less time. That's what Graph DB is designed to do altogether. On the bigger picture, a knowledge draft management platform should cover a bigger set of capabilities from yeah, building, build build building these big data data artifacts, storing index in operations, accessibility, federation exploration analytics. For each of those there, yeah, more specific capabilities that should be there. We develop a lot of these, the orange boxes ourselves and, we we we we made decision to to use our partner ecosystem tools from our partners for other capabilities. Right? Like, some editors, some toll generators, data catalogs, chatbots. So that's our way of working in this field because we can't, cover it all, at at at matching our our our criteria for metro software. We are, the leader of the RDF space. You you know, the like, the graph database field and the graph technology field has, like, property graphs chapter and the RDF chapter. We're in the RDF chapter. And from this position I want to to tell you what are, what were the historic advantages of property gas and and and how, RDF address this. So, the easiest argument for someone to make, on on on the property graph side was, well, in in in in and everything on top of it, you can attach, properties to to edges and say things about the edges in the graph. While in RDF, you can only do so for for the notes. And then the second big big argument was, well, in thinking of everything is designed and thought of, yeah, providing efficient graph traversal. And historically, this wasn't the case in in RDF. But over the last three years, yeah, RDF, yeah, addressed this in a very, very comprehensive manner. The first thing was, RDF star. That's specific specification for attached metadata to the edge edges of the graph that started two thousand fourteen, and now there are plenty of implementations across different vendors. The second topic is, yeah, the SPARCO extensions which allow for graph traversal path search and generally speaking exploration of multiple relationships in graphs. So I'll continue this presentation with a bit more flash and technical detail on the how RDF star is implemented and how RDF engines implement the graph reversal. And then, I'll give you my view on, yeah, what's the added value of knowledge graphs and finally how RDF enables this this added value. To start with, yeah, here are the example. If you want to encode the information that Abraham Lincoln was, president of the United States, that's a very easy way to to do it. You have two nodes and, one relationship, one edge labeled with president of you can have a slightly more advanced version having labels or names for the two nodes. But the reality check says, well, actually, you you you have to be able to do much more than this. So, you need to to provide contextual temporal information, as well as provenance metadata so that you have all the all the important information about this fact, this edge. Historically, in RDF, there was no easy way to attach this information to an edge in the graph. But yeah. Now now it now you can see how it works in RTS star. So, you can have the statement, Abraham Lincoln, position held president of the United States. And through this entire statement, you can make another statement saying, well, this thing, this fact, this statement has start starting date whatever whatever, literal that is the starting date. So you can make statements about statements. If you want to look at this, at the at the diagram, think of having an edge and then being able to make another edge that starts from the first one, and making statement about it. That's much more expressive than the key value pairs that you can do in property graphs. That's the most consensual part of, what what is coming for some optimization in RDF one point two. Before before RDF star, there were other ways to do the same thing like standard reconciliation in RDF or any relationships or singleton property, singleton name graphs, but those were not very handy. So RDF star made, attach metadata to the notes really simple. And we also made sure that the graph implementation makes it makes it efficient so we have efficiency gain. We we took a a Wikidata fragment that if you encode it with the standard verification it takes almost four hundred million statements. It takes a bit less than an hour to load it and so on. And then if you encode the same data in rdf star, it is almost twice smaller, less statements and and it takes much less time to load and it takes much less time on your hard drive. So we also have, this, RDF star support now for for several for kind of couple of years. And, we have some of our partners already using it. To give you the use case, Synaptica is the developer Graphite. That's one of the leading vocabulary management, taxonomy management tools. They use Graph DB to store, all these taxonomies and vocabulary seen as cost data, but historically they had problem of how how they manage their access control lists. And without RDFStar expressing which users can control if you have access to, what access to which properties in which schema for what project was, like, what you see at the top. It was encoded with five different statements. And now with RDF star, it is much simple. There are no auxiliary notes, and you can you you need only three statements to encode this information. So it's it's obviously much simpler. We also took care to to, we made our homework to see, how the other engines handle different corner cases like, statements, valid statements, valid statements, nested nested nested embedded triples, or how how they deal with the situation. Imagine you delete the triple from from your repository, and you want to maintain information about who and when deleted this one. So the statement is not there, but you should be able to keep metadata about it and this kind of thing. So we we took care to others then well. So by now you should be convinced that, yeah, edges, edges and properties are covered very, very well in the RDS space. So let's move on to the graph traversal and how it is implemented. It it is a computationally very complex problem. It comes in different flavors like, checking whether there is a path between two nodes or finding the shortest path or finding all paths or the neighboring nodes and stuff like this. And what they have in common is that they're computation like, heavy problems. Doesn't matter whether you do a deep first search or breadth first search, it it is complex. And complexity goes exponentially with the length of the of of the path that you're searching for. So still there there are a number of good reasons to to find a way to implement it because it's very useful for, yeah, navigation or for knowledge graph analysis, supply chain management, and so on and so forth. There are plenty of use cases. So, historically, yeah, SPARCO was able to almost do this, but in Sparkle one one, you can do property pass to discover whether there is a path, but you cannot get the intermediate notes. And it's very hard also to to to implement in in in spark in plain spark one one shortest path. There are workarounds, but none of them is something you are going to like, they're also slow. So to address this, all the all the vendors of the databases did, yeah, some extensions of sparkle to address this. That's how our extension looks like. So you you you you have a service clause in in in in sparkle that where you you tell I want to do all paths search, and you provide source and destination if you want to or just the source. And then you can specify the maximum path optionally, and so on and so forth. You can also specify that the the pattern that that you want to to to trace so that that you can specify specific conditions, patterns in the graph that need need to be to to be there for each step of this path. You can also do shortest path and, you can do bidirectional search also. So we covered all all flavors of this task. Again, we did our excess our homework to see how others do the same thing and explore cases. And, yeah, you see that our competitors have different advantages and disadvantages in this field. But what's most important for us is that we try to keep keep compliance so not to not to implement it by some hacks in the sparkle process. So, this table wasn't oh, yeah. In a way, we are checking the boxes, seeing whether whether we can do this and that. What really matters for us is also whether we do it well, whether we do it in efficient manner. And that's where, yeah, benchmarks help to to see what what is the actual complexity with specific hardware, with specific volumes of data, what are the choke points, what are the inefficiencies, and address those. We use the link data, benchmarking councils, social network benchmark for this purpose. LDPC is a TPC like industry body of the graph the graph database vendors, on space, Neo four j, CVI. We're among the the the the founders. And then now you have all the all the, yeah, major graph database vendors in there. A few words about the specific benchmark. SMB is the most comprehensive graph on it is benchmark. It simulates the kind of questions that you would ask for the data behind the social network platform. And there were, many many person years invested in making good data generators. So this graph is both realistic and and challenging and covers all the distributions and connectivity patterns in an in an interesting fashion so that you can really challenge the engines and and compare them. To give you feeling of the data model, so you have relations between between people like, knows relationship. You have attributes for people like addresses, these cities and so on. You have relationships also between people and companies or university and stuff like that. There are different sets of queries, and, yeah, the interactive queries, you have, like, one of them saying, well, let let's get all all the people that you can get within three steps following the nose relationship from a given person or find the shortest path between two persons. So this this is what these queries are like. And Graph DB is the first RDF engine to pass the to pass this benchmark. We do this in a stepwise manner so we we are essentially push pushing the boundaries of what we can get out of workstation or mid range server with yeah, the thirty thirty two to sixty four gigabytes of RAM. So far, we got wonderful results with, at scale factor ten, which is half a billion edges graph with social network graph with half a billion edges. And we're working now to to to to to the next scales and get it to the five billion edges. So we benchmark Graph DB against the one of the most popular top three graphs engines, and we we score very, very well. We we get better results on the interactive queries. And I gotta admit we we are a bit slower on loading the data and and and updates. One way in which we can take benefit of what RDF engines do well is to use inference and we extensively use inference for this purpose. So for instance when you have to to follow a a a a path that involves two types of relationships, knows and has person, you know, on each step, in the chain like, right here. We use inference to to do some sort of shortcuts, these direct nodes, relationships which are shortcuts of the relationships that we're trying to follow, and this allowed us to simplify the query. And this way, yeah, we made a query that takes half a minute thirty times faster and gets sub sub sub second responses. So materialization using inference could be could be very, very useful, for some of these tasks. So we we we get also the craft reversal and then the finding part in the craft to recover. So, let's see what what the benefits, what what are knowledge graphs graphs all about and how they differ from other ways of doing similar things. So a quick quiz, let's think a little bit on what are, what is in common between these, these IT problems, these information management problems that that that I list for you. So think of a a a very, very big, retail bank that wants to consolidate information because it's IT systems to do root cause analysis whenever there is a problem like an ATM not functioning. Or a market market intelligence agency, that is that is the, provides reference price for oil and gas. They need to to have a really rich set of signals about everything that may have something that may may impact the price of oil and gas on some markets sometime. Or big investment advisory that wants to have the best m and a intelligence database on earth with all the companies and all the transactions and all the sections that you can do analysis by industries and technologies and so on and so forth. Or the fourth case here, building management system, yeah, consolidating the information about all all sorts of systems in a big building, like, the elevators, heating, ventilation, electricity, access control, and, everything like that. Then finally, I think of a pharma company that receives a compliance, a regular inquiry from FDA about side effects of drugs, and they gotta find relevant information in, within really thousands of long reports from clinical trials of drugs. What these problems have in common is that we have proven through the years that the knowledge graphs can help for each and every of those problems. And what then what makes them specific and and complex is that, well, you need a unified view across diverse information to be able to address them. So you need to combine several databases developed with us without in in different departments of finding the same organization, and to also use information extract information from, documents and unstructured content. And quite often to to to to get this right, you need to use external knowledge, global domain knowledge in combination with the proprietary data. And, this is this is very, very specific and very important for, domains where you have to deal and tasks where we have you have to deal with hundreds of thousands of concepts and entities. Or even if there are not that many, situations where you have very complex interconnections and lots of, relationships, not just the taxonomy, but more complex relationships between between, concepts and entities within the domain, as well as situations where, the the semantics of the relationships really matter to properly interpret the data. So, yeah, to to to to wrap it up, what what knowledge graphs do in these cases is that they they they provide the business with competitive insights through better intelligence. And better intelligence means, deeper understanding of the data based on a deeper knowledge, copy of signals that you can derive from diverse data, and also instantaneous updates that that that that come if you have data fabric or another another data integration platform that gives you flexibility and and sustainable updates. This is all these all these capabilities is what makes knowledge graph, helps knowledge knowledge graphs, disrupt several existing information management fields like content management, data management, knowledge management, and nowadays more and more from the business process management and automation industry and infrastructure, and and and manufacturing. So you you see that in the terminal nodes of this map of graph, you have plenty of plenty of different applications, that that we have seen knowledge gaps being being successful so far. So, the added value, to to again wrap it up is that knowledge graphs can, serve as a helper across different systems, data management, content management, and all sorts of metadata across them. And what's more important, we we we we layer semantic metadata, on on top of all these all these knowledge to make it easy easier to use. So on the technical side, what what it means in practice, is point one linking data together so that you can interpret them better because a conceptual network of thousands of concepts can, provide much more information, and computers can take much more meaning out of it as comparative tables, having the same data in in tables. Then the second thing would be is, to overlay semantic metadata to avoid unambiguous interpretation because when you get data from different sources, it is very likely that in a different context, the information can be misinterpreted. Having proper the specification of the meaning of the different data pieces is very important. Knowing what kind of price is this, whether it's with or without VAT, whether it's before or after sales commission, all these small things is what you need to describe in a very good manner so that you can get a unique useful useful unified use. And the worst thing here is that you you you, if you have a a stable semantic data model, and and the reference data, that's that's the way to to basically have continuous updates of your data and to be able to reuse data prepared for other analytic purposes beforehand. So finally, what is that RDF does better to to to cover all these capabilities that are necessary for for knowledge graphs, as compared to any other data management paradigm. So explicit semantics that that allows you to align the meaning of the different, modeling consumptions across different IT systems as well as you can use semantics for to validate data on a semantic level to check consistencies and this way maintain the quality of this combined combined data set. There are plenty of features in RDF that foster interoperability. So that's federation protocols and remote access protocols as well as, different flavors of syntax, that that that really help easy easy exchange of data. Global identifiers, which make sure that when you put two date two two piece of data from different sources together, the the the things that must be linked there will will click and link automatically, and there will be no no identifier clashes. And, finally, there is there there there are thousands of datasets available as RDF as linked data. Life science is good example, but now we we see more and more of this also in the, industry and the infrastructure. You may not know, but, now you can get the entire information about the electricity grids and network, in in Europe, in RDF and do quite a lot of interesting things with it. Finally, it's about standards, standards about everything, standard serialization syntax formats, schema languages, query languages, update languages, and everything on top of this. Using standards is we have can can come at a cost at the beginning, but, this is what what what makes your data management enterprise data management setup future proof. And also standards is what what what gives you great, yeah, much lower levels of vendor lock in. And what is that property graphs lack to serve such knowledge graph platforms? Well, it's all of this. So there is no formal semantics, there is very little to help you in terms of interoperability. And, yeah, standardization, they just started. So probably in five or ten years, it will be there. So RDF engines, yeah, the best platform to implement knowledge graphs, they check all the boxes and now also traversal and analytics, along with, yeah, all the all the enterprise requirements. And, yeah, I have to be fair that there's still case in which property graphs are better. So if you want to do some very specific programming Gremlin to explore graph, yeah, that's the way to do it. Or if you if you want to do really really heavy analytics with petabytes of data, then you you you take a rack full of, yeah, servers with terabytes of RAM And again, property graphs are likely to serve you a bit better. For everything else, RDF is what you need to get your graph infrastructure right. Thank you.",
  "language": [
    {
      "id": "http://vocab.getty.edu/aat/300388277",
      "type": "Language",
      "_label": "English"
    }
  ],
  "created_by": {
    "id": "https://example.org/cdkg/Creation/presentation/22",
    "type": "Creation",
    "caused_by": [
      {
        "id": "https://example.org/cdkg/Activity/presentation/22",
        "type": "Activity",
        "_label": "Presentation of \"RDF Leveled the Advantages of LPG and Keeps 3 Key Benefits: Standards, Semantics & Interoperability\""
      }
    ]
  },
  "digitally_carried_by": [
    {
      "id": "https://example.org/cdkg/DigitalObject/recording/22",
      "type": "DigitalObject",
      "_label": "Recording of \"RDF Leveled the Advantages of LPG and Keeps 3 Key Benefits: Standards, Semantics & Interoperability\""
    }
  ]
}