{
  "@context": "https://linked.art/ns/v1/linked-art.json",
  "id": "https://example.org/cdkg/LinguisticObject/presentation/7",
  "type": "LinguisticObject",
  "_label": "Textual content of \"How a Knowledge Graph Can Support Advanced Price Analytics in Supply Chain Management\"",
  "classified_as": [
    {
      "id": "http://vocab.getty.edu/aat/300027388",
      "type": "Type",
      "_label": "Transcript"
    }
  ],
  "content": "Hello. Thanks for the introduction, and welcome to the talk on the application of a knowledge graph to enhance price analytics in the procurement domain. Before we look at the agenda, let me introduce myself. My name is Markus Noelke. I work for Siemens Energy, now for more than ten years. My office is located at the factory in Mulheim close to Dusseldorf. As a cost value engineer, I act as a bridge between engineering, procurement, suppliers, with focus on improving our product cost position. For cost transparency and identification of cost out potential, I utilize extensively data analytics. In the presentation, I will explain the use case or motivation for the application of an application, with details on used ontologies, patterns, and tools. Presentation is structured as follows. I will give a short overview about Siemens Energy and then explain what I mean by advanced price analytics and will show you, the related knowledge graph implementation. And I will close out with a short summary and and next steps. But let us first, start with the introduction of Siemens Energy. Siemens Energy is active in the energy market, mainly as an equipment supplier, with a global footprint. More than fifty percent on the worldwide installed generation capacity is based on our technology. Although we have a fossil background, we are transforming to renewables, and our our portfolio is already more than fifty percent carbon free. Along our core products, we offer solutions and and services mainly maintenance and repair. We have products in generation, transmission, and storage of energy. The portfolio ranges from conventional equipment like gas turbines, to renewables, like wind turbines, through our daughter, we, Garmesa, Siemens Garmesa, and to hydrogen generators. The use case currently focuses on conventional gas turbines. So let us have a look at the product structure and, the, related, supply chain. Our product portfolio is, quite large, including systems, with subsystems or sub subsystems. So here on the left, you can see, there's a complete gas fired power plant, which we sell. And then what we also sell is the just the gas turbine package, which is within this power plant. And the gas turbine package can be further broken down into the core engine, the gas turbine, the actual gas turbine which we manufacture in house, and also auxiliary elements like the air intake which provides filter air to the filter air to the to the gas turbine. So this, we buy, complete as a as a system, but as mentioned, the gas turbine, that is manufactured in house. So here, we need to further break down, into components, which are quite numerous. So from machine parts to raw material, standard parts like bolts and nuts and and and and so on. And and so we we buy all sorts of things from systems all the way down to to raw material and also seems, energy buys a lot at about ten billion euros annually. It's the purchase volume. Almost all components, are engineered with, lot size one. They vary from project to project most often. And, yeah, the specification are always need need to always to be adapted. We have a global purchasing footprint, with regional hubs, and, like, with global supply base, of course, following our projects and and customers. And we have, lastly, facing strong competition and our our cost base is heavily influenced, of course, by the external supply, and we have to to buy at best price. And there we have finally our use case. We use data analytics to detail to to determine if we actually buy always throughout the entire organization at best price possible. And one example or one method here is the, perform what we call the performance pricing. That means relating, the price of a purchased item, to its aspects and properties. So here is one example of a dashboard with an air intake portfolio, where prices for air intakes are shown globally for our entire, gas turbine, portfolio from from small gas turbines, with low mass flow to large gas turbines with a large mass flow, and we have all the cost information, mapped here in this chart over the mass flow. And and we have, one one type of of data is from from our estimated from our product tracking databases, for instance, from proposal calculation and so on. And the other prices are actually projection prices from from purchase orders which are fixed, and this can also be shown here. And, with this method, you can compare price over a wide range of application with varying parameters. The results might be a benchmark prices so you can determine an average price line here for instance over the the entire portfolio, or you can even determine a lowest price line, or you can investigate further outliers here, and and and do further drill down. So, also, you can derive target cost here if you want. So if you design a new gas turbine and you just know the functional parameter, mass flow, you can go in here and determine a price for this component which is not actually designed at this point in time. So that is, as you can see, that is a very simple method with just one parameter but in reality we have of course several parameters, which are crucial for for for for influencing the price, which we call the cost driver, and and they need to be evaluated in a multivariate regression. But also, you can see that this easy statistical method also relies on the collection and integration of lots of data throughout the whole supply chain organization, but also integrating a lot of technical information. And that is the problem we try to solve with the knowledge graph. But there are some challenges as, yep, as probably in many organizations. We have, unstructured data like pricing information comes to PDF technical documentation. We have Excel sheets. We have drawings. Some information comes, per email. Also, as shown, we have high complexity, of products. So the it's quite heterogeneous what we buy. We have also procurement and also engineering and manufacturing is distributed globally. We have that really results also in different philosophies, legacy systems, and and related vocabulary. And, of course, we have data more organized in applications, and if data is pulled out of its relational databases mostly the context is gone. So, the general idea here is to use a good knowledge graph, to integrate, the data and, defining a shared vocabulary across all the domains, procurement, engineering, technical vocabulary, but also service, for instance, and manufacturing. And there, the idea is to, to have a data integration cut from within based on on the knowledge graph, and then we bring together all the transactional data, planning data, design data from various data silos, and, we map them, to the individual concepts classes, and and and shared the vocabulary, and and we can evaluate it. And in that process, information becomes knowledge that can be formally processed by a machine. So here, this is one example. Firefront is assigned a unit, which is defined in the core ontology of the QEDT. Then we have, okay we say it's an inlet mass flow, it's a value for an inlet mass flow which is defined in the domain ontology that can also be defined by international standards. And then, this inlet mass flow is mapped to to technical asset, the air intake, which has also a corresponding semantic definition. It's it's a technical asset, which again is defined in in the core ontology. And then, yeah, this this data with a shared vocabulary, this map data can then be shared to applications through an API, and the apps can query content from the notice graph via sparkle queries. And and here we have also on the top our performance pricing, dashboards, but also, yeah, just simple query visualizations, traversing the graph, doing graph, explorations is also possible. But let's look further at the ontology framework, we are using here. So modeling scope, respectively, technical assets is is done along with the ISO and the asset specification model from the REDI project. So here, the gas turbine, in this example, the gas turbine auxiliary system is modeled, with a system engineering approach starting, with the main system and, breaking it further down into, subcomponents, all the way to the bottom here. So that is that generally is done related to aspects, to the aspects function, product, and location as per the ISO, eight one three four six. The product aspect is here omitted for the sake of, simplicity. So function and location are here, part of quite quite a different breakdown of hierarchies. And and, by mixing the two aspects, variable context contextual, information is is is lost. And and here also location can be referred to, boundary conditions, where you apply your technical assets. Along with the aspects, functional properties, we we which are modeled here, which is the the power, for instance, requirement for the guest, auxiliary system, and also the mass flow. We can also, model location related properties, which wind which is wind speed, temperature, which is different indoor than, compared to outdoor. And, so and and then, the the, and an integrated object is created, through connecting, the respective objects in in the hierarchies, and that is basically the specification for for this, intake. And, it's it's a functional object, and it integrates all the aspects and and, and all the requirements are then flowing down, to this functional object. And, then this objects can be also related to to offers being made for such an asset, or purchased, orders being placed. And this gives the connection between the cost data, and the technical parameters, which can then be utilized, in the performance, pricing effort. But be before we get to that, I show you how to, to model the, the properties. So what what are the typical property patterns here? Here we have, the the ISO one five nine two six fourteen pattern with a verification , which has a higher, expressivity, than modeling properties directly, to the assets. So, yeah, here we have, the the the type, of of of the property as a as a as an instance of of a class, where the class is, this is the nominal operating mass flow, which is defined by a g force standard. And it can also, classify, the functional object further into, an oil and gas type of equipment. And then to this, mass flow instance, we have the actual value, connected and also, the, the unit, which is coming from QEDT or is mapped to the QEDT framework. For the purchase order, the quotation here, we have to map, for instance, a monetary value, which is, a class from defined in in the Fibo ontology. And and there, we have, also, yeah, the value, then the verification, with the value connected and also the currency connected. And and the currency, is defined also through the Fibro ontology where you have all the currencies and all the country codes, mapped, and and defined in the ontology. So yeah. And and that brings us then to the, yeah, to the integration and into our use case. And, here's the example, we we looked at already. And here you can see how how everything comes together. So we have here now, the our functional object, basically, with the connected estimated as pricing, for instance, from a quotation. And we have the properties basically here from from this, asset specification model. So in this case, the mass flow, which are then maps, so we we have this connection here, and we have also the connection here of to the purchase part, which is, coming from a purchase order transaction for a different, functional object, and and that gives us the possibility, to, derive that, dashboard, in an automatic way, for all sorts of technical assets which are defined in the knowledge graph and all, defines, technical, or properties or qualities we have in the knowledge graph or which can be fine, and and then we can do our analytics based on that. But, to make that all work, we need reasoning. Here, it's based on on all, or additional data log rules, based on the RDFox implementation we have here in in various areas. So one one example here is the, the property allocation from the partonomies. So if you're on top, we have defined, power requirement. It is, then serving down to the subcomponents, basically. Also, the part of is is also it's a transitive property, so you have also that that connection. But for instance, if the filtration is also part of the GG auxiliary system that is then a separate property, so do not not mix direct part relationship to to to indirect part relationships. Then we have the aggregation of cost. So if you have, a percolation scheme where we have to add, yeah, certain components, and and and aggregate it into into a higher scope, for instance, for mechanical equipment for a certain project. That is also done through reasoning, and we have the integration of the data with identifier matching. So here, ISO eight one three four six IDs can be used, or also, legacy identifiers from the various databases to bring, the data together. And then finally, we have, constraint checking here with subclass requirements to check if, certain technical assets, where we have the, yeah, requirements for for certain technical values if they are present and also if they are in a certain range. And and and here, yeah, we need to rely also on closed word reasoning, which is also coming from the RDFox database technology. And that leads me to the architecture which is implemented. So this is the software architecture, starting from the from the data sources. So here we have a center query, point, via Denodo. Through an ODBC connection, we can access, for instance, SAP data, or other databases. We have connected Oracle database, Redshift data warehouse, and or a simple Excel sheets. And then, yeah, we can, via SQL, we can get the tabular data. And in in in the SQL lens, this data is then using the r two r m l mapping files or mapping language, is then transferred into r d f RDF, so called n quotes which contain also as a fourth dimension the provenance data, that is put into the TRIPISSTORM, yeah, which contains all the provenance data and all historical data. And then, yeah, through either direct replication or special queries we have connected our RDFox reasoning engine, which is, yeah executing in memory, inferencing and reasoning based on data log rules or old ontologies. And, yeah. And that is the the databases are queried through, the Metafactory front end, which is, a low code, yeah, tool to enable traversing, through the graph and visualizing graph data, within HTML pages, and so that the spaCy queries can be directly incorporated in HTML pages. And also there are web connectors defined for a dashboard tool like Tableau, where you can directly then, from Tableau, query into through Metafactory query into the data in the graph database, which is not directly possible from Tableau. And also, yeah, you can, through Metafactory, factory, you can realize certain class based visualizations or just simple graph exploration schemes. And, of course, that is then connected to a user management system. Alright. That's all. So I would like to conclude the talk with conclusion and and also some next steps. So I think the, the, knowledge graph is a good solution for for applying performance pricing at scale. It really reduces, the manual work we have to do right now to collect all the data to clean, and and and to to put it into a spreadsheet so we can analyze it. So with this approach, an automatic or semiautomatic performance pricing throughout all components we are buying is possible. But, also, what is what is, yeah, I think, clear is that the supply chain is in a difficult domain to model because we we are more inter interface, discipline. So we rely on on on good data, and the right data models from from, engineering, from the product life cycle management. And, yeah, then so so we, we and and, also, they they said the the models, are quite heterogeneous, and that makes life difficult in in the knowledge graph. And, also, I think, closed word reasoning is is essential, to get, yeah, the reasoning we we need, to to make connections and and to to do the constraint checking here. And, yeah, possible next steps, I would say, from my perspective, is really adding, equations. So once we have the statistical analytics in place and we have to derive, for instance, a parametric formula for an intake, I would like to put that into the graph and also do reasoning against that. So if if, let's say, identifying outliers based on that and and utilizing the numeric calculation also for further, analytics. And, also, of course, it would be very nice to link external data, like a material index. So for instance, if you have, so you can map, for instance, for all technical assets which use in certain carbon steel. You can map easily, the price index and have, direct information flowing into, what are the price, what is the price increase for this particular raw material. Also very important for for certain for tracking, price, for for tracking, price movements in the in the market. Yeah. And lastly, I think also it's it's a good possibility to to do, data exchange with your business partners, with your suppliers, but also with your customers. So with suppliers, I could imagine that at the end that, proposals, are really exchanged through the knowledge graph, and we don't get PDFs, which we have to, then evaluate further. So and extract the valuable information, but, yeah, so we can get, already the, the semantic enriched information from a supplier. And, also, we have already projects with customers where they are asking for a certain data model, for instance, in Geforce, which needs to be modeled in, in an old graph. Yeah. So that's concluding my talk, and, yeah, I'm happy to answer additional questions. Thanks a lot.",
  "language": [
    {
      "id": "http://vocab.getty.edu/aat/300388277",
      "type": "Language",
      "_label": "English"
    }
  ],
  "created_by": {
    "id": "https://example.org/cdkg/Creation/presentation/7",
    "type": "Creation",
    "caused_by": [
      {
        "id": "https://example.org/cdkg/Activity/presentation/7",
        "type": "Activity",
        "_label": "Presentation of \"How a Knowledge Graph Can Support Advanced Price Analytics in Supply Chain Management\""
      }
    ]
  },
  "digitally_carried_by": [
    {
      "id": "https://example.org/cdkg/DigitalObject/recording/7",
      "type": "DigitalObject",
      "_label": "Recording of \"How a Knowledge Graph Can Support Advanced Price Analytics in Supply Chain Management\""
    }
  ]
}