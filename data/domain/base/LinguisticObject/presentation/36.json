{
  "@context": "https://linked.art/ns/v1/linked-art.json",
  "id": "https://example.org/cdkg/LinguisticObject/presentation/36",
  "type": "LinguisticObject",
  "_label": "Textual content of \"Sales AI: Building and maintaining a knowledge graph\"",
  "classified_as": [
    {
      "id": "http://vocab.getty.edu/aat/300027388",
      "type": "Type",
      "_label": "Transcript"
    }
  ],
  "content": "So it's my great pleasure to, introduce Eran Avidan of Intel. Eran is a machine learning engineer and architect of the advanced analytics group at Intel, which delivers AI and big data solutions across Intel. His work at the Advanced Analytics Group includes research and development of cutting edge distributed architectures in the AI domain. So let's give a warm warm welcome to Aram, and, and we can get started. Over over to you. So hi, everyone. So I was introduced, very formally, very excited, to be here, starting this, presentation day. And, today, I want to share with you and give you a bit of a overview of what we did for our sales department at Intel. Actually, a project, building and maintaining a knowledge graph we started working on two years ago. And, to continue my introduction, my name is Ivan, and, I'm a machine learning engineer and the architect of, Intel IT AI. And been working at the Advanced Analytics Group for the past ten years or so, where we provide, products and solution across many of Intel's division, specifically machine learning and artificial intelligence, solutions from, helping better design the chips through manufacturing, increasing the yields through analytics, up until sales and marketing where we help increase the revenues by recommendation systems, and so on. And we even have some products, working with Mobileye, harnessing insights from cars that transmit data through, Mobileye sensored vehicles. But today, we're gonna talk about sales and marketing, and I would like to introduce you to our sales AI system. But before that, let's talk a little bit about our sales department. Intel sales department, like every other big enterprise, is is huge. It's actually a division. It's huge. It's composed of thousands of people, account managers covering hundreds of thousands of, customer accounts. And they can offer, hundreds of different products from CPUs, soon to be GPUs, and memory and storage. This enormous complexity, may lead to some missed business opportunities. And this is why we've built, this sales AI platform. We're looking to bridge this gap of missed opportunities by first, in real time, scan customer information, store it, detect intent to buy, and maybe identify some potential opportunities, automating and recommending a personal or best action for the account manager, and then feeding back to the system learning based on customer response. So that's the sales AI framework we've been working on for the past six years. And over the past six years, our sales AI program led to a community of sales revenue growth of approximately more than half a billion dollars. And through those years, we've supplied different solutions for account managers. And to give you an example, one such solution is called the sales assist as it assists account managers in identifying new business opportunities, with leads about their respective customers. For instance, an account manager can open his dashboard and see the companies that it, he or she covers and see that company a, we're gonna talk a lot a lot about company a, is about to attend the conference. Or maybe they browse into website for products they never purchased before, and this may lead to a business opportunity. And we all these, we supplied many such solution, but then we started thinking, well, all these insights that we give account managers, are they visible for any future model or system that inquires about company a? Or do we even have a single layer of sensing where we store all the, information we collect for future progress? Do we store back all the insights into that same place where someone can query about company and get all the information back? And the answer, as you can imagine, was, no. We do not. So we decided to start studying now. We decided to record all activities as well as insights into a single centralized place. This would basically simulate in the sensing part what an account manager would do. So we look at sensing at scale in a nutshell, what I'm talking about to, give you an example. It it starts from basic information that the company gives us with enrolls into Intel's program, like location and maybe the field it works in. Then we can start recording some internal information, sales transactions, what they bought, how many, maybe what courses they took in, for Intel products. Up to, collecting customer website information and going through social media and recording these activities as well. So usually, when you take pieces of information, then they're scattered all over the place. But we decided to take all those silos of information, all those silos of data, and and put them into a single centralized knowledge graph. And why would one store store these things in a graph structure? So for us, the motivation was, I believe, similar to a lot of others. The world is composed of entities and relationship between them. So this makes things very easy to explain. When you give predictions or even a simple analysis of a situation, you can show a graph and it makes sense. When you want information about some kind of entity, for instance, company a, everything is really close. So I want things about company a. Like, everything is one hop, two hop, or maybe three hops away, but I get this nice image about company a. And since everything is really dynamically structured and you're not limited to any schema, you can simply add more data easily, connect it to company a if it's relevant to company a. And when you have, started growing a nice knowledge graph, then you can infer knowledge directly from that graph, simply by traversing it or running simple, graph algorithms. And later on, you can use that graph as it grows and grows. They have a really great knowledge base for AI as we're gonna see in a few slides. So we started thinking we wanna grow an old graph, and I'll illustrate how things work, when it comes to sales and marketing at the end. First off, company a enrolled into into this program to become a distributor or buy products for itself. What it was enrolling, get some basic information. It says that it's part of the IoT and, the NAR industries, and it's it's a builder in those industry. Later on, we started saving some internal information. Through sales transaction, company a went on and bought product six and product y, and we connected company a to those products in our knowledge graph. And as a side effect, we got that company a is interested in data center products. Then looking at company a's website, we we've noticed our smart medical devices. So maybe we can add this edge of company a to the health care industry. Then there was a social media tweet that company b have invested in company a, so we can connect company a with company b. And if we're lucky, we may have company b in our knowledge graph, and then we get some secondhand information about company a. Company a went on and browsed intel dot com, and we recorded that activity. And there was more and more activities which we recorded and added edges for. And what's interesting is that we started only with what we were told about company a given by company a. And this union of internal and external data sources can create a really strong knowledge base, which is basically the basic basis of our knowledge graph. You can simply take any piece of information you get about company a and edit as you're not restricted, to any specific schema. And this information, and this is key, will not be hidden somewhere in some other data sources. You can simply query about company a and everything, by the number of hops, and you will get all the information about company a and anything related. You can then if you wanna train a model relating companies, you can say, I wanna extract features about, company a. So maybe I'll I'll do it by proximity, and I can rank those features by any graph logic, I determine or, for instance, by the distance from the company I know. And then I get a set of features to train my AI model. And if we think about it, most of these things are facts. And using the knowledge graph algorithms, we can generate more and more edges with some confidence that as our assumption, is and was and still that we live in a world where we always have only part of the knowledge even if we have all the data. So in a graph, this is is very easy to extend our knowledge by adding more edges between existing nodes, between existing data points, simply generating more knowledge with data that we already have. And let's move, to the more technical part or that the idea or the solution that we built in order to make this graph grow. So identifying information was basically flowing in through the system. We've decided to build a stream processing system. Specifically, we build one asynchronously to make it also more efficient. And we start thinking what would, what we would need for such a system. And we've identified several capabilities we believe are crucial for this process, and we've designed, respective component for each one of them. So if we look at the architecture in high level, and again, this is an abstraction, the capabilities are the key things here. We see that we have a microservice architecture, and we have a component for each capability we're gonna talk about. And we have a message bus since everything is asynchronous. We have a message bus in the middle which synchronizes things. And there are loaders which load internal data. And since we because this is the basic information we have and we knew we wanted to load it, But then we knew we wanted to transform it somehow into graph formation, so created the transformers, which is the second capability. We want to enrich this internal data with external information. So we created the extractors, which go out to the web and fetch new information about internal information where all they have. Everything needed to be persisted into a graph database, so we created the graph builder. And to keep everything up to date, specifically external information, we created refreshers. And let's talk a little bit about each one of those capabilities and, what we believe we need in order to make this happen. So the loaders, they import internal data as I stated, and they support a variety of sources and formats. It's it's kind of a plug in system. It's a microservice based, architecture, and they work with the push or pull. They can query daily from SQL database or can they can get a push notification from the size five system that the CSV have just arrived. They load the data. They convert it into JSON format and publish it to the message bus to later be transformed and persistent. So the loaders are fairly simple. The second capability, this is actually the basic processing unit of the system are the transformers. And these do the most, let's say, probably are the most busy logic wise as they convert information into graph semantics, into graph formations. They're always on. They're all they asynchronous. They're stateless, which make them by nature extremely fault tolerant and scalable. We specifically use the Kafka streams technology, but you can use whatever you want. The transformers know how to transform information into graph formations by configurations per entity. This specific transformer knows how to transform product information, which it gets from internal data or external data into nodes and edges. And it does so simply by configuration. We've created a nice YAML based DSL that tells which fields become which node and which, field becomes which edge and which nodes it connects. And it goes on and on. And this DSL became more and more robust up until a system analyst can simply configure any new, source information it found, it finds in order to instruct the transformer how to transform this information into nodes and edges. So that's, let's say, like the, middleman, the expert in the middle. Since the internal information is not enough, we we created the extractors, which actually trans extends transformers. They also transform the information into graph formations, but they also enrich it with some external information. So on top of being configured per entity, per product, in this case, they're also configured per data source. You would need to instruct them how to get this external information and what to do with it, with what kind of post processing to do with it when they, retrieve that information the information . For instance, running matching algorithms of sorts and scoring the validity of the information they just got. They just received, that is. The next capability is the most b d the like, the busiest place in our system is the graph builder. It's only sure is to translate those graph formations into graph query language. In our case, we support Cypher as we work with Neo four j. It's the most prevalent, I think, GQLs out there. And the graph builder, you can think about it like a microservice data layer. Nobody writes to the graph database by but the graph builder Since you wanna keep everything in check, you don't want any other processes disturbing, your throughput or the integrity of your knowledge graph. The Graph Builder, same as every other mod microservice, is very scalable. So you can scale it out or in, depending what throughput you need and what resources you have. And if you're worried about the graph database we chose, Neo four j. So when we started working at two or three years ago, we've examined many graph databases. We found Neo four j to be the most mature and and fairly stable one. It's really easy to use. It supports Cypher, which actually decouples us from Neo four j as we can always switch to other databases that support Cypher, that work with Cypher. But for me and and a lot of other people, the the best selling point was the great UI. You you remember that we talked I I talked. You listen about, decks. The it's easy to explain. Well, it's easy to explain if you have a UI. If you don't have a UI, it's very hard to explain. So, this is actually a screenshot we're gonna talk a little bit about later that I took a year ago from our knowledge graph. The last component of refreshers, they're fairly simple. They are the only one that query the graph as they simply traverse the graph looking for stale nodes. Once they find the stale node, they trigger an extractor. In this case, a product extractor, in order to fetch and refresh that, data or that information in the graph, and they move on to look for other failed notes. And if you wanna keep your graph up to date, you should have a few running and traversing the graph. They're like agents looking for old nodes and edges and updating them, as time goes by. So this was the architecture. But let's talk a little bit about what we did with and how about what what the graph structure actually looks like. So the first thing that we will approach with, like, some kind of proof of concept, was we the business wanted to get a view of customers and their partner companies. So how would you how would we configure the system to get partner companies for, inter customers or companies we already have. So we configured the system in this manner. Right? We needed a few loaders, sales, company, product. We needed the corresponding transformer for each one of those loaders so we can transform this information into graph formations. And the key configuration here, the key implementation, it was the partner extractor. Right? The partner extractor was implemented by crawling, partner page in the company website, analyzing the logos through deep learning models, and connecting the company with their partner company's URLs. To in order to keep this up to date, we also, put in partner refresh. And after configuring the system this way and letting it work for a few, a few days, we got this nice graph as we can infer knowledge directly from simply by granting it and getting this nice, UI. And what we see here is that we have company URL cell and some other companies. And here we can see an image of two companies that buy things from Intel and their partner companies. So if we look at specifically the graph formation we chose to use and since this is, let's say, quite different than you would expect, basically, in order to keep things dynamic, everything that has more than a single relationship is a node. As you cannot connect a single edge between more than two nodes. Right? URL can be used by several companies, so we deemed it a note. Sale is an action of purchase, you can say. Company a purchased this product, but it's also a sales transaction that may have different relationships such as buyer, seller, product, etcetera. So we made it a node as well. So we were pro nodes, as it makes things more easy to manage, easy to persist, and you would not lose any information. But while but while it's great for inferring simple or inferring knowledge directly from the graph by simple queries, it is not optimal for training machine learning models as we will soon see. Another principle, we found important, was to keep trace, as I mentioned, of information and where it comes from, as well as the option that there could be inconsistencies when retrieving data from external data sources. And this was the key feature, like, enriching with external information. And I'll illustrate. We remember company a. So maybe we went on out to some company a's Wiki page and we've classified that this Wiki page, let's say, predicted that company a belongs to the cloud industry with some sort of probability. So we can do this, but then we lost a lot of information along the way. Like, we lost that company a belongs to the cloud industry through this Wiki page, which also, talks about NAR and, also what if company a has another Wiki page. So we don't want to keep track of all these things, and then maybe company a belongs to the software industry through some Wiki page and the cloud industry through another Wiki page. And maybe later on, we would like to run an algorithm on top of all our Wiki pages class reclassifying them since we got, like, a better algorithm. And we do not have to, touch or change anything with company a since everything goes through those Wiki pages. So that just gives us more flexibility and we don't lose any information, which is basically we want a good knowledge base. So we kept everything. Having said that, what I said about machine learning, this may require to query a fab graph for training machine learning and artificial intelligence models. As knowledge graph algorithms usually aim to capture some sort of correlation between nodes and edges. It link prediction, entity resolution, for instance, deduplication, classification, by using other nodes or maybe even link based clustering. All of these, can be done usually by predicting the existence of a a triplet via some sort of score function. So when you wanna train your algorithm on top of a knowledge graph, this simply won't work as there are these Wiki page intermediate nodes. And they disturb the algorithm because it doesn't need it in order to predict the edge between company a and the cloud industry. So this would be a better choice for the algorithm or the only choice that the algorithm can run on top of. But fortunately, this transition this transition between the full blown knowledge graph into a sub graph for the algorithm is usually very fast complexity as well as code wise, at least according to our data scientists as this is what they do. They query for a subgraph and train the model on top of it. I just wanted to see where I stand, time wise. So the last piece I wanna, touch upon is when you have a really reliable, knowledge graph, we can create new knowledge using the graph we already have and enriching it without going to outside sources using knowledge graph algorithms. And this is a paper we've, published about a year ago showing how we used our knowledge graph in order to perform customer segmentation into verticals, and the specific deep learning and machine learning models used in this study, relay on Wikipedia information as well as home page information and, of course, internal information we already knew about those companies. And all of the above, could be found in our knowledge graph, our knowledge base, is thus and and thus enabled this research. And if you find this idea intriguing, you can give it a look at, Google Scholar or the Intel AI website. So this was, like, an overview of what, we were trying to solve in our sales AI platform and how we grow a knowledge graph. A little bit glimpse of the architecture and the capabilities we found were crucial in order to make this happen. And I've I hope you found this interesting. And, I believe I'm I'm done, and I'm open to questions. We do have a couple of questions for you, actually. Okay. So some people some people were asking, like, if how did you manage to use Neo four j, with triples and the, semantics part and all of that stuff. But, actually, they already answered among themselves that there is a plug in that, for that from Neo four j that allows you to do that. So I'm going to Okay. Give you a more specific question, which is, what is the schema that you keep inside Kafka? And if you serialize your graph data, JSON or something similar. So, what was the schema that what's the schema that you use inside Kafka? What do you mean? This is the question. It's, no. So I I I'll I'll be happy to answer it, but I I don't really I don't really understand the question. So, basically, whoever asked the question, it's probably a good question. So you can ask me, like, in Slack. Maybe we can talk, and you can explain the question, and then I'll answer it. Maybe. Okay. So let's see what else we got there for you. Right. We have a question from who's asking how would analysts or business users, gather insights, from your knowledge graph, or is it only for use by data scientists? Oh, so, for leads, like, the beginning of the process finding, simple insights, like the partner company insights we saw. They can system analysts can, take this query the graph looking for some manually looking for some insights before we start automating them, showing those snapshots to the business units to see if they're viable. And if they are, we can start building, more robust models and solutions that will they won't need the graph image anymore like the the drawing. They will simply get the recommendation, according to the graph. But this is mainly the querying is done for, for the first stages when you're looking for hints of things which you can, get, benefits from opportunities from. Okay. There's another question from, Matthias Sisbuie, if I'm pronouncing correctly, who's asking whether you can speak about the iterative process to build, such to build this from scratch. So, actually, it's not it's as as you've seen, it it's composed of, like, five different microservices with a message bus. So this is more of a software engineering question because we first, we put everything into place, like, all these capabilities, and they were empty. Right? They did, like, simple things or they almost did nothing. And then we started once everything was ticking, we started putting in the logic inside the system. And, so but that's an iterative, microservice framework, development process we use, most of the times. We put, the entire framework, like, the key things in the framework. We mock it, and then we started putting the logic inside. Okay. I have another question from Deborah who's asking if you could further expand on how you handle entity resolution, meaning how the loaders and transformers know that two websites are referring to company a, but they're, for instance, spelled differently. I'm not sure I I understand, but I'll try. Like, the way we transform this information into graph formations, Currently, we did an expert that would manually define using this YAML, DSL we created which piece of information turns into what. But it does so one time as usually we work with, APIs which return data in a known structure. Right? Like, you get the JSON, you know what fields to expect. So we do that for each entity. But we do it one time and we let it run on all of the same type entities, if that was the question. I'm not sure. But even if it wasn't, I think I think the question probably refers to entity disambiguation. Say, which is a very well known and very thorny issue in Novice Graph. So how do you Oh, okay. Okay. So when you get the external information and you wanna make sure it belongs indeed to the company, like the example with the Wiki page. So we ran, when we fetch external information, we always have a matching algorithm that matches the internal entity that we already know is correct by company a with some external information we got from this Wiki page that claims it's company a. And we do this cross matching algorithm and we give it, a score. Like the. What I I I think it's confidence level that I wrote here and we give it a score, how much we believe that company a actually, like, this Wiki page actually belongs to company a. We do not make this a strict connection. It's kind of a loose connection with some certainty level. So if you know, it's query the graph and say, I want only weekly information about company a, which, is above ninety percent, certainty, like, confidence, if it makes sense. Okay. There's a quick one, and I'm guessing the answer is probably no, but I'm going to ask you anyway if you have, like, a gift card for this. Fortunately, no. I'm saying fortunately because, the the process of getting, extremely difficult. I would love to do so. I cannot do it on my own. So no. I I I don't, but I I'll be I'll be glad to contribute any piece of knowledge that I have to anyone that wanna talk. I'll I'll do it for free and for fun. That's that's fair enough. You know, an open invitation to people as well. Yeah. I have a there's another one from Mark who's asking something quite fundamental, actually. Why are you using your message bus? Couldn't you work directly on the graph database? Well, that's, again, a software engineering one. Since you wanna you wanna control one of the basic things with asynchronous system that you wanna control the throughput. You would don't want burst to turn down, like, destroy your system. That's why we have Kafka. You have back what's called back pressure. If you got a lot of information flowing in and you don't have enough connections to the database and the throughput of the database is not enough because there's an IO limit, the bottleneck will probably be your graph build. Right? And you want things, like, to queue up behind your graph builder. You don't want the the service itself trying to create a graph because a lot of services will try to create a graph and they just bring it down. You wanna control the throughput, and that's one of the reasons you use the message bus and the one of the reasons Lambda functions use, SQS, etcetera. Okay. Makes sense. I think we're good, actually. I mean, we can as you said, you're on Slack as well, so you can continue the conversation there if people have even more specific questions because I think some of them were actually quite specific already. So I guess we're good to wrap up. Thank you once again, Aran, and we'll see you. Thank you. Thank you. Bye bye. Cheers. Bye.",
  "language": [
    {
      "id": "http://vocab.getty.edu/aat/300388277",
      "type": "Language",
      "_label": "English"
    }
  ],
  "created_by": {
    "id": "https://example.org/cdkg/Creation/presentation/36",
    "type": "Creation",
    "caused_by": [
      {
        "id": "https://example.org/cdkg/Activity/presentation/36",
        "type": "Activity",
        "_label": "Presentation of \"Sales AI: Building and maintaining a knowledge graph\""
      }
    ]
  },
  "digitally_carried_by": [
    {
      "id": "https://example.org/cdkg/DigitalObject/recording/36",
      "type": "DigitalObject",
      "_label": "Recording of \"Sales AI: Building and maintaining a knowledge graph\""
    }
  ]
}