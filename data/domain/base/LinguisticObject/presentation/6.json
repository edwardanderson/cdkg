{
  "@context": "https://linked.art/ns/v1/linked-art.json",
  "id": "https://example.org/cdkg/LinguisticObject/presentation/6",
  "type": "LinguisticObject",
  "_label": "Textual content of \"Graph Thinking\"",
  "classified_as": [
    {
      "id": "http://vocab.getty.edu/aat/300027388",
      "type": "Type",
      "_label": "Transcript"
    }
  ],
  "content": "This talk is about graph thinking, and the slides are online. There's a URL shown here. Actually, there's a lot of background material, a lot of links. Let's, let's consider a scenario. There's a village somewhere in the woods. In our village, there's someone named Pat who runs the pub, local pub. Pat has a couple of friends, Hannah and Thomas. Now Hana works the fields, grows the grain. Hana has a friend named Aiden. Thomas works poultry, raises hens and produces eggs. And Thomas has a friend named Brenda. And then Aiden is the miller, Hannah's friend. Hannah sells grain to Aiden. Aiden, in turn, has a friend named Chris. And Brenda, Thomas's friend, works the brewery. Brenda buys grain from Hannah and she makes beer, which she sells to Pat, who runs the pub. Brenda also has a friend named Kim. And then Chris, Iden's friend, buys eggs from Thomas and buys flour from Iden's mill and produces bread. Chris works the bakery. And of course, his bread is sold to Pat, who runs the pub. And then there's Kim, Brenda's friend. Kim works the recycler, and Kim buys organic waste from Thomas and from Brenda and Chris and then mix fertilizer to sell back to Han. So we had a graph that we described there. Now if we're gonna put this into a normalized form for a relational database, you see the schema here on the left. You would see maybe six tables if you had a fully normalized form. And the thing is, the information about the relationships in that village are completely dismantled and destroyed and atomized here by having a relational view. How can you look at this and tell what those relationships are? On the other hand, if you've got a graph, you're presented with exactly that context: the relationships of who knows who in this village, who produces what, who sells to whom, and just the general flow of commerce that's going on around inside of this little Black Forest medieval village. Now the thing is that graphs bring a kind of network view. They bring the data closer to people who can make sense of it. This is acknowledging the the complexity of the context. This is about identifying emergent patterns. This is about being able to make informed decisions based on the relationships that exist. That is the essence of graph thinking. Really it boils down to thinking in patterns. So let's get a couple of examples here. Hannah's relatively new in the village. She'd like to expand her business. She's noticed that one of her customers, Brenda, happens to buy a lot of grain. Now who are the other villagers who are similar to Brenda? Perhaps she could upsell. Well, doing a little bit of work with the graph here, you can see there's another person named Chris who also sells product to Pat, also sells waste to Canon. You know, maybe Chris's bakery would be a direct customer of Hana. I don't know. Selling grain directly to a bakery. Maybe they could sprout or make malt or something like that. Hannah is also interested in sponsoring a co marketing campaign here in her medieval village. And so she'd like to try to help drive the demand for more grain. So we can do some analysis here. Who are the customers of Hannah's grain customers? And so for this, we do some graph reversals. We're looking a couple of hops out. We noticed that Chris, Pat, and Kim are each a minimum of two hops away. And it turns out we heard about Kim a couple of times here. And just to make it more interesting, a tech billionaire uses time travel to relocate back to this medieval village in the Black Forest. Which businesses are the most influential as potential acquisition targets? Well, it turns out if you do some graph algorithms based on this data right here, there's a family called centrality. There There's a variant of this called between the centrality and it's really looking and seeing which nodes are between all the other nodes. If you calculate that out, you find that in fact Hana has the highest ranking there for between and centrality. Chris has the second highest ranking there. So, you know, these are the businesses that probably would be your acquisition targets for Musk or, you know, whomever. And it's interesting to me that when I first showed this graph to someone who has absolutely no background on graph algorithms, my my friend, she was able to immediately look at the graph, look at the relationships, and say, Hana's really hanging out there. I mean, the person making the grain is raking in the money. And indeed, that's what graph algorithms show. This is about the complexity of relationships. Now there's a lot of background when we study complexity of relationships. I wanna cover these in a couple of ways. If you go back to nineteen ninety nine, Dave Snowden was a consultant at IBM, and he developed a framework called Kinev, which tries to assess a kind of context in which business leaders are confronting problems and need to be able to apply decision making. You may have heard this referenced. It it was where, sort of the origin of unknown unknowns, which of course was referenced after nine eleven. But, in the Cynefin framework, you proceed from, say, simple context where there are established facts. You just need to go in and categorize the needs and apply the best practices, the rules. Or you can progress in more complicated kinds of situations where some kind of expertise is needed, some kind of analysis. So analysts go in, they assess the facts, they provide their analysis, and then leaders can respond based off that analysis, sort of, you know, warehouses and business intelligence. But then you can get into more complex business environments where you really must probe the situation. There's no clear answer. And the way through it is to be able to sense emergent patterns and make informed decisions based off of understanding those patterns. And that is the complex side of business decision making. And increasingly in our world, we're seeing more and more kinds of complex challenges, whether we're talking about tangled supply chains or pandemics or on and on, climate change, etcetera. The world of business has become increasingly complex as we've globalized, and these are the kind of things here that must be applied. This type of confronting complexity as opposed to sweeping it under the rug and just trying to apply best practices. Now there's a corollary also if you look at pedagogy, if you look at learning theory. Susan Ambrose, has a book at a two thousand ten called How Learning Works, and part of this describes sort of the journey from being a novice, a complete beginner in a particular subject, advancing into someone who's more advanced, becoming more become competent practitioner, and then eventually becoming an expert. And what we find as we're teaching, as as people are learning about a new subject, when they're novices, they start out with memorizing some facts, mostly disconnected facts. As people become more advanced, they gain more understanding. They start to string together the facts. You get this kind of linear thinking. Amongst competent practitioners though, we see really what what could be called decision trees cognitive structures that are much more tree like in terms of decision making. But when people move into expertise in a field, what we see are essentially learning how to break the rules, not just following those decision trees blindly, but understanding where they apply and where they they don't necessarily apply. And that creates cognitive structures, which are graphs. We should learn something about this that essentially when we're talking about working in a complex context, when we're talking about learning perceived emergent patterns in a in a complex challenging situation and being able to gain expertise on how to behave in that kind of environment, we need to go move toward more graph like cognitive structures. And this should be definitely a clue for the way forward with AI. Relational data management and reporting, that all arose from simple business contexts. And that led to things like data warehouses and data lakes, practices like business intelligence. But the the complexities, the uncertainties of the twenty first century, this really forces more where leaders must increasingly rely on sense making by leveraging the use of graph patterns. Now in contrast, there is something called ambiguity aversion. If you look this up, it's it comes from cognitive psychology but also behavioral economics. And it has to do with how when faced with uncertainty, many people will do exactly the wrong thing. They'll make exactly the wrong choice. This is highly important for AI applications in terms of helping organizations and leaders be able to augment their decision making processes to understand that, in fact, some people just try to sweep it under the rug. But really, we must be embracing the complexity, working with emergent patterns, working with graph thinking. I'd like to acknowledge a good friend colleague, Juergen Buehler from, BSF in Germany. Juergen and I put together this scenario of the village to explore graph thinking and help to illustrate this concept. We have an article also that goes in a bit more detail that that's on Medium. So talking about graphs. You know, the the thing is that in business graphs, connected data is everywhere. Now when you talk with people, when you teach people about doing data science, when you talk with people about data, they typically respond by describing a table, rows and columns, rectangles, matrices, spreadsheets, reporting tables, these kinds of things. People are trained to think in terms of square patterns when they hear the word data. And, okay, that works except when when it's not the case. Spreadsheets, SQL reporting, all these rely on graphs. Within every Excel spreadsheet, there's a dependency graph. That's the key to calculating it. Within every SQL query, there is a query plan which is directed acyclic graph. There are complex ERDs for being able to represent the scheme and resolve that. The fact is that these kinds of computational techniques rely on graphs. And in fact the metadata and the business rules which go part and parcel with these internal graphs, they get obscured by the kind of tabular format. It becomes difficult to troubleshoot and test and reuse an audit. It creates technical debt. And if you don't believe me, understand that ninety five percent of the global two thousand companies in the world, when they do their final tax reporting, the final stages of ninety five percent of the firms is done in spreadsheets. And those spreadsheets are not consistent from one quarter to the next. That is tech debt, and it's a problem. Gartner had been somewhat iffy with regards to graph technologies. However, in early twenty twenty one in February, Gartner did an about a face. They're saying that, graph technologies will bump up to eighty percent of data analytics, up from ten percent usage in twenty twenty one. Those are very rapid change. And what they're pointing toward is this. By exposing metadata and business rules, the very thing that gets obscured by spreadsheets and relational databases, which leads to tech debt, This is what graphs surface and allow domain experts to be able to manipulate. And so we're seeing a lot of rise of of graph technologies, this kind of usage throughout industry. It's appalling to me when I talk to people about graphs, they immediately say, oh, well, that's that's just for Google or Facebook, which is which is utterly ridiculous. When you look at it in the industrial applications, I I know use cases, for instance, in manufacturing where one single instance within a manufacturing firm of their graph applications, one use case alone is larger than the entirety of Google's knowledge graph. And this particular company has dozens and dozens of different use cases. So I I I think that this is a conceit to say that the the Silicon Valley tech firms have a lead in graph. In fact, they're laggards. The real problem with graphs, this is happening out in finance, happening in pharma, in manufacturing, and a lot of areas of regular industry, not tech companies. The common themes that we see have to do with data integration across business silos, having to do essentially motif mining, understanding data objects as shapes or topologies, geometries, grappling with complexity and uncertainty, working on disambiguation problems, working on, you know, eliminating cycles and data, and also being able to drill down the details, which are which are critical, more important than just receiving a bunch of aggregate accounts that you would typically get out of a lot of BI tools. You know, I'll point out a few here. Fudome, with Albert Lezlo, Barbasi, really, brilliant work is they've invented the term of network medicine. And there's a nature article that describes what they're doing. If you haven't seen this so far, it's one of the most sophisticated uses of CRAP technologies and probably some of the most major impact of what we'll see in terms of of human outcomes. Also, you know, I'll point to drug discovery from companies like Novartis and AstraZeneca, and we can talk about Roche and many others. I'm pointing here especially to, Stephen Reilings talks out of Novartis using graphs for, drug discovery, Connor Hill at AstraZeneca, in manufacturing. I mentioned about my colleague, Jurgen Buehler at BASF, also, Stefan Lempater and Thomas Hjubauer at Siemens. You know, definitely in the tech firms, you do see things like Amazon's product graph, and definitely Luna Dom is doing some amazing work there. Mark Grover was formerly product manager of data at Lyft. He's done some amazing work, talking about their use cases for metadata management using graphs. Refinitiv and Bloomberg and others, of course, in in the tech space, but, you know, Tom Baker and others talking about that with fintech data. And, and certainly, if you get a chance, check out some of the talks from Charles Hay, Sheng Ma Hay at, Ant Financial on what they're doing. And and if I were trying to to sort of paraphrase a lot of these industry presentations about levering craft technologies, one of the models that I use to try to understand these these case studies is essentially this triangle of know your business, know your customer, and know your data. So this is a a bit of rounding the edges, but it's it's sort of a a middle model for me to try to understand how are these use cases related to each other. Okay. Let me shift gears a bit. Let me talk a little bit about the graph theory because I I think it's very important for understanding how do we leverage what's happening in hardware. I'll just say that. So, we've had really good math to handle graphs for a long time. And a typical kind of thing is it's inside of algebraic graph theory. The idea is you have a complex graph and you can take parts of it and push it into a vector. You can vectorize it. And and we do this when we're training neural networks a lot. Alternatively, you could take, and represent the edges and relationships in a graph by using something, a matrix representation. So non negative matrix factorization, using things like an adjacency matrix. Or you can get a little bit complex and use something called a tensor, which is essentially like a three an n dimensional matrix, if you will. So there's ways to go between what we call algebraic objects, vectors, matrices, tensors, and graphs. You can transform back and forth. And for a long time, the trick was to use the matrix approach. There's something called non negative matrix factorization, which says, for instance, we'll run page rank. We'll take a graph. We'll make a big matrix out of it. We'll apply a lot of linear algebra and do a lot of transforms and compute up the results, the rankings that we need. And there's some really fascinating work on this. I definitely point you toward Tim Davis at Texas a and m. He runs the, Sparse Matrix Museum. You see pictured there, also David Glack at Purdue and others. You know, some of these folks, Tim Davis, of course, have been working on graphBLAST. Really fascinating work in terms of factorization, but that's kind of what we've seen in the past. There's been a lot of that. Sometimes you can't just get away with this trick of taking a complex graph and a lot of symbolic relationships and take that information and making it all numeric and then number crunching. Sometimes you have to work with symbolic. So certainly when you're working on graph visualizations, when you're calculating deep learning models, training them, when you're recalculating graph algorithms, you need to take your graph and put in some sort of numeric representation usually to be efficient. But when you're working with natural language, when you're understanding regulatory compliance audits, when you have human loop, like, some type of active learning situations, when you're trying to represent rules from domain expertise or explainable AI, these are cases where you need to work more in the symbolic representation. So, what I recommend is this thing with all apologies to Daniel Kahneman, what I call thinking sparse and dense. And this is the idea that when you're in a data workflow describing a pipeline and all the transformations across the data workflow, there are stages that are more sparse and stages that are more dense. Typically, when you're doing your data preparation, it's usually more sparse. You pull things together and oftentimes these are relatively bandwidth limited. But then when you go and train a model, for instance, it's probably more compute limited. And that's not always the case when you're doing a deep learning model. You know, parts of a deep learning model, of course, the convolution layers, those will be compute limited. A lot of vector processing there. But when you have to calculate a loss function in deep learning, this is bandwidth limited. So thinking sparse and dense across a problem, you know, typically once you've calculated and trained your model, then you have to apply it with real world data. So you sort of shift from sparse to dense back to sparse. We go through this in a lot more detail. Dean Wampler and I did a a report, working with the open source machine learning, technology leads at NVIDIA, earlier this year, and it's a free download if you wanna check. It's called hardware greater than software, greater than process. And at the core of this is this mathematical idea that we can go from a relatively complex graph and then project it into some other sort of space, reshape it as a matrix, reshape it as a tensor, do our calculation, transform it back, and then populate the answers that that we've gained to the calculation back into the graph. So a lot of transforms and inverse transforms. And this is really crucial for being able to leverage contemporary hardware. And there's a whole bunch of evolution going on right now in the hardware side for accelerators, for GPUs and FPGAs and others, etcetera. What this points to is a general kind of narrative in data science about, you know, starting with unstructured data and progressing to more and more structure and then leveraging that structure. And that is about dimensionality and how to really manage dimensionality. It's a really essential component of this trade up between numeric and symbolic representation being able to shift back and forth to those transforms. So it's it's a lot of how we're projecting this notion of graph data science, how to apply graph technologies in a very formal way at different points in a workflow and how to really manage and get the best leverage out of it. There's a GitHub repo and a community of developers around it. Really, we we wanna tackle a few things, but, you know, some of the main parts about this were that there's a lot of great tools, and they don't necessarily play well with each other. There's a lot of camps that are working graph technologies, different parts, but they weren't really talking with each other. And we wanted something that would would work well in open source with, you know, a typical data science, py data type technology stack. But at the same time, still be very suitable for for parallelization with things like Dask and Ray and RAPIDS and Spark and that. So we've been working on a lot of integration. We pull together a lot of packages into this kind of abstraction layer. And I I want to show a few of these here. You know, one is just when you're building the graph, if you want to work with w three c standards, RDF, composing, graphs out of triples, basically, being able to use a number of different controlled vocabularies. So we support this and make it a lot simpler than the low level tools like you would see in already ebblib. So with KGLab, you can define your namespaces, instantiate a graph, and then just start adding different nodes to it, leveraging those namespaces. If you're familiar with w three c, there's, like, a dozen different standards for how to serialize. And, of course, we pick up support for those along with making use of things like Pathlib and FS spec codecs and all that. Of course, this has gone into RDF lib six, but we definitely have support there. You know, along with this, a bunch of other formats too if you wanna work with dot or or, d max or in, you know, some of these as well as CSVs. But the interesting thing that we found is that Apache Arrow and Apache Parquet are orders of magnitude more efficient, actually more than two orders of magnitude more efficient than, say, using something like CSVs or or some of the forms of, like, w three c standards for serialization. So, we highly recommend using these, and, and it really fits with large distributed graphs, by the way. There's a number of excellent tools in the visualization space. These don't necessarily blend all that well with, say, RDF lib. So we built out paths to be able to do the transforms back and forth. Here, I'm showing how to do a transform for something called pyvis. We have integrations with Cairo and Matplotlib, of course, but also with our forensic Graphistry, which I highly recommend. It's a GPU accelerated. As far as querying, you know, you can do sparkle queries, but then you get back a named tuple iterator or a pandas data frame. Again, something that's just much easier to use with the PyData tech stack. Shackle is, I think, one of the one of my favorite more recent additions to w three c stack and certainly being able to do shape constraints for validation and prescription. Really a lot of great use cases like unit tests. Of course, there's a there's a wide range of different types of graph algorithms, but we provided pathways for integration of network x, iGraph, and, of course, cuGraph for, out of RAPIDS for GPU use. And and we've been looking at graph tools as well to integrate that further. There's been so much going on in this field of graph neural networks and PyTorch geometric etcetera. And so we we do support pathways for integration with PyTorch geometric and there's also some inference that's been integrated in terms of using statistical relational learning that is to say probabilistic graphs. You can provide rules that are probabilistic predicates and then run probabilistic soft logic on them, for instance. And I find this is really, really useful because it's a way of representing uncertainty per node, per edge, but also for parts of the graph overall. And, of course, there's a lot of other types of inference when we talk about graphs. We could be doing closures in OWL or RDFS. We could be doing transitive and SCAs. Of course, deep learning graph neural networks is a form of inference. There's also, you know, types of inference that can be drawn from graph algorithm use such as clustering. There's creative ways to do inference with shackle. What I want to point out here is that not all of these are quite the same. In fact, you can plot them. Some are better in terms of formalism and more analytic solutions. Others are better at working with messy noisy data and represent and being able to represent uncertainty. The point that we wanna make is mix and match. For a given use case. Find out how can you blend these together. We see how these types of operations fit in at each point. Thank you very much. If you wanna get a hold of me, here are some links. Look forward to talking to you on Twitter and throughout the rest of the conference",
  "language": [
    {
      "id": "http://vocab.getty.edu/aat/300388277",
      "type": "Language",
      "_label": "English"
    }
  ],
  "created_by": {
    "id": "https://example.org/cdkg/Creation/presentation/6",
    "type": "Creation",
    "caused_by": [
      {
        "id": "https://example.org/cdkg/Activity/presentation/6",
        "type": "Activity",
        "_label": "Presentation of \"Graph Thinking\""
      }
    ]
  },
  "digitally_carried_by": [
    {
      "id": "https://example.org/cdkg/DigitalObject/recording/6",
      "type": "DigitalObject",
      "_label": "Recording of \"Graph Thinking\""
    }
  ]
}