{
  "@context": "https://linked.art/ns/v1/linked-art.json",
  "id": "https://example.org/cdkg/LinguisticObject/presentation/13",
  "type": "LinguisticObject",
  "_label": "Textual content of \"Knowledge Graphs, Graph AI, & the Need for High performance Graph Computing\"",
  "classified_as": [
    {
      "id": "http://vocab.getty.edu/aat/300027388",
      "type": "Type",
      "_label": "Transcript"
    }
  ],
  "content": "It's a great pleasure to give this talk. Thank you so much for inviting me. My name is Keshav Pingali. I'm the CEO of Katana Graph. I'm also a chair professor in the computer science department at the University of Texas at Austin. And today, I want to talk about knowledge graphs, graph AI, and the need for high performance graph computing. Let me begin by telling you why we at Katana Graph feel there is a great need for high performance graph computing. Usually, when people talk about high performance computing, they think about computational science applications, such as the design of aircraft or the design of buildings, for which people use very large clusters, like the stampede cluster that we have at the Texas Advanced Computing Center in Austin. However, about ten years ago, a group of, researchers, including myself and and some colleagues at MIT, CMU, and other places, realized that there was a big opportunity to take all of the techniques that we had developed in the context of computational science app applications in high performance computing and to use them for doing high performance graph computing. And, basically, there are two parts to the argument for why this is a useful thing to do. One having to do with the volume of data and the other having to do with time to insight. So first, when it comes to the volume of data, we're all familiar with terms like the data tsunami, the data flood, data deluge. What all these apocalyptic terms tell us is that the world has generated an enormous amount of data and is generating even more data at an ever accelerating pace. And so if you read documents like IDC twenty, for example, you find statistics like more than half of the world's data was created in the last two years, but less than two percent has been analyzed. To be sure, some of this data that's being generated is relational data. You can represent the data very usefully as tables or relations, store them in relational databases, and then query them using SQL and other time honored techniques. But what we find is more and more of the new data that's being generated is what we call unstructured data that doesn't quite fit the relational model that well. And a lot of this unstructured data, it turns out, can be usefully represented as a graph and then processed using graph techniques. So that is the first side of the argument, the first part of the argument about why high performance graph computing is essential, the sheer volume of graph data that needs to be analyzed. The second part of the argument is time to insight. In most application areas, there's usually a window of opportunity. And if you can perform your analytics on the data within that window of opportunity and obtain some insight, then you can profit from it. Whereas if your insights come after the window of opportunity has expired, well, then the, processing that you have done, the analytics that you have done are not very useful. And so these are the two reasons why we believe high performance scale out graph computing is going to be increasingly important as we go forward. Just the sheer amount of graph data that needs to be processed and then the need to process that very quickly. One question that we're always asked is, what are some of the areas where we see the use of graphs and graph technologies? This wheel over here tells you about some of the areas where we have, seen our customers using graphs. And I'm just going to go very quickly over some of the highlights. At the top right hand side of the circle, you see intrusion detection, fraud detection, large multinational defense contractor that was interested in doing real time intrusion detection in computer networks. And the way they wanted to do it was by building what are called interaction graphs and then mining these interaction graphs to find forbidden patterns within these graphs. So finding needles within a big haystack, as it's sometimes called. So that is one application, fraud detection, anti money laundering, lots of applications there. Another area where we see graphs being used increasingly is in the identity management space. So one of our customers ultimately wants to go to a graph with a trillion edges, which is very large indeed. Even in more conventional areas that have been around for a while, like electronic design automation, so circuit design, we see increasing use of graphs and hypergraphs. So circuits can be looked at as graphs or hypergraphs. The pins of the circuit correspond to the nodes of the graph. The wires correspond to the edges. And it turns out that a lot of the things they do in chip design, like static timing analysis, logic synthesis, placement routing, All of them can be done very effectively using parallel algorithms of the kind that we do at Katana. The final area I mentioned is medical knowledge graphs. So precision medicine, hypothesis generation for drugs. So here, the problem is you have a lot of medical data, and you want to represent that medical data in one big graph. And so this is called a medical knowledge graph. And one of the customers that we're currently dealing with wants to be able to mine these medical knowledge graphs in order to generate promising treatments for diseases and equally importantly, to rule out potential treatments for diseases before having to go to the lab, do a lot of expensive tests, and then find out that that treatment actually doesn't work. So these are just some of the areas where we see graphs. There are, in fact, many more. So this brings us to a data graph. So what do we do? Well, at the heart of our system, and I'll show you a cartoon diagram of our system on the next slide, we basically have a scale out graph engine that is optimized for doing analytics, for doing AI machine learning on very large graphs. So what we are building is what we call a graph intelligence platform, and it's been architected from the get go to handle massive graphs. It's been tested with some of the largest publicly available web crawl graphs, like w d c twelve, which has a hundred and twenty eight billion edges. Our system has been designed from the get go for graph computing and not so much for, for being a graph database unlike many of the other systems in this space. And so we can do in memory analytics, AI, and machine learning very fast. And for some of the routines that we have implemented for our customers, we're, like, ten to a hundred times faster than competing solutions. Massive scalability, we've shown that our graph engine scales to up to two fifty six machines on the big stampede supercomputing cluster that we have at Texas. We also run on all the three open clouds. So AWS is your Google Cloud. We are cloud first, as they say. And then the final thing that we have in our graph engine is we have native AI and machine learning. So we support a lot of unsupervised and supervised training algorithms. And then you can have your AI and machine learning pipelines that include querying, analytics, as well as third party integrations in your complete workflow while without leaving our system. And I'll show you some examples of that as we go forward. So here is the one click down cartoon level picture of our system. So at the heart of it is in the middle, you can see the scalable distributed graph engine. So it's got three parts to it. So the first part is CUSP, which is a streaming partitioner. If you have a big graph, it's usually where it's at rest. It's in AWS s three storage, your Google Blob storage, or somewhere. So what CUSP does is if you point it to this particular graph that's sitting in secondary storage, and then you say, here are the number of machines on which we want you to partition or shard the graph. And then you could also specify the, sharding policy or the distribution policy. You can even write your own partitioning policy quite easily using CUSP. What CUSP does is it then reads in the graph from secondary storage, and then it shards or partitions the graph between as many machines as you have told it to do. And then it builds an in memory representation of each portion of the graph on each of the machines in your cluster. Then the Galois graph computing engine kicks in. So the Galois graph computing engine is a shared memory graph computing engine. It's got a bunch of data structures and a runtime system that, again, has been optimized for many years for the needs of graph computing. Now anytime you shard or partition a graph, you need to worry about what happens at the boundaries of the partitions. You're going to need some synchronization and communication in order to keep the data on the different machines in lockstep, so to speak. So we support, programming model that's called the bulk synchronous programming model. And the communication and synchronization that's required in order to keep all of the different machines working together is performed by the Gluon communication engine. This, again, is a communication runtime that has been designed by us and optimized for the needs of graph computing. So this is essentially the heart of our scalable distributed graph engine. This is our platform. And as I told you, we have shown that it scales up to two fifty six machines, and it would probably scale to even larger numbers of machines as needed. We're exposing the functionality of this graph engine through c plus plus. So if you like writing c plus plus code, we give you a small number of constructs, and then you can go write c plus plus code and orchestrate the graph engine directly. And many of our routines in our libraries are written that way. But for data scientists, we're also providing, access to the graph engine through Python. So, again, we give you a small number of constructs, some of our data structures, and then you can orchestrate the graph engine directly from Python. What we are doing on this graph engine, the libraries that we are building are in the space of graph query. So we support OpenCypher, which is a query language that Neo four j has popularized, and we like it a great deal. GQL is a new query language that's coming out soon, so we're going to support that as well. We also have a lot of graph analytics routines, so all of the routines for path finding and community detection between the centrality and so on. If you go to our website, you can see a long list of these analytics routines that we support. We have some of the world's experts on graph pattern mining working in the company, and so we also have a lot of graph mining routines such as, for example, click detection or frequent subgraph mining routines. And then, of course, increasingly important areas, so Graph AI. So we have both supervised and unsupervised learning algorithms. And as I was saying, you can build an entire AI machine learning pipeline based on our system, as I'll show you later. We run on CPUs as well as GPUs, and we could easily extend this to FPGAs as well, although we haven't found that much of traction in the customer space for FPGAs yet. So now let me double click a little bit and tell you about knowledge graphs in a little more detail in some of the verticals that we are engaged in. So we're engaged in three verticals, pharma, financial services, and information security. And what I want to show you in the next few slides is basically what these knowledge graphs look like or what these graphs look like in these areas, and then the kind of graph AI, graph analytics that's required in these domains. So first up is health and life sciences. So here, as I was telling you earlier, people want to build these big medical knowledge graphs. And the nice thing about graphs is that they can represent entities of many different types. So graphs have heterogeneous nodes, heterogeneous edges. And this sort of cartoon graph over here shows you nodes for drugs, a node for a molecule, a node for a target protein, a node for a side effect. So all of these are entities that have different types. They can have different kinds of properties, but they can all live together in the same graph, so to speak. And then, of course, the edges also have type. So you can say a drug has a certain structure, which is this molecule. A drug has a certain side effect. It's associated with certain proteins, and so on. Another nice thing about graphs is that they can represent information at many different levels of abstraction. So for example, a given chemical molecule has a given chemical structure that the chemists know, and you can represent that chemical structure itself as a graph. And so that chemical structure of this molecule can live in the same data space, so to speak, as but drug and proteins, drug side effects, and so on. So data at many different levels of abstraction can all live together in the same data space if you use graphs. So what are the use cases that we are seeing? Well, I was telling you about a medical, a pharma company that we're working with that's growing drug hypothesis and discovery. Another area that we're very actively engaged in is in precision medicine, and I'll tell you a little more detail about that in the next few slides. Another place where graphs are used extensively is in financial services or fintech. So the idea here is to take financial transaction data and then represent that as a big heterogeneous graph, and then basically do analytics, AI, mining, and so on on these graphs in order to do things like fraud detection, identity theft, customer three sixty, and so on. So if you look at this, small graph over here, it's showing you a client, and then the client has an occupation. The client has purchased some services, client owns an account, the account has been invested in. And and, again, the key thing to take away is these are big heterogeneous graphs where the nodes have different types, each type has different sets of properties, and then the edges themselves have different types as well. And that is the kind of data that can be represented very conveniently using a graph. And then what we need to do in this particular area is to feed these graphs into traditional machine learning models in order to do things like fraud detection and so on. The final kind of knowledge graphs I'll show you are from the information security area. So I was telling you earlier that, we worked with a multinational company on real time intrusion detection and networks. So here is, interaction graph that was actually built by our system for this company over a period of time. And what you notice again is that there are different kinds of nodes. So, for example, there are websites. There are nodes for processes, nodes for files, for ports, for users, and so on. And if, for example, you read a certain file or you write to a certain file or you fork a process and so on, all of that activity can be represented very nicely using a graph of this sort. And then the intrusion detection problem that we solved for them was looking for certain forbidden patterns within this graph. I don't have the time to tell you about the particular pattern that we detected, but the highlighted edges in this graph were a forbidden pattern and that was highlighted and then reported to the operator. So that is the data. All of this data in these domains are being represented using graphs or knowledge graphs. Now what we need to do in order to do the analytics is you can have supervised learning, unsupervised learning. So we support unsupervised methods like Louvain clustering, page rank, betweenness centrality, which is very important in security applications, as well as supervised learning algorithms like GNNs and GTNs. And, again, you need high performance scale out processing because the datasets in these worlds are very, very large. So you could have hundreds of billions of nodes in these graphs with hundreds of billions of edges, and then you need to do all of this processing very fast. And so I'm making the same argument that I started with, earlier in my talk, except that now I made it a little more concrete by giving you a bunch of use cases and, showing you what is done with that data. So here is a slide that goes a little more into this pharma use case that I was, showing you. And the takeaway from this particular use case is the need for an integrated system with querying, analytics, AI, all built into the same workflow. So let's take a look at what this is. In this particular company, they had a big medical knowledge graph, and then they gave us a query, which I've shown in the bottom right. The query says extract oral drugs and associated targets for heart failure treatment, return the top ten chemical compounds most similar to a given compound, such as benzene in this case, and return their targets. So what is done is to take that English language specification, and then that gets translated into a small graph that I've shown at the left in my diagram. So you have this big graph, which is the medical knowledge graph. That's called the host graph. This is called the query graph. And then one of the key things that you need to do to process this query is to find all instances of this query graph in this big medical knowledge graph. So each instance corresponds to a hit. Let's call it that. And so you basically return the list of all the hits that you got for this particular query. Then the next thing you need to do is to take all of these hits and then find their similarity. You need to give a similarity score to each one of these hits using a particular similarity metric that's called Tanimoto similarity. Now this is something that's implemented by what they call a chemical cartridge. So these are just routines, libraries that they use. We're not going to reimplement them, but basically, this shows you that when you build this kind, what you need to be able to do is to do already integration of these kinds of third party packages into your overall workflow. Now since you have a bunch of hits and you can compute their similarity scores in parallel, one of the nice things that we can do in our system is that we can compute the similarity scores themselves in parallel using our engine. And so this chart at the top shows you scaling for computing tiny motor similarity on a shared memory machine. And as you can see, you get very good scaling as you increase the number of threads. Once you've got the scores, the similarity scores for all of the hits, you sort them, and then you return the top ten. And so what you see over here is a relatively complicated workflow. We work with far more complicated workflows. But even in this example, you can see the need for querying. You see the need for integration with third party packages, such as these chemical cartridges like RDKit. And then you see the need for doing ranking and other kinds of algorithms. So let me end by telling you about a few use cases. So this is GraphAI in the real world. This is some work that we're doing with one of the biggest, fintech companies in the world. So they want to handle massive datasets with billions of nodes and edges. And they gave us a bunch of problems, and I'll show you results for two of them. One of the things they wanted was a page rank computation, and then the other is a unsupervised learning algorithm called Louvain clustering, which is basically a community detection algorithm that builds hierarchical communities, and that's very popular now in the machine learning world. And so the chart below shows you some numbers, for two different graphs, a small graph with a hundred million nodes and a bigger graph with twenty billion nodes. And, we're also showing you the number of machines we used and then the time to compute page rank and Louvain. And these are substantially faster than what they were able to do on the system that they had been using previously before they started working with us. Here's another application. Lots of interest in this area. So molecular property predictions. So here is the way to think about the problem. I have a bunch of molecules whose properties I know, And these properties could be at many different levels of attraction, so they could be quantum properties, physical, chemistry properties, or even physiology properties, like whether this particular compound is toxic to human beings or not. So whatever these properties are, the way to think about the training data in this case is that you have a collection of molecules, and each molecule is labeled with a property. And then the inference task is given a new molecule whose properties you don't quite know, well, can you predict what the properties of this molecule are given all of the training data for the molecules that you do know? So this is a very important problem. Lots of interest in this area. There's a benchmark competition that's called TDC benchmark competition that lots of people are entering now. And, recently, some of our guys came up with a innovative way of using graphs in this particular area. So given a bunch of molecules and their labels, what they actually do is to build what they call a molecular similarity graph. So in the molecular similarity graph, each node corresponds to one of the molecules, and then there is an edge between them if they are similar in their properties according to some RDKit measure. So you can think about this as representing similarity of the different molecules that you have in your training data. And the node features come from the Katana Life Science module. So what we do with this labeled graph is that we use a g n n in order to create vector space representations for each of the nodes. So we embed each of the nodes in some high dimensional space. And then given a new molecule, we basically use these learned embeddings in order to do the prediction . And we were able to improve the leaderboard by about six percent, in with just about two weeks of effort. So what you can see is what we are doing is inventing new algorithms, new ways of thinking about problems, AI machine learning problems using graphs. But then we also have this very nice platform that we can quickly implement these new algorithms on and then see how well they do. Here's another application from the precision medicine area. Lots of VC interest in this area. Lots of companies working in this area. So here is the precision medicine problem at a very simple level. Right now, if you have a disease, you go to a doctor, chances are you will get pretty much the same treatment that he would have given to some other patient who has the same disease that you have. What we would like to do, obviously, is to be more precise about the treatment that we give you by exploiting things like your medical history, your genetic profile, everything that's known about your family, your financial circumstances, your environment, and so on. And so what we need to do is essentially get a patient three sixty information, so to speak, about each patient and then use that information in order to direct the treatment that we're giving you for a particular disease. So we are working with one of the big precision medicine companies in this space, and here is how they want to do it. They want to take all of the data about a patient and represent that data as a graph. So for in your training set, you have a bunch of patients, and so you've abstracted each patient out into a graph. And then for each patient in your training set, you know whether the treatment has been successful or not. And so you can label the graphs in your training set with that particular label. And now the problem is the following. If a new patient comes in, what we do is we build a graph corresponding to the data for that person, and then we infer the label for this new graph based on everything that we know from the training set. So this is basically what's called a graph classification problem. And, again, we're using state of the art techniques in graph transformer networks and graph neural networks in order to solve this particular problem using AI and machine learning techniques. This is, some recent work on graph transformer networks. So this is something that we developed for the precision medicine use case. And, I'm not going to go through the details of the running times and accuracy, but I want to make two points using this slide. The first is that we were able to improve the running time over previous algorithms for GTNs. And the way we do it was by inventing a very new algorithm that uses sampling without losing accuracy. So developing new algorithms for these graph machine learning problems is one of the things that we do at Katana. And then again, as I've been emphasizing, we have this this platform on which we can quickly implement these algorithms in parallel and see how well they do. This is my last slide. I want to conclude by telling you about something that you all know, and that is that knowledge graphs and AI, at least in our opinion, are the next base big thing. So this whole area of analytics started out with what we can call descriptive analytics. So this was the old style analytics where basically you just said, what happened? So you said, I had, pain in my chest, and there was a shooting pain down my left arm. That's what happened. Diagnostic analytics is the next more sophisticated kind of analytics. So here, what we want to do is to also say why something happened. So it's not just an account of what happened, but also you probably had all of those problems that you described because you were having a heart attack. So that is diagnostic analytics. The next more sophisticated analytics is prescriptive analytics. Given that these things have happened and given that this is why they probably happen, here are all the things you need to do in the future in order to mitigate the bad effects of whatever happened. So here you obviously need some predictive models in order to be able to say what needs to be done in the future. And then the most sophisticated thing is what we call predictive analytics. And in predictive analytics, you're not really building models based on rules or something that people give you, but you build the models automatically using all of the big data that you have and all of these wonderful machine learning and AI techniques that are available to us. So that is where there is going to be a lot of action, predictive analytics. There are many kinds of predictive analytics on different kinds of data, but we believe graphs are going to be a very big part of predictive analytics. And that is the space that katana graph plays in. With that, I'll conclude. Thank you for your attention.",
  "language": [
    {
      "id": "http://vocab.getty.edu/aat/300388277",
      "type": "Language",
      "_label": "English"
    }
  ],
  "created_by": {
    "id": "https://example.org/cdkg/Creation/presentation/13",
    "type": "Creation",
    "caused_by": [
      {
        "id": "https://example.org/cdkg/Activity/presentation/13",
        "type": "Activity",
        "_label": "Presentation of \"Knowledge Graphs, Graph AI, & the Need for High performance Graph Computing\""
      }
    ]
  },
  "digitally_carried_by": [
    {
      "id": "https://example.org/cdkg/DigitalObject/recording/13",
      "type": "DigitalObject",
      "_label": "Recording of \"Knowledge Graphs, Graph AI, & the Need for High performance Graph Computing\""
    }
  ]
}